{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-05T03:55:44.966Z"
    },
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from retinanet import model\n",
    "from retinanet import coco_eval\n",
    "from retinanet.dataloader import CocoDataset_inOrder,rehearsal_DataSet, collater, Resizer, AspectRatioBasedSampler, Augmenter, Normalizer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import torch\n",
    "root_path = '/home/deeplab307/Documents/Anaconda/Shiang/CL/'\n",
    "method = 'w_distillation'\n",
    "data_split = '15+1'\n",
    "start_round = 1\n",
    "batch_size = 1\n",
    "\n",
    "checkpoint_epoch = 50\n",
    "\n",
    "def checkDir(path):\n",
    "    \"\"\"check whether directory exists or not.If not, then create it \n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "def get_checkpoint_path(method, now_round, epoch, data_split =\"None\"):\n",
    "    global root_path\n",
    "    # global data_split\n",
    "    \n",
    "    \n",
    "    checkDir(os.path.join(root_path, 'model', method, 'round{}'.format(now_round)))\n",
    "    checkDir(os.path.join(root_path, 'model', method, 'round{}'.format(now_round), data_split))\n",
    "    \n",
    "    path = os.path.join(root_path, 'model', method, 'round{}'.format(now_round), data_split,'voc_retinanet_{}_checkpoint.pt'.format(epoch))\n",
    "    return path\n",
    "\n",
    "\n",
    "def readCheckpoint(method, now_round, epoch, data_split, retinanet, optimizer = None, scheduler = None):\n",
    "    print('readcheckpoint at Round{} Epoch{}'.format(now_round, epoch))\n",
    "    prev_checkpoint = torch.load(get_checkpoint_path(method, now_round, epoch, data_split))\n",
    "    retinanet.load_state_dict(prev_checkpoint['model_state_dict'])\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(prev_checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(prev_checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "\n",
    "\n",
    "# coco_path = '/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012'\n",
    "\n",
    "\n",
    "\n",
    "# dataset_train = CocoDataset_inOrder(coco_path, set_name='TrainVoc2012', dataset = 'voc',\n",
    "#                                     transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]),\n",
    "#                                    data_split=data_split, start_round=start_round)\n",
    "\n",
    "# # dataset_val = CocoDataset_inOrder(os.path.join(root_path, 'DataSet', 'VOC2012'), set_name=\"ValVoc2012\", dataset = 'voc', \n",
    "# #                 transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "# #                 start_round=1, data_split = \"20\")\n",
    "\n",
    "# dataset_train = CocoDataset_inOrder(os.path.join(root_path, 'DataSet', 'VOC2012'), set_name='TrainVoc2012', dataset = 'voc',\n",
    "#                                     transform=transforms.Compose([Normalizer(), Resizer()]),\n",
    "#                                    data_split=data_split, start_round=start_round)\n",
    "# retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# retinanet.cuda()\n",
    "\n",
    "# #sampler = AspectRatioBasedSampler(dataset_train, batch_size = batch_size, drop_last=False)\n",
    "# # # dataloader_train = DataLoader(dataset_train, num_workers=2, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "# loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "#readCheckpoint(method, start_round, checkpoint_epoch,data_split, retinanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T02:49:27.708209Z",
     "start_time": "2021-06-25T02:49:27.670266Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "def collater(data):\n",
    "\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [s['annot'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "        \n",
    "    widths = [int(s.shape[0]) for s in imgs]\n",
    "    heights = [int(s.shape[1]) for s in imgs]\n",
    "    batch_size = len(imgs)\n",
    "\n",
    "    max_width = np.array(widths).max()\n",
    "    max_height = np.array(heights).max()\n",
    "\n",
    "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = imgs[i]\n",
    "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "    \n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "\n",
    "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': padded_imgs, 'annot': annot_padded, 'scale': scales}\n",
    "\n",
    "class AspectRatioBasedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size, drop_last):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = self.group_images()\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.groups)\n",
    "        for group in self.groups:\n",
    "            yield group\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.data_source) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def group_images(self):\n",
    "        # determine the order of the images\n",
    "        order = list(range(len(self.data_source)))\n",
    "        order.sort(key=lambda x: self.data_source.image_aspect_ratio(x))\n",
    "\n",
    "        # divide into groups, one group = one batch\n",
    "        return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]\n",
    "\n",
    "class A_GEM(object):\n",
    "    def __init__(self, model, replay_dataset, batch_sample = 5):\n",
    "        self.memory = replay_dataset\n",
    "        self.model = model\n",
    "        self.batch_sample = batch_sample\n",
    "        self.replay_grad = None\n",
    "        self.update_dataLoader()\n",
    "        \n",
    "    def cal_replay_grad(self, optimizer):\n",
    "        #print(\"calculate replay grad!\")\n",
    "        self.replay_grad = []\n",
    "        #data = self.dataLoader.__iter__().next()\n",
    "        \n",
    "        num_groups = len(self.sampler.groups)\n",
    "        for group in self.sampler.groups:\n",
    "            data = []\n",
    "            for idx in group:\n",
    "                data.append(self.sampler.data_source[idx])\n",
    "            data = collater(data)\n",
    "            temp = []\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                prev_status = self.model.distill_loss\n",
    "                self.model.distill_loss = False\n",
    "                with torch.cuda.device(0):\n",
    "                    classification_loss, regression_loss = self.model([data['img'].float().cuda(), data['annot'].cuda()])\n",
    "                    classification_loss = classification_loss.mean()\n",
    "                    regression_loss = regression_loss.mean()\n",
    "                    loss = classification_loss + regression_loss\n",
    "                    loss.backward()\n",
    "                    print('Replay Data: Classification loss: {:1.5f} | Regression loss: {:1.5f}'.format(float(classification_loss), float(regression_loss)))\n",
    "                    for name, p in self.model.named_parameters():\n",
    "                        if \"prev_model\" not in name and \"bn\" not in name and p.requires_grad:\n",
    "                            temp.append(p.grad.view(-1))\n",
    "                    \n",
    "                    temp = torch.cat(temp) / num_groups\n",
    "                    if self.replay_grad == []:\n",
    "                        self.replay_grad = temp\n",
    "                    else:\n",
    "                        self.replay_grad += temp\n",
    "                        \n",
    "                        \n",
    "                    del temp\n",
    "                    del classification_loss, regression_loss\n",
    "                optimizer.zero_grad()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        \n",
    "#             self.cal_replay_grad(optimizer)\n",
    "        self.model.distill_loss = prev_status\n",
    "    \n",
    "    def fix_grad(self):\n",
    "        # cal current gradient\n",
    "        cur_grad = []\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if \"prev_model\" not in name and \"bn\" not in name and p.requires_grad:\n",
    "                cur_grad.append(p.grad.view(-1))\n",
    "        cur_grad = torch.cat(cur_grad)\n",
    "        length_replay = (self.replay_grad * self.replay_grad).sum() # the vector of replay_grad's length \n",
    "        angle = (cur_grad * self.replay_grad).sum() # the two gradient's angle\n",
    "        \n",
    "        \n",
    "        # update grad\n",
    "        if angle < 0:\n",
    "            proj_grad = cur_grad - ((angle / length_replay) * self.replay_grad) # project gradient\n",
    "            #proj_grad = torch.where(torch.ge(angle, 0), cur_grad, proj_grad)\n",
    "            index = 0\n",
    "            for name, p in self.model.named_parameters():\n",
    "                if \"prev_model\" not in name and \"bn\" not in name and p.requires_grad:\n",
    "                    n_param = p.numel()  # number of parameters in [p]\n",
    "                    p.grad.copy_(proj_grad[index:index+n_param].view_as(p))\n",
    "                    index += n_param\n",
    "            del proj_grad\n",
    "        del cur_grad, self.replay_grad\n",
    "            \n",
    "    def update_dataLoader(self):\n",
    "        self.sampler = AspectRatioBasedSampler(self.memory, batch_size = self.batch_sample, drop_last=False)\n",
    "        #self.dataLoader = DataLoader(self.memory, num_workers=2, collate_fn=collater, batch_sampler=sampler) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T02:49:30.035522Z",
     "start_time": "2021-06-25T02:49:29.808510Z"
    }
   },
   "outputs": [],
   "source": [
    "rehearsal_dataset = rehearsal_DataSet('/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012', set_name='TrainVoc2012', dataset = 'voc',\n",
    "                        transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "                        data_split = \"15+1\",method = \"random\", per_num = 2)\n",
    "rehearsal_dataset.reset_by_round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T02:50:01.633463Z",
     "start_time": "2021-06-25T02:50:01.628460Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = AspectRatioBasedSampler(rehearsal_dataset, batch_size = 5, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T02:52:33.127966Z",
     "start_time": "2021-06-25T02:52:32.767594Z"
    }
   },
   "outputs": [],
   "source": [
    "for group in sampler.groups:\n",
    "    data = []\n",
    "    for idx in group:\n",
    "        data.append(sampler.data_source[idx])\n",
    "    data = collater(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T09:22:42.045660Z",
     "start_time": "2021-06-24T09:22:42.039987Z"
    }
   },
   "outputs": [],
   "source": [
    "agem = A_GEM(retinanet, rehearsal_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T09:24:15.133246Z",
     "start_time": "2021-06-24T09:24:12.651911Z"
    }
   },
   "outputs": [],
   "source": [
    "agem.cal_replay_grad(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改sample的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T14:57:43.637618Z",
     "start_time": "2021-06-30T14:57:42.509297Z"
    }
   },
   "outputs": [],
   "source": [
    "from retinanet.dataloader import CocoDataset_inOrder,rehearsal_DataSet, collater, Resizer, AspectRatioBasedSampler, Augmenter, Normalizer\n",
    "from torchvision import transforms\n",
    "rehearsal_dataset = rehearsal_DataSet('/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012', set_name='TrainVoc2012', dataset = 'voc',\n",
    "                        transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "                        data_split = \"15+1\",method = \"random\", per_num = 2)\n",
    "rehearsal_dataset.reset_by_round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T08:13:00.275842Z",
     "start_time": "2021-06-25T08:13:00.269848Z"
    }
   },
   "outputs": [],
   "source": [
    "rehearsal_dataset.cocoHelper.catNameToId('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T08:14:23.879500Z",
     "start_time": "2021-06-25T08:14:20.319263Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgIds = rehearsal_dataset.coco.getImgIds(catIds=17)\n",
    "random.shuffle(imgIds)\n",
    "\n",
    "i = 1\n",
    "for imgId in imgIds:\n",
    "    print(imgId)\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(im)\n",
    "    i += 1\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T07:37:52.875840Z",
     "start_time": "2021-06-25T07:37:52.866778Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "catIds = rehearsal_dataset.classOrder['id'][0]\n",
    "\n",
    "names = rehearsal_dataset.cocoHelper.catIdToName(catIds)\n",
    "print(names)\n",
    "\n",
    "sample = defaultdict(list)\n",
    "i = 0\n",
    "\n",
    "for name in names:\n",
    "    for _ in range(2):\n",
    "        sample[name].append(rehearsal_dataset.image_ids[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T07:41:44.923777Z",
     "start_time": "2021-06-25T07:41:44.916738Z"
    }
   },
   "outputs": [],
   "source": [
    "rehearsal_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T08:15:07.488110Z",
     "start_time": "2021-06-25T08:15:07.478660Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = {'person': [2008001302],\n",
    "         'car': [2010004059],\n",
    "         'bicycle': [2008004603],\n",
    "         'bus':     [2009004871], \n",
    "         'motorbike': [2010004848],\n",
    "         'aeroplane': [2011001871],\n",
    "         'boat':  [2008008616],\n",
    "         'chair': [2010002870],\n",
    "         'bottle': [2009005057],\n",
    "         'diningtable': [2010003078],\n",
    "         'bird': [2008006923],\n",
    "         'cat': [2009001948],\n",
    "         'cow': [2009003510],\n",
    "         'dog': [2008001479],\n",
    "         'horse': [2010004247]}\n",
    "\n",
    "samples = []\n",
    "for v in sample.values():\n",
    "    samples.extend(v)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T08:15:11.813133Z",
     "start_time": "2021-06-25T08:15:09.058319Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "per_num =1\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4*per_num,50))\n",
    "gs = gridspec.GridSpec(len(samples) // per_num, per_num)\n",
    "i, j = 0, 0\n",
    "for imgId in samples:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.subplot(gs[i,j])\n",
    "    plt.imshow(im)\n",
    "    j += 1\n",
    "    if j == per_num:\n",
    "        j = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T14:57:47.351342Z",
     "start_time": "2021-06-30T14:57:47.339200Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = {'person': [2008002080, 2008001302],\n",
    "         'car': [2010004059, 2010001043],\n",
    "         'bicycle': [2009004340, 2008004603],\n",
    "         'bus':     [2009004871, 2009004383], \n",
    "         'motorbike': [2010004848, 2011000233],\n",
    "         'aeroplane': [2009001541, 2008007629],\n",
    "         'boat':  [2008002850, 2008008616],\n",
    "         'chair': [2010004660, 2010002870],\n",
    "         'bottle': [2008006004, 2009005057],\n",
    "         'diningtable': [2011002818, 2010003078],\n",
    "         'bird': [2009001751, 2010003929],\n",
    "         'cat': [2009005037, 2009005177],\n",
    "         'cow': [2008008521, 2008008121],\n",
    "         'dog': [2010000484, 2008001479],\n",
    "         'horse': [2010004247, 2009001147]}\n",
    "\n",
    "samples = []\n",
    "for v in sample.values():\n",
    "    samples.extend(v)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T15:04:13.449593Z",
     "start_time": "2021-06-30T15:04:13.438251Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4*per_num,50))\n",
    "gs = gridspec.GridSpec(len(samples) // per_num, per_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T15:09:17.423612Z",
     "start_time": "2021-06-30T15:09:17.417346Z"
    }
   },
   "outputs": [],
   "source": [
    "len(rehearsal_dataset.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T15:10:43.407131Z",
     "start_time": "2021-06-30T15:10:35.066209Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "per_num = 4\n",
    "\n",
    "plt.figure(figsize=(4*per_num,50))\n",
    "gs = gridspec.GridSpec(len(samples)*2 // per_num, per_num)\n",
    "i, j = 0, 0\n",
    "\n",
    "\n",
    "replay_id = 0\n",
    "for imgId in samples:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.subplot(gs[i,j])\n",
    "    plt.imshow(im)\n",
    "    j += 1\n",
    "\n",
    "    if j == 2:\n",
    "        for _ in range(2):\n",
    "            imgId = rehearsal_dataset.image_ids[replay_id]\n",
    "            im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "            plt.subplot(gs[i,j])\n",
    "            plt.imshow(im)\n",
    "            j += 1\n",
    "            replay_id += 1\n",
    "    \n",
    "    if j == per_num:\n",
    "        j = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T14:59:30.557193Z",
     "start_time": "2021-06-30T14:59:26.270145Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "per_num =2\n",
    "\n",
    "plt.figure(figsize=(4*per_num,50))\n",
    "gs = gridspec.GridSpec(len(samples) // per_num, per_num)\n",
    "i, j = 0, 0\n",
    "for imgId in rehearsal_dataset.image_ids:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.subplot(gs[i,j])\n",
    "    plt.imshow(im)\n",
    "    j += 1\n",
    "    if j == per_num:\n",
    "        j = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T05:14:01.230915Z",
     "start_time": "2021-06-15T05:14:01.208621Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.gridspec as gridspec\n",
    "i = 0\n",
    "cat_name = 'motorbike'\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "future_class_id = []\n",
    "for i in range(1, len(rehearsal_dataset.classOrder['id'])):\n",
    "    future_class_id.extend(rehearsal_dataset.classOrder['id'][i])\n",
    "\n",
    "future_imgIds = set(rehearsal_dataset.cocoHelper.getImgIdFromCats(future_class_id))\n",
    "\n",
    "imgIds = rehearsal_dataset.cocoHelper.getImgIdFromCats(catIds=rehearsal_dataset.cocoHelper.catNameToId(cat_name))\n",
    "\n",
    "imgIds = list(set(imgIds) - set(future_imgIds)) \n",
    "\n",
    "random.shuffle(imgIds)\n",
    "for imgId in imgIds:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.figure()\n",
    "    plt.title(imgId)\n",
    "    plt.imshow(im)\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T14:58:06.799096Z",
     "start_time": "2021-06-30T14:58:03.328449Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "import os\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "catIds = rehearsal_dataset.classOrder['id'][0]\n",
    "names = rehearsal_dataset.cocoHelper.catIdToName(catIds)\n",
    "print(names)\n",
    "per_num = 1\n",
    "\n",
    "plt.figure(figsize=(4*per_num,50))\n",
    "gs = gridspec.GridSpec(len(rehearsal_dataset.image_ids) // per_num, per_num)\n",
    "i, j = 0, 0\n",
    "for imgId in rehearsal_dataset.image_ids:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.subplot(gs[i,j])\n",
    "    plt.imshow(im)\n",
    "    j += 1\n",
    "    if j == per_num:\n",
    "        j = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T12:47:27.196444Z",
     "start_time": "2021-06-14T12:47:22.813350Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "names = [name for name in rehearsal_dataset.classOrder['id'][:15]]\n",
    "print(names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,50))\n",
    "gs = gridspec.GridSpec(len(rehearsal_dataset.image_ids) // 2, 2)\n",
    "i, j = 0, 0\n",
    "for imgId in rehearsal_dataset.image_ids:\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.subplot(gs[i,j])\n",
    "    plt.imshow(im)\n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T14:58:40.025636Z",
     "start_time": "2021-06-30T14:58:34.247689Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "names = [name for name in rehearsal_dataset.classOrder['name'][:15]]\n",
    "print(names)\n",
    "for imgId in rehearsal_dataset.image_ids:\n",
    "    print(imgId)\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.figure()\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:34:06.117883Z",
     "start_time": "2021-06-06T15:33:59.922793Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img_path = \"/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012/images\"\n",
    "\n",
    "\n",
    "names = [name for name in rehearsal_dataset.classOrder['name'][:15]]\n",
    "print(names)\n",
    "for imgId in rehearsal_dataset.image_ids:\n",
    "\n",
    "    im = cv2.imread(os.path.join(img_path, str(imgId)[:4] + '_' + str(imgId)[4:] +'.jpg'))\n",
    "    plt.figure()\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查模型間參數的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:40.701512Z",
     "start_time": "2021-07-04T02:36:27.991672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to ./resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b900c1ce2f471782f8f63ac1a210ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102502400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "readcheckpoint at Round2 Epoch50\n",
      "readcheckpoint at Round1 Epoch50\n",
      "readcheckpoint at Round1 Epoch50\n"
     ]
    }
   ],
   "source": [
    "retinanet = model.resnet50(num_classes=16, pretrained=True)\n",
    "retinanet_upper = model.resnet50(num_classes=16, pretrained=True)\n",
    "retinanet15 = model.resnet50(num_classes=15, pretrained=True)\n",
    "readCheckpoint(\"w_distillation\", now_round = 2, epoch = 50, data_split=\"15+1\",retinanet=retinanet)\n",
    "readCheckpoint(\"special_try\", now_round = 1, epoch = 50, data_split=\"custom\",retinanet=retinanet_upper)\n",
    "readCheckpoint(\"w_distillation\", now_round = 1, epoch = 50, data_split=\"15+1\",retinanet=retinanet15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:58.985428Z",
     "start_time": "2021-07-04T02:36:58.978045Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_names(model):\n",
    "    names = []\n",
    "    for name,parameters in model.named_parameters():\n",
    "        names.append(name)\n",
    "    return names\n",
    "names = get_names(retinanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較1-15/1-16與1-16/continual model的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:58:21.103785Z",
     "start_time": "2021-07-04T02:58:19.918662Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "MSE = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "\n",
    "diffs_for_15 = defaultdict(float)\n",
    "diffs_for_cur = defaultdict(float)\n",
    "diffs_for_cur_15 = defaultdict(float)\n",
    "crit = L1\n",
    "for name in names:\n",
    "    if \"classificationModel.output\" not in name:\n",
    "        diff = crit(retinanet15.state_dict()[name], retinanet_upper.state_dict()[name])\n",
    "        diffs_for_15[name] = float(diff)\n",
    "        \n",
    "        diff = crit(retinanet.state_dict()[name], retinanet_upper.state_dict()[name])\n",
    "        diffs_for_cur[name] = float(diff)\n",
    "        \n",
    "        diff = crit(retinanet.state_dict()[name], retinanet15.state_dict()[name])\n",
    "        diffs_for_cur_15[name] = float(diff)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:58:21.252348Z",
     "start_time": "2021-07-04T02:58:21.247099Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_rank(diffs):\n",
    "    diffs_keys = [key for key in diffs.keys()]\n",
    "    diffs_values = list(diffs.values())\n",
    "    ascending = sorted(range(len(diffs)), key=lambda k: diffs_values[k])\n",
    "    \n",
    "    num = 100\n",
    "    \n",
    "    for idx in ascending[len(ascending) - num:]:\n",
    "        print(diffs_keys[idx], diffs_values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:58:22.868646Z",
     "start_time": "2021-07-04T02:58:22.857534Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_diff(diffs):\n",
    "    diffs_keys = [key for key in diffs.keys()]\n",
    "    diffs_values = list(diffs.values())\n",
    "    descending = sorted(range(len(diffs)), key=lambda k: diffs_values[k], reverse=True)\n",
    "    return diffs_keys, diffs_values, descending\n",
    "\n",
    "def print_rank_comp(diffs1, diffs2, num = 100):\n",
    "    diffs_keys1, diffs_values1, descending1 = sort_diff(diffs1)\n",
    "    diffs_keys2, diffs_values2, descending2 = sort_diff(diffs2)\n",
    "    \n",
    "    for i in range(num):\n",
    "        idx1 = descending1[i]\n",
    "        idx2 = descending2[i]\n",
    "        \n",
    "        print(\"{:32} {:.8f}  |  {:32} {:.8f}\".format(diffs_keys1[idx1], diffs_values1[idx1], diffs_keys2[idx2], diffs_values2[idx2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:56:17.484333Z",
     "start_time": "2021-07-04T02:56:17.466415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpn.P4_1.weight                  0.10018185  |  fpn.P4_1.weight                  0.10018935\n",
      "fpn.P3_1.weight                  0.10007307  |  fpn.P3_1.weight                  0.10008013\n",
      "fpn.P5_1.weight                  0.09966445  |  fpn.P5_1.weight                  0.09966066\n",
      "classificationModel.conv1.weight 0.03360674  |  classificationModel.conv1.weight 0.03360471\n",
      "classificationModel.conv4.weight 0.03352086  |  classificationModel.conv4.weight 0.03352169\n",
      "classificationModel.conv2.weight 0.03349030  |  classificationModel.conv2.weight 0.03348951\n",
      "classificationModel.conv3.weight 0.03348867  |  classificationModel.conv3.weight 0.03348842\n",
      "fpn.P7_2.weight                  0.03344531  |  fpn.P7_2.weight                  0.03344568\n",
      "fpn.P6.weight                    0.03338930  |  fpn.P6.weight                    0.03339134\n",
      "regressionModel.conv3.weight     0.03335461  |  regressionModel.conv3.weight     0.03336151\n",
      "fpn.P3_2.weight                  0.03334579  |  fpn.P3_2.weight                  0.03334575\n",
      "regressionModel.conv4.weight     0.03332213  |  regressionModel.conv4.weight     0.03333170\n",
      "regressionModel.conv2.weight     0.03331900  |  regressionModel.conv2.weight     0.03332369\n",
      "fpn.P4_2.weight                  0.03330471  |  fpn.P4_2.weight                  0.03330435\n",
      "regressionModel.conv1.weight     0.03327340  |  regressionModel.conv1.weight     0.03327766\n",
      "fpn.P5_2.weight                  0.03327161  |  fpn.P5_2.weight                  0.03327219\n",
      "fpn.P3_1.bias                    0.02972578  |  fpn.P3_1.bias                    0.02973367\n",
      "fpn.P4_1.bias                    0.02106580  |  fpn.P4_1.bias                    0.02106799\n",
      "fpn.P5_1.bias                    0.01646834  |  fpn.P5_1.bias                    0.01646559\n",
      "regressionModel.conv3.bias       0.01546363  |  regressionModel.conv3.bias       0.01556874\n",
      "fpn.P5_2.bias                    0.01518534  |  fpn.P5_2.bias                    0.01519023\n",
      "regressionModel.conv2.bias       0.01487613  |  regressionModel.conv2.bias       0.01491041\n",
      "classificationModel.conv1.bias   0.01476725  |  classificationModel.conv1.bias   0.01478671\n",
      "classificationModel.conv4.bias   0.01443348  |  classificationModel.conv4.bias   0.01444369\n",
      "classificationModel.conv2.bias   0.01430341  |  classificationModel.conv2.bias   0.01434013\n",
      "regressionModel.conv1.bias       0.01420505  |  regressionModel.conv1.bias       0.01423143\n",
      "fpn.P4_2.bias                    0.01408595  |  fpn.P4_2.bias                    0.01408360\n",
      "regressionModel.conv4.bias       0.01401001  |  regressionModel.conv4.bias       0.01402425\n",
      "classificationModel.conv3.bias   0.01391205  |  classificationModel.conv3.bias   0.01395192\n",
      "fpn.P3_2.bias                    0.01328680  |  fpn.P3_2.bias                    0.01328437\n",
      "fpn.P7_2.bias                    0.01322552  |  fpn.P7_2.bias                    0.01323851\n",
      "fpn.P6.bias                      0.00527022  |  fpn.P6.bias                      0.00526947\n",
      "layer4.2.conv2.weight            0.00287802  |  layer4.2.conv2.weight            0.00289656\n",
      "layer4.2.conv3.weight            0.00256723  |  layer4.2.conv3.weight            0.00258096\n",
      "layer4.0.conv2.weight            0.00254584  |  regressionModel.output.weight    0.00257029\n",
      "regressionModel.output.weight    0.00245550  |  layer4.0.conv2.weight            0.00255976\n",
      "layer4.1.conv3.weight            0.00241819  |  layer4.1.conv3.weight            0.00243371\n",
      "layer4.1.conv2.weight            0.00241811  |  layer4.1.conv2.weight            0.00243206\n",
      "layer4.0.downsample.0.weight     0.00236487  |  layer4.0.downsample.0.weight     0.00237932\n",
      "layer4.0.conv3.weight            0.00215166  |  layer4.0.conv3.weight            0.00216553\n",
      "layer4.2.bn3.weight              0.00214498  |  layer4.2.bn3.weight              0.00216331\n",
      "layer4.0.conv1.weight            0.00212653  |  layer4.0.conv1.weight            0.00213765\n",
      "layer4.2.conv1.weight            0.00201478  |  layer4.2.conv1.weight            0.00202701\n",
      "layer3.4.conv3.weight            0.00197879  |  layer3.4.conv3.weight            0.00199045\n",
      "layer3.5.conv3.weight            0.00195966  |  layer3.5.conv3.weight            0.00197179\n",
      "layer3.0.conv2.weight            0.00193861  |  layer3.0.conv2.weight            0.00194826\n",
      "layer3.5.conv2.weight            0.00190276  |  layer3.5.conv2.weight            0.00191497\n",
      "layer4.1.conv1.weight            0.00189932  |  layer4.1.conv1.weight            0.00190953\n",
      "layer3.1.conv3.weight            0.00187260  |  layer3.1.conv3.weight            0.00188334\n",
      "layer3.5.conv1.weight            0.00185969  |  layer3.5.conv1.weight            0.00186956\n",
      "layer3.3.conv3.weight            0.00185393  |  layer3.3.conv3.weight            0.00186354\n",
      "layer3.2.conv3.weight            0.00183781  |  layer3.2.conv3.weight            0.00184719\n",
      "layer3.4.conv2.weight            0.00180836  |  layer3.4.conv2.weight            0.00181999\n",
      "layer3.3.conv2.weight            0.00180259  |  layer3.3.conv2.weight            0.00181260\n",
      "layer3.4.conv1.weight            0.00179238  |  layer3.4.conv1.weight            0.00180243\n",
      "layer3.3.conv1.weight            0.00177189  |  layer3.3.conv1.weight            0.00178104\n",
      "layer3.2.conv1.weight            0.00176551  |  layer3.2.conv1.weight            0.00177512\n",
      "layer4.2.bn3.bias                0.00169031  |  layer4.2.bn3.bias                0.00169891\n",
      "layer3.0.downsample.0.weight     0.00168961  |  layer3.0.downsample.0.weight     0.00169859\n",
      "layer2.3.conv1.weight            0.00166587  |  layer2.3.conv1.weight            0.00167618\n",
      "layer3.2.conv2.weight            0.00166124  |  layer3.2.conv2.weight            0.00167031\n",
      "layer4.1.bn3.weight              0.00162810  |  layer4.1.bn3.weight              0.00165125\n",
      "layer3.1.conv2.weight            0.00159643  |  layer4.2.bn2.weight              0.00162448\n",
      "layer4.2.bn2.weight              0.00159086  |  layer3.1.conv2.weight            0.00160591\n",
      "layer3.1.conv1.weight            0.00158272  |  layer3.1.conv1.weight            0.00159064\n",
      "layer2.0.conv2.weight            0.00156919  |  layer2.0.conv2.weight            0.00157840\n",
      "layer2.3.conv3.weight            0.00156158  |  layer2.3.conv3.weight            0.00157423\n",
      "layer2.1.bn3.weight              0.00153708  |  layer2.1.bn3.weight              0.00155874\n",
      "layer2.1.conv3.weight            0.00152775  |  layer2.1.conv3.weight            0.00153805\n",
      "layer2.2.conv1.weight            0.00149957  |  layer2.2.conv1.weight            0.00151106\n",
      "layer2.2.conv3.weight            0.00148406  |  layer2.2.conv3.weight            0.00149256\n",
      "layer3.0.conv3.weight            0.00148235  |  layer3.0.conv3.weight            0.00149034\n",
      "layer2.2.conv2.weight            0.00146433  |  layer2.2.conv2.weight            0.00147125\n",
      "layer3.1.bn3.weight              0.00145985  |  layer3.1.bn3.weight              0.00146265\n",
      "layer4.0.downsample.1.weight     0.00144428  |  layer4.0.downsample.1.weight     0.00145840\n",
      "layer1.2.conv3.weight            0.00144198  |  layer1.2.conv3.weight            0.00145236\n",
      "layer2.3.conv2.weight            0.00143629  |  layer2.3.conv2.weight            0.00144354\n",
      "layer3.4.bn3.weight              0.00141701  |  layer3.4.bn3.weight              0.00143261\n",
      "layer2.3.bn3.weight              0.00140436  |  layer2.3.bn3.weight              0.00142612\n",
      "layer2.2.bn3.weight              0.00138363  |  layer2.1.conv1.weight            0.00139344\n",
      "layer2.1.conv1.weight            0.00138268  |  layer2.2.bn3.weight              0.00139277\n",
      "layer3.2.bn3.weight              0.00138218  |  layer3.2.bn3.weight              0.00139019\n",
      "layer3.0.conv1.weight            0.00138194  |  layer3.0.conv1.weight            0.00139006\n",
      "layer4.0.bn3.weight              0.00134411  |  layer4.0.bn3.weight              0.00134902\n",
      "layer3.3.bn3.weight              0.00133676  |  layer3.3.bn3.weight              0.00134699\n",
      "layer2.0.conv3.weight            0.00132967  |  layer2.0.conv3.weight            0.00133897\n",
      "layer3.5.bn3.weight              0.00131543  |  layer3.5.bn3.weight              0.00133582\n",
      "layer1.1.conv2.weight            0.00130336  |  layer1.1.conv2.weight            0.00131056\n",
      "layer1.1.conv3.weight            0.00127769  |  layer1.1.conv3.weight            0.00128560\n",
      "layer1.2.conv2.weight            0.00126403  |  layer1.2.conv2.weight            0.00127379\n",
      "layer1.2.conv1.weight            0.00125173  |  layer1.2.conv1.weight            0.00126251\n",
      "layer2.1.conv2.weight            0.00121789  |  layer2.1.conv2.weight            0.00122917\n",
      "layer1.2.bn3.weight              0.00120788  |  layer1.2.bn3.weight              0.00122031\n",
      "layer3.0.downsample.1.weight     0.00112114  |  layer3.0.downsample.1.weight     0.00112310\n",
      "layer2.0.downsample.0.weight     0.00106524  |  layer2.1.bn1.weight              0.00110501\n",
      "layer2.0.bn3.weight              0.00105088  |  layer2.0.downsample.0.weight     0.00107235\n",
      "layer1.1.bn3.weight              0.00103874  |  layer2.0.bn3.weight              0.00104763\n",
      "layer2.1.bn1.weight              0.00103230  |  layer1.1.bn3.weight              0.00104655\n",
      "layer2.1.bn2.weight              0.00100881  |  layer2.1.bn2.weight              0.00103406\n",
      "layer2.0.conv1.weight            0.00100699  |  layer3.0.bn2.weight              0.00101981\n"
     ]
    }
   ],
   "source": [
    "print_rank_comp(diffs_for_15, diffs_for_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:58:34.206287Z",
     "start_time": "2021-07-04T02:58:34.188241Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpn.P4_1.weight                  0.10018185  |  regressionModel.output.weight    0.00058768\n",
      "fpn.P3_1.weight                  0.10007307  |  regressionModel.conv1.weight     0.00054166\n",
      "fpn.P5_1.weight                  0.09966445  |  regressionModel.conv4.weight     0.00048191\n",
      "classificationModel.conv1.weight 0.03360674  |  regressionModel.conv3.weight     0.00046034\n",
      "classificationModel.conv4.weight 0.03352086  |  regressionModel.conv2.weight     0.00045789\n",
      "classificationModel.conv2.weight 0.03349030  |  regressionModel.conv1.bias       0.00038689\n",
      "classificationModel.conv3.weight 0.03348867  |  regressionModel.conv4.bias       0.00035082\n",
      "fpn.P7_2.weight                  0.03344531  |  classificationModel.conv4.bias   0.00033922\n",
      "fpn.P6.weight                    0.03338930  |  classificationModel.conv3.bias   0.00030602\n",
      "regressionModel.conv3.weight     0.03335461  |  fpn.P4_1.weight                  0.00030318\n",
      "fpn.P3_2.weight                  0.03334579  |  regressionModel.conv3.bias       0.00029391\n",
      "regressionModel.conv4.weight     0.03332213  |  fpn.P5_2.weight                  0.00028364\n",
      "regressionModel.conv2.weight     0.03331900  |  fpn.P6.weight                    0.00028349\n",
      "fpn.P4_2.weight                  0.03330471  |  classificationModel.conv4.weight 0.00027859\n",
      "regressionModel.conv1.weight     0.03327340  |  classificationModel.conv1.weight 0.00027263\n",
      "fpn.P5_2.weight                  0.03327161  |  regressionModel.conv2.bias       0.00026802\n",
      "fpn.P3_1.bias                    0.02972578  |  fpn.P3_1.weight                  0.00026326\n",
      "fpn.P4_1.bias                    0.02106580  |  fpn.P7_2.weight                  0.00026195\n",
      "fpn.P5_1.bias                    0.01646834  |  classificationModel.conv2.bias   0.00025288\n",
      "regressionModel.conv3.bias       0.01546363  |  classificationModel.conv3.weight 0.00025166\n",
      "fpn.P5_2.bias                    0.01518534  |  layer4.2.conv2.weight            0.00024431\n",
      "regressionModel.conv2.bias       0.01487613  |  classificationModel.conv2.weight 0.00024162\n",
      "classificationModel.conv1.bias   0.01476725  |  layer4.0.conv2.weight            0.00022957\n",
      "classificationModel.conv4.bias   0.01443348  |  fpn.P4_2.weight                  0.00022931\n",
      "classificationModel.conv2.bias   0.01430341  |  layer4.1.conv2.weight            0.00022803\n",
      "regressionModel.conv1.bias       0.01420505  |  layer4.2.bn3.weight              0.00022668\n",
      "fpn.P4_2.bias                    0.01408595  |  layer4.1.conv3.weight            0.00022095\n",
      "regressionModel.conv4.bias       0.01401001  |  layer4.0.downsample.0.weight     0.00022064\n",
      "classificationModel.conv3.bias   0.01391205  |  layer4.2.conv3.weight            0.00021730\n",
      "fpn.P3_2.bias                    0.01328680  |  fpn.P3_2.weight                  0.00021484\n",
      "fpn.P7_2.bias                    0.01322552  |  classificationModel.conv1.bias   0.00021127\n",
      "fpn.P6.bias                      0.00527022  |  layer4.0.conv3.weight            0.00020783\n",
      "layer4.2.conv2.weight            0.00287802  |  fpn.P5_1.weight                  0.00020718\n",
      "layer4.2.conv3.weight            0.00256723  |  layer4.0.conv1.weight            0.00020230\n",
      "layer4.0.conv2.weight            0.00254584  |  layer3.5.conv2.weight            0.00019875\n",
      "regressionModel.output.weight    0.00245550  |  layer3.4.conv3.weight            0.00019771\n",
      "layer4.1.conv3.weight            0.00241819  |  layer3.5.conv3.weight            0.00019535\n",
      "layer4.1.conv2.weight            0.00241811  |  layer3.4.conv2.weight            0.00019422\n",
      "layer4.0.downsample.0.weight     0.00236487  |  layer4.2.bn2.weight              0.00019353\n",
      "layer4.0.conv3.weight            0.00215166  |  layer3.0.conv2.weight            0.00019334\n",
      "layer4.2.bn3.weight              0.00214498  |  layer4.1.conv1.weight            0.00019261\n",
      "layer4.0.conv1.weight            0.00212653  |  layer3.3.conv2.weight            0.00019255\n",
      "layer4.2.conv1.weight            0.00201478  |  layer3.5.conv1.weight            0.00019202\n",
      "layer3.4.conv3.weight            0.00197879  |  layer3.1.conv3.weight            0.00019198\n",
      "layer3.5.conv3.weight            0.00195966  |  layer4.1.bn3.weight              0.00019119\n",
      "layer3.0.conv2.weight            0.00193861  |  layer3.4.conv1.weight            0.00019060\n",
      "layer3.5.conv2.weight            0.00190276  |  layer3.3.conv3.weight            0.00019030\n",
      "layer4.1.conv1.weight            0.00189932  |  layer3.2.conv3.weight            0.00018927\n",
      "layer3.1.conv3.weight            0.00187260  |  layer3.3.conv1.weight            0.00018880\n",
      "layer3.5.conv1.weight            0.00185969  |  regressionModel.output.bias      0.00018498\n",
      "layer3.3.conv3.weight            0.00185393  |  layer3.2.conv1.weight            0.00018447\n",
      "layer3.2.conv3.weight            0.00183781  |  layer3.1.bn3.weight              0.00018442\n",
      "layer3.4.conv2.weight            0.00180836  |  layer4.2.conv1.weight            0.00018292\n",
      "layer3.3.conv2.weight            0.00180259  |  layer3.2.conv2.weight            0.00018136\n",
      "layer3.4.conv1.weight            0.00179238  |  layer2.1.bn3.weight              0.00017905\n",
      "layer3.3.conv1.weight            0.00177189  |  layer3.5.bn3.weight              0.00017861\n",
      "layer3.2.conv1.weight            0.00176551  |  layer2.3.conv1.weight            0.00017832\n",
      "layer4.2.bn3.bias                0.00169031  |  layer3.1.conv2.weight            0.00017743\n",
      "layer3.0.downsample.0.weight     0.00168961  |  layer3.0.downsample.0.weight     0.00017518\n",
      "layer2.3.conv1.weight            0.00166587  |  layer3.1.conv1.weight            0.00017445\n",
      "layer3.2.conv2.weight            0.00166124  |  layer3.4.bn3.weight              0.00017052\n",
      "layer4.1.bn3.weight              0.00162810  |  layer2.2.conv1.weight            0.00016723\n",
      "layer3.1.conv2.weight            0.00159643  |  layer2.0.conv2.weight            0.00016381\n",
      "layer4.2.bn2.weight              0.00159086  |  layer3.3.bn3.weight              0.00016335\n",
      "layer3.1.conv1.weight            0.00158272  |  layer2.3.conv3.weight            0.00016251\n",
      "layer2.0.conv2.weight            0.00156919  |  layer3.2.bn3.weight              0.00016246\n",
      "layer2.3.conv3.weight            0.00156158  |  layer4.0.downsample.1.weight     0.00016150\n",
      "layer2.1.bn3.weight              0.00153708  |  layer2.2.conv2.weight            0.00016103\n",
      "layer2.1.conv3.weight            0.00152775  |  layer3.0.conv3.weight            0.00016091\n",
      "layer2.2.conv1.weight            0.00149957  |  layer2.1.conv3.weight            0.00016075\n",
      "layer2.2.conv3.weight            0.00148406  |  layer2.2.conv3.weight            0.00016054\n",
      "layer3.0.conv3.weight            0.00148235  |  layer2.3.conv2.weight            0.00015692\n",
      "layer2.2.conv2.weight            0.00146433  |  layer2.1.conv1.weight            0.00015656\n",
      "layer3.1.bn3.weight              0.00145985  |  layer2.1.bn1.weight              0.00015631\n",
      "layer4.0.downsample.1.weight     0.00144428  |  layer1.1.conv2.weight            0.00015536\n",
      "layer1.2.conv3.weight            0.00144198  |  layer2.3.bn3.weight              0.00015444\n",
      "layer2.3.conv2.weight            0.00143629  |  layer2.2.bn3.weight              0.00015391\n",
      "layer3.4.bn3.weight              0.00141701  |  layer4.0.bn3.weight              0.00015244\n",
      "layer2.3.bn3.weight              0.00140436  |  layer1.2.conv3.weight            0.00015177\n",
      "layer2.2.bn3.weight              0.00138363  |  layer1.1.conv3.weight            0.00015161\n",
      "layer2.1.conv1.weight            0.00138268  |  layer3.0.conv1.weight            0.00014968\n",
      "layer3.2.bn3.weight              0.00138218  |  layer2.1.conv2.weight            0.00014902\n",
      "layer3.0.conv1.weight            0.00138194  |  layer1.2.bn3.weight              0.00014801\n",
      "layer4.0.bn3.weight              0.00134411  |  layer1.2.conv1.weight            0.00014800\n",
      "layer3.3.bn3.weight              0.00133676  |  layer1.1.bn3.weight              0.00014675\n",
      "layer2.0.conv3.weight            0.00132967  |  layer1.2.conv2.weight            0.00014318\n",
      "layer3.5.bn3.weight              0.00131543  |  layer2.2.bn1.weight              0.00014065\n",
      "layer1.1.conv2.weight            0.00130336  |  layer2.0.conv3.weight            0.00013994\n",
      "layer1.1.conv3.weight            0.00127769  |  layer2.3.bn1.weight              0.00013856\n",
      "layer1.2.conv2.weight            0.00126403  |  layer2.1.bn2.weight              0.00013781\n",
      "layer1.2.conv1.weight            0.00125173  |  layer4.2.bn3.bias                0.00013661\n",
      "layer2.1.conv2.weight            0.00121789  |  layer1.1.bn1.weight              0.00013647\n",
      "layer1.2.bn3.weight              0.00120788  |  layer3.0.downsample.1.weight     0.00013266\n",
      "layer3.0.downsample.1.weight     0.00112114  |  layer3.1.bn1.weight              0.00012887\n",
      "layer2.0.downsample.0.weight     0.00106524  |  fpn.P3_2.bias                    0.00012520\n",
      "layer2.0.bn3.weight              0.00105088  |  layer2.0.downsample.0.weight     0.00012402\n",
      "layer1.1.bn3.weight              0.00103874  |  layer4.1.bn3.bias                0.00012270\n",
      "layer2.1.bn1.weight              0.00103230  |  layer1.1.conv1.weight            0.00012241\n",
      "layer2.1.bn2.weight              0.00100881  |  layer2.0.conv1.weight            0.00012209\n",
      "layer2.0.conv1.weight            0.00100699  |  fpn.P4_2.bias                    0.00012182\n"
     ]
    }
   ],
   "source": [
    "print_rank_comp(diffs_for_15, diffs_for_cur_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:40:08.871798Z",
     "start_time": "2021-07-04T02:40:08.836225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv3.weight 1.820434022192785e-06\n",
      "layer2.1.bn1.weight 1.8592998003441608e-06\n",
      "layer1.1.bn3.weight 2.0318209408287657e-06\n",
      "layer1.1.conv1.weight 2.188258349633543e-06\n",
      "layer3.0.downsample.1.weight 2.19436401494022e-06\n",
      "layer2.0.downsample.0.weight 2.267564241265063e-06\n",
      "layer2.0.bn3.weight 2.3382508516078815e-06\n",
      "layer1.2.bn3.weight 2.556528443165007e-06\n",
      "layer1.2.conv2.weight 2.8052418201696128e-06\n",
      "layer2.1.conv2.weight 2.8120136903453385e-06\n",
      "layer1.2.conv1.weight 2.8213712539582048e-06\n",
      "layer4.0.bn3.weight 2.9196075956861023e-06\n",
      "layer1.1.conv3.weight 2.9213504149083747e-06\n",
      "layer3.5.bn3.weight 2.958079676318448e-06\n",
      "layer3.3.bn3.weight 3.0446065011346946e-06\n",
      "layer1.1.conv2.weight 3.0847247671772493e-06\n",
      "layer2.2.bn3.weight 3.166517217323417e-06\n",
      "layer3.0.conv1.weight 3.1765207495482173e-06\n",
      "layer3.2.bn3.weight 3.2613720577501226e-06\n",
      "layer3.4.bn3.weight 3.325446868984727e-06\n",
      "layer2.3.bn3.weight 3.328600541863125e-06\n",
      "layer2.0.conv3.weight 3.4086483537976164e-06\n",
      "layer2.3.conv2.weight 3.4101237815775676e-06\n",
      "layer4.0.downsample.1.weight 3.4700435662671225e-06\n",
      "layer1.2.conv3.weight 3.4746533401630586e-06\n",
      "layer2.2.conv2.weight 3.5954856230091536e-06\n",
      "layer3.1.bn3.weight 3.6099991120863706e-06\n",
      "layer2.1.conv1.weight 3.6214696592651308e-06\n",
      "layer2.2.conv3.weight 3.70081852452131e-06\n",
      "layer2.2.conv1.weight 3.7752865864604246e-06\n",
      "layer3.0.conv3.weight 3.820722668024246e-06\n",
      "layer2.1.bn3.weight 4.009641543234466e-06\n",
      "layer2.3.conv3.weight 4.033327059005387e-06\n",
      "layer2.1.conv3.weight 4.035451638628729e-06\n",
      "layer2.0.conv2.weight 4.144656486460008e-06\n",
      "layer3.1.conv2.weight 4.201503998046974e-06\n",
      "layer3.1.conv1.weight 4.2581441448419355e-06\n",
      "layer4.2.bn2.weight 4.266520591045264e-06\n",
      "layer4.1.bn3.weight 4.4148446249892e-06\n",
      "layer4.2.bn3.bias 4.470451585802948e-06\n",
      "layer2.3.conv1.weight 4.515783075476065e-06\n",
      "layer3.2.conv2.weight 4.534892013907665e-06\n",
      "layer3.0.downsample.0.weight 4.8874394451559056e-06\n",
      "layer3.2.conv1.weight 5.1101351346005686e-06\n",
      "layer3.3.conv1.weight 5.117498403706122e-06\n",
      "layer3.4.conv1.weight 5.225198947300669e-06\n",
      "layer3.3.conv2.weight 5.34208447788842e-06\n",
      "layer3.4.conv2.weight 5.428875283541856e-06\n",
      "layer3.2.conv3.weight 5.5424093261535745e-06\n",
      "layer3.5.conv1.weight 5.6228336688946e-06\n",
      "layer3.3.conv3.weight 5.635944489768008e-06\n",
      "layer3.1.conv3.weight 5.80287269258406e-06\n",
      "layer4.1.conv1.weight 5.884871370653855e-06\n",
      "layer3.5.conv2.weight 5.9934645832981914e-06\n",
      "layer3.0.conv2.weight 6.05064678893541e-06\n",
      "layer3.5.conv3.weight 6.304499038378708e-06\n",
      "layer3.4.conv3.weight 6.427442258427618e-06\n",
      "layer4.2.conv1.weight 6.642402695433702e-06\n",
      "layer4.0.conv1.weight 7.3513556344551034e-06\n",
      "layer4.0.conv3.weight 7.470504442608217e-06\n",
      "layer4.2.bn3.weight 7.502805601689033e-06\n",
      "layer4.0.downsample.0.weight 9.094601409742609e-06\n",
      "layer4.1.conv2.weight 9.513307304587215e-06\n",
      "layer4.1.conv3.weight 9.513847544440068e-06\n",
      "layer4.0.conv2.weight 1.034640536090592e-05\n",
      "layer4.2.conv3.weight 1.0582959475868847e-05\n",
      "regressionModel.output.weight 1.1568749869184103e-05\n",
      "layer4.2.conv2.weight 1.3280425264383666e-05\n",
      "fpn.P6.bias 4.2541818402241915e-05\n",
      "fpn.P7_2.bias 0.00025631801690906286\n",
      "fpn.P3_2.bias 0.00027207870152778924\n",
      "classificationModel.conv3.bias 0.0002875125501304865\n",
      "fpn.P4_2.bias 0.00028981672949157655\n",
      "regressionModel.conv4.bias 0.0002996775147039443\n",
      "classificationModel.conv4.bias 0.00031027154182083905\n",
      "classificationModel.conv2.bias 0.00031202432001009583\n",
      "regressionModel.conv1.bias 0.00032020878279581666\n",
      "classificationModel.conv1.bias 0.0003295135102234781\n",
      "regressionModel.conv2.bias 0.0003316439688205719\n",
      "fpn.P5_2.bias 0.0003344690485391766\n",
      "regressionModel.conv3.bias 0.0003581053751986474\n",
      "fpn.P5_1.bias 0.0003944335039705038\n",
      "fpn.P4_1.bias 0.0006627438706345856\n",
      "fpn.P3_1.bias 0.001282719080336392\n",
      "regressionModel.conv1.weight 0.0017370304558426142\n",
      "fpn.P5_2.weight 0.001739718485623598\n",
      "fpn.P4_2.weight 0.0017425032565370202\n",
      "regressionModel.conv2.weight 0.0017446652054786682\n",
      "regressionModel.conv4.weight 0.0017454760381951928\n",
      "regressionModel.conv3.weight 0.001745512941852212\n",
      "fpn.P3_2.weight 0.0017479482339695096\n",
      "fpn.P6.weight 0.0017513082129880786\n",
      "fpn.P7_2.weight 0.0017547764582559466\n",
      "classificationModel.conv2.weight 0.0017600585706532001\n",
      "classificationModel.conv3.weight 0.0017613170202821493\n",
      "classificationModel.conv4.weight 0.0017657370772212744\n",
      "classificationModel.conv1.weight 0.0017737721791490912\n",
      "fpn.P5_1.weight 0.015619486570358276\n",
      "fpn.P4_1.weight 0.015759918838739395\n",
      "fpn.P3_1.weight 0.01576324738562107\n"
     ]
    }
   ],
   "source": [
    "print_rank(diffs_for_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算所有data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:43:56.995758Z",
     "start_time": "2021-05-27T13:35:11.543629Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "retinanet.train()\n",
    "retinanet.freeze_bn()\n",
    "\n",
    "dataset = dataset_train\n",
    "\n",
    "fail_id = []\n",
    "losses = collections.defaultdict(list)\n",
    "\n",
    "for idx, data in enumerate(dataset):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.device(0):\n",
    "            if torch.cuda.is_available():\n",
    "                classification_loss, regression_loss = retinanet([data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0), data['annot'].cuda().unsqueeze(dim=0)])\n",
    "            else:\n",
    "                print('not have gpu')\n",
    "                break\n",
    "\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            img_id = dataset.image_ids[idx]\n",
    "\n",
    "\n",
    "            classification_loss = float(classification_loss)\n",
    "            regression_loss = float(regression_loss)\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            losses[img_id] = [classification_loss, regression_loss, loss]\n",
    "\n",
    "            #optimizer.step()\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            #epoch_loss.append(float(loss))\n",
    "            end = time.time()\n",
    "\n",
    "            print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(\n",
    "                                              '50', \n",
    "                                              idx, \n",
    "                                              float(classification_loss), \n",
    "                                              float(regression_loss), \n",
    "                                              np.mean(loss_hist),\n",
    "                                              end - start))\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        fail_id.append(idx)\n",
    "        continue\n",
    "\n",
    "print('fail_id:',fail_id)\n",
    "\n",
    "\n",
    "with open(os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50).split('/')[:-1]), 'losses.pickle'), 'wb') as f:\n",
    "    pickle.dump(losses, f)\n",
    "#print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:13:50.683846Z",
     "start_time": "2021-06-06T15:13:50.595229Z"
    }
   },
   "outputs": [],
   "source": [
    "img = dataset_train.load_image(74)\n",
    "ann = dataset_train.load_annotations(74)\n",
    "data = {'img': img, 'annot': ann}\n",
    "data = dataset_train.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:16:08.944527Z",
     "start_time": "2021-06-06T15:16:08.937293Z"
    }
   },
   "outputs": [],
   "source": [
    "data['annot'].cuda().unsque`eze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:12:23.923486Z",
     "start_time": "2021-06-06T15:12:23.916190Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train.cocoHelper.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:13:55.381660Z",
     "start_time": "2021-06-06T15:13:55.315557Z"
    }
   },
   "outputs": [],
   "source": [
    "retinanet.each_cat_loss = True\n",
    "classification_loss, _ = retinanet([data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0), data['annot'].cuda().unsqueeze(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T15:11:29.945048Z",
     "start_time": "2021-06-06T15:11:29.936988Z"
    }
   },
   "outputs": [],
   "source": [
    "classification_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分別為每個類別計算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T14:46:42.703754Z",
     "start_time": "2021-06-06T14:37:59.732942Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "retinanet.train()\n",
    "retinanet.freeze_bn()\n",
    "\n",
    "dataset = dataset_train\n",
    "\n",
    "fail_id = []\n",
    "losses = [collections.defaultdict() for _ in dataset.seen_class_id]\n",
    "\n",
    "retinanet.each_cat_loss = True\n",
    "for idx, data in enumerate(dataset):\n",
    "\n",
    "    try:\n",
    "        with torch.cuda.device(0):\n",
    "            if torch.cuda.is_available():\n",
    "                classification_loss, _ = retinanet([data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0), data['annot'].cuda().unsqueeze(dim=0)])\n",
    "            else:\n",
    "                print('not have gpu')\n",
    "                break\n",
    "\n",
    "\n",
    "            img_id = dataset.image_ids[idx]\n",
    "\n",
    "            for key in classification_loss.keys():\n",
    "                losses[key][img_id] = float(np.mean(classification_loss[key]))\n",
    "\n",
    "            print(idx)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        fail_id.append(idx)\n",
    "        continue\n",
    "\n",
    "print('fail_id:',fail_id)\n",
    "\n",
    "\n",
    "with open(os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50,data_split).split('/')[:-1]), 'losses_each_cat_new.pickle'), 'wb') as f:\n",
    "    pickle.dump(losses, f)\n",
    "#print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T14:36:16.411626Z",
     "start_time": "2021-06-06T14:36:16.405118Z"
    }
   },
   "outputs": [],
   "source": [
    "classification_loss.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T05:34:58.264517Z",
     "start_time": "2021-06-06T05:34:58.209957Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50,data_split).split('/')[:-1]), 'losses_each_cat.pickle'), 'rb') as f:\n",
    "    losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T05:35:24.814800Z",
     "start_time": "2021-06-06T05:35:24.807388Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in losses:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:55:41.434110Z",
     "start_time": "2021-05-27T13:55:41.348495Z"
    }
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:48:23.556669Z",
     "start_time": "2021-05-27T13:48:23.548848Z"
    }
   },
   "outputs": [],
   "source": [
    "os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50).split('/')[:-1]), 'losses.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:58:00.745982Z",
     "start_time": "2021-05-27T13:58:00.740224Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = [v[2] for v in losses.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:58:20.940278Z",
     "start_time": "2021-05-27T13:58:20.917729Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:10:11.360151Z",
     "start_time": "2021-04-06T18:10:11.054507Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'w_distillation'\n",
    "\n",
    "for i in range(50,51,10):\n",
    "    print(np.mean(readCheckpoint(method, 1, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:09:05.086275Z",
     "start_time": "2021-04-06T18:09:04.282162Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'incremental'\n",
    "\n",
    "for i in range(10,31,10):\n",
    "    print(np.mean(readCheckpoint(method, 1, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:05:13.442587Z",
     "start_time": "2021-04-06T18:05:12.170534Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'incremental'\n",
    "\n",
    "for i in range(10,51,10):\n",
    "    print(np.mean(readCheckpoint(method, 0, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(readCheckpoint(method, 0, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:32:59.345393Z",
     "start_time": "2021-04-06T04:32:59.339106Z"
    }
   },
   "outputs": [],
   "source": [
    "parts = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    parts.append(retinanet.classificationModel.output.weight.data[i*9: i*9 + 9,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:33:01.611318Z",
     "start_time": "2021-04-06T04:33:01.601089Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "output = nn.Conv2d(256, 9 * 20, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:36:24.690145Z",
     "start_time": "2021-04-06T04:36:24.679573Z"
    }
   },
   "outputs": [],
   "source": [
    "output.weight.data[0:9,:,:,:] = parts[3]\n",
    "#vehicle(7)\n",
    "for i in range(0,7):\n",
    "    output.weight.data[9 + i*9:9 + i*9 + 9,:,:,:] = parts[0]\n",
    "#furniture(6)\n",
    "for i in range(0,6):\n",
    "    output.weight.data[72 + i*9:72 + i*9 + 9,:,:,:] = parts[1]\n",
    "#animals(6)\n",
    "for i in range(0,6):\n",
    "    output.weight.data[126 + i*9:126 + i*9 + 9,:,:,:] = parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:36:53.559211Z",
     "start_time": "2021-04-06T04:36:53.550539Z"
    }
   },
   "outputs": [],
   "source": [
    "(output.weight.data[72:81,:,:,:] == output.weight.data[81:90,:,:,:]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:31.511361Z",
     "start_time": "2021-03-16T17:29:31.372639Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "prev_model = copy.deepcopy(retinanet)\n",
    "retinanet.increase_class(1)\n",
    "\n",
    "retinanet.cuda()\n",
    "prev_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:38.239997Z",
     "start_time": "2021-03-16T17:29:38.234111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.ones(2,256,30,30).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:39.297163Z",
     "start_time": "2021-03-16T17:29:39.286858Z"
    }
   },
   "outputs": [],
   "source": [
    "prev_out = prev_model.classificationModel.output_act(prev_model.classificationModel.output(test))\n",
    "cur_out = retinanet.classificationModel.output_act(retinanet.classificationModel.output(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:52.746312Z",
     "start_time": "2021-03-16T17:29:52.734269Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_shape1(out, num_classes):\n",
    "    out1 = out.permute(0, 2, 3, 1)\n",
    "    batch_size, width, height, channels = out1.shape\n",
    "    out1 = out1.view(batch_size, width, height, 9, num_classes)\n",
    "    \n",
    "    return out1.contiguous().view(2, -1, num_classes)\n",
    "def change_shape2(out, num_classes):\n",
    "    out1 = out.permute(0, 2, 3, 1)\n",
    "    batch_size, width, height, channels = out1.shape\n",
    "    out1 = out1.view(batch_size, width, height,num_classes, 9)\n",
    "    \n",
    "    out1 = out1.permute(0, 1, 2, 4, 3)\n",
    "    \n",
    "    return out1.contiguous().view(2, -1, num_classes)\n",
    "prev_out_new1 = change_shape1(prev_out, 19)\n",
    "cur_out_new1 = change_shape1(cur_out, 20)\n",
    "\n",
    "# prev_out_new2 = change_shape2(prev_out, 19)\n",
    "# cur_out_new2 = change_shape2(cur_out, 20)\n",
    "# (prev_out == cur_out[:,:171,:,:]).any()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:shiang]",
   "language": "python",
   "name": "conda-env-shiang-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
