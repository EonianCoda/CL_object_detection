# #enable A_GEM
# enable_agem = False

# root_dir = ''
# method = 'w_distillation'
# w_distillation = True
# data_split = ""

# # rehearsal
# rehearsal_method = None
# replay_per_num = 2

# mas_penalty = False

# # enhance new taks on old picture's loss
# enhance_error = False

# # when calculate new task'focal lossï¼Œthe ground truth label 1 -> 0.9
# decrease_positive = False

# # warm-up
# enable_warm_up = True
# warm_up_epoch = 10
# enable_warm_up2 = True
# warm_up_epoch2 = 10
# enable_warm_up3 = False
# warm_up_epoch3 = 10


        ################################
        #            A-GEM             #           
        ################################
#         #update rehearsal dataset
#         if cur_task > 1:
#             if dataset_replay.cur_task == None:
#                 if len(dataset_replay.image_ids) == 0:
#                     dataset_replay.reset_by_round(cur_task)
#                 else:
#                     dataset_replay.cur_task = cur_task
#             if enable_agem:
#                 print("create agem")
#                 agem = a_gem.A_GEM(retinanet, dataset_replay, parser.batch_size)
        
        
        for epoch_num in range(parser.start_epoch , parser.end_epoch + 1):
            
            # unfreeze classification/regression subnet, then freeze restnet+fpn
            if enable_warm_up and epoch_num == warm_up_epoch + 1:
#                 for g in optimizer.param_groups:
#                     g['lr'] = 1e-6
                retinanet.freeze_except_new_classification(True)
                if enable_warm_up2:
#                     retinanet.freeze_regression(False)
                    retinanet.freeze_resnet(False)
#                     retinanet.freeze_resnet_n_fpn(False)
                retinanet.freeze_bn()
            # unfreeze resnet+fpn, then freeze restnet
            if epoch_num == warm_up_epoch + warm_up_epoch2 + 1:
#                 for g in optimizer.param_groups:
#                     g['lr'] = 1e-7
#                 retinanet.freeze_regression(True)
                retinanet.freeze_resnet(True)
#                 retinanet.freeze_resnet_n_fpn(True)
#                 retinanet.freeze_resnet(False)
                retinanet.freeze_bn()
#                 if enable_warm_up3:

#                 retinanet.freeze_bn()    
                
#             # unfreeze all layer
#             if enable_warm_up3 and epoch_num == warm_up_epoch + warm_up_epoch2 + warm_up_epoch3 + 1:
#                 retinanet.freeze_resnet(True)
#                 retinanet.freeze_bn()
            
            
            epoch_loss = []
            retinanet.decrease_positive = decrease_positive
#             retinanet.enhance_error = enhance_error

            for iter_num, data in enumerate(dataloader_train):
                if enable_warm_up and epoch_num >= warm_up_epoch + 1 and enable_agem:
                    agem.cal_replay_grad(optimizer)
                    
                start = time.time()
                #try:
                optimizer.zero_grad()
                with torch.cuda.device(0):
                    if torch.cuda.is_available():
                        losses = retinanet([data['img'].float().cuda(), data['annot'].cuda()])
                        if retinanet.enhance_error and retinanet.distill_loss:
                            classification_loss, regression_loss, dist_class_loss, dist_reg_loss, dist_feat_loss, enhance_loss = losses
                        elif retinanet.distill_loss:
                            classification_loss, regression_loss, dist_class_loss, dist_reg_loss, dist_feat_loss = losses
                        else:
                            classification_loss, regression_loss = losses
                    else:
                        print('not have gpu')
                        return

                    classification_loss = classification_loss.mean()
                    regression_loss = regression_loss.mean()
                    loss = classification_loss + regression_loss

                    #add distillation loss
                    if retinanet.distill_loss:
                        loss += dist_class_loss + dist_reg_loss + dist_feat_loss
                        #mas penalty
                        if mas_penalty:
                            mas_loss = mas.penalty(retinanet)
                            loss += mas_loss
                        if enable_warm_up and cur_task > 1 and epoch_num > warm_up_epoch:
                            if retinanet.enhance_error:
                                if enhance_loss != None:
                                    loss += enhance_loss
                                else:
                                    enhance_loss = 0
                        else:
                            enhance_loss = 0

                    if bool(loss == 0):
                        continue

                    loss.backward()

                    # Eliminate old task grad in 1-stage warm-up
                    if enable_warm_up and cur_task > 1 and epoch_num <= warm_up_epoch:
                        for i in range(retinanet.classificationModel.num_anchors):
                            retinanet.classificationModel.output.weight.grad[i * retinanet.num_classes : i * retinanet.num_classes + retinanet.prev_num_classes,:,:,:] = 0
                            retinanet.classificationModel.output.bias.grad[i * retinanet.num_classes : i * retinanet.num_classes + retinanet.prev_num_classes] =  0


                    #######################
                    #   A-GEM proj grad   #
                    #######################
                    if enable_agem and enable_warm_up and epoch_num >= warm_up_epoch + 1:
                        agem.fix_grad()


                    torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)

                    ####################
                    #   special clip   #
                    ####################
#                         if cur_task > 1 and (not enable_warm_up or (enable_warm_up and epoch_num > warm_up_epoch)):
#                             for name, p in retinanet.named_parameters():
#                                 if "prev_model" not in name and "bn" not in name and p.requires_grad:
#                                     temp = p.abs()
#                                     grad_std = torch.std(temp)
#                                     grad_mean = torch.mean(temp)
#                                     threshold = grad_mean - 1.5 * grad_std

# #                                     temp = p.abs()
# #                                     threshold = torch.quantile(temp, 0.001)
# #                                     print((temp < threshold).sum())
#                                     p.grad[temp < threshold] = 0

                    optimizer.step()
                    loss_hist.append(float(loss))

                    epoch_loss.append(float(loss))
                    end = time.time()


                    info = [epoch_num, iter_num, float(classification_loss), float(regression_loss)]
                    output = 'Epoch: {0[0]} | Iteration: {0[1]} | Classification loss: {0[2]:1.5f} | Regression loss: {0[3]:1.5f}'

                    # distillation loss
                    if retinanet.distill_loss:
                        output += ' | dis_class_loss: {0[4]:1.4f} | dis_reg_loss: {0[5]:1.4f} | dis_feat_loss: {0[6]:1.4f}'
                        info.extend([float(dist_class_loss), float(dist_reg_loss), float(dist_feat_loss)])
                    # Mas loss
                    if mas_penalty:
                        output += ' | mas_loss: {0[%d]:1.4f}' % (len(info))
                        info.append(float(mas_loss))

                    #enhance_loss
                    if retinanet.enhance_error:
                        output += ' | enhance_loss: {0[%d]:1.4f}' % (len(info))
                        info.append(float(enhance_loss))

                    output += ' | Running loss: {0[%d]:1.5f} | Spend Time:{0[%d]:1.2f}s' % (len(info), len(info) + 1)
                    info.extend([np.mean(loss_hist), end - start])
                    print(output.format(info))

#                         if not retinanet.distill_loss:
#                             print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist),end - start))

#                         elif not mas_penalty:
#                             if enhance_error:
#                                  print('Epoch: {} | Iteration: {} | Class loss: {:1.4f} | Reg loss: {:1.4f} | dis_class_loss: {:1.4f} | dis_reg_loss: {:1.4f} | dis_feat_loss: {:1.4f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), float(dist_class_loss), float(dist_reg_loss), float(dist_feat_loss), np.mean(loss_hist),end - start))
#                             else:
#                                 print('Epoch: {} | Iteration: {} | Class loss: {:1.4f} | Reg loss: {:1.4f} | dis_class_loss: {:1.4f} | dis_reg_loss: {:1.4f} | dis_feat_loss: {:1.4f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), float(dist_class_loss), float(dist_reg_loss), float(dist_feat_loss), np.mean(loss_hist),end - start))
#                         else:
#                             print('Epoch: {} | Iteration: {} | Class loss: {:1.4f} | Reg loss: {:1.4f} | dis_class_loss: {:1.4f} | dis_reg_loss: {:1.4f} | dis_feat_loss: {:1.4f} | mas_loss: {:1.4f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), float(dist_class_loss), float(dist_reg_loss), float(dist_feat_loss),float(mas_loss), np.mean(loss_hist),end - start))


                    del classification_loss
                    del regression_loss
                    if retinanet.distill_loss:
                        del dist_class_loss, dist_reg_loss, dist_feat_loss
#                 except Exception as e:
#                     print(e)
#                     continue
            retinanet.enhance_error = False
            retinanet.decrease_positive = False
            if rehearsal_method != None and cur_task != 1:

                if dataset_replay.cur_task == None:
                    if len(dataset_replay.image_ids) == 0:
                        dataset_replay.reset_by_round(cur_task)
                    else:
                        dataset_replay.cur_task = cur_task
                
                if dataloader_rehearsal == None:
                    sampler = AspectRatioBasedSampler(dataset_replay, batch_size = 5, drop_last=False)
                    dataloader_rehearsal = DataLoader(dataset_replay, num_workers=2, collate_fn=collater, batch_sampler=sampler)  
                prev_status = retinanet.distill_loss
                retinanet.distill_loss = False
                
                retinanet.enhance_error = enhance_error
                print('Rehearsal start!')
                
                for iter_num, data in enumerate(dataloader_rehearsal):
                    if enable_warm_up and (epoch_num <= warm_up_epoch):
                        break
#                     if enable_warm_up2 and epoch_num <= warm_up_epoch+warm_up_epoch2 + 1:
#                         break
                    start = time.time()
                    try:
                        optimizer.zero_grad()
                        with torch.cuda.device(0):
                            if torch.cuda.is_available():

                                losses = retinanet([data['img'].float().cuda(), data['annot'].cuda()])
                                if not enhance_error:
                                    classification_loss, regression_loss = losses
                                else:
                                    classification_loss, regression_loss, enhance_loss = losses
                            else:
                                print('not have gpu')
                                return

                            classification_loss = classification_loss.mean()
                            regression_loss = regression_loss.mean()
                            loss = classification_loss + regression_loss
                            if enhance_error:
                                loss += enhance_loss
                            if bool(loss == 0):
                                continue

    #                             if loss <= np.mean(loss_hist) / 2:
    #                                 ratio = ((np.mean(loss_hist) / 2) / loss)
    #                                 classification_loss *= ratio
    #                                 regression_loss *= ratio
    #                                 loss = classification_loss + regression_loss
                            loss.backward()
                            torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)
                            optimizer.step()
                            loss_hist.append(float(loss))
                            epoch_loss.append(float(loss))
                            end = time.time()
                            if not enhance_error:
                                print('Rehearsal Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist),end - start))
                            else:
                                print('Rehearsal Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Enhance loss: {:1.5f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss),float(enhance_loss), np.mean(loss_hist),end - start))
                            del classification_loss
                            del regression_loss

                    except Exception as e:
                        print(e)
                        continue
                
            
                retinanet.distill_loss = prev_status
                retinanet.enhance_error = False
            
            #scheduler.step(np.mean(epoch_loss))
            savePath = get_checkpoint_path(method, cur_task, epoch_num)
            #if thd directory doesn't exist, then create it
            dirPath = '/'.join(savePath.split('/')[:-1])
            checkDir(dirPath)
            
            #save checkpoint
            if retinanet.prev_model != None:
                prev_model = retinanet.prev_model
                retinanet.prev_model = None
                
                
            
            if rehearsal_method == None:
                torch.save({
                    'epoch': epoch_num,
                    'model_state_dict': retinanet.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': epoch_loss,
                    }, savePath)
            else:
                torch.save({
                    'epoch': epoch_num,
                    'model_state_dict': retinanet.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': epoch_loss,
                    'replay_sample_imgs': dataset_replay.image_ids,
                    'replay_per_num': dataset_replay.per_num
                    }, savePath)
            
            
            retinanet.prev_model = prev_model
            if epoch_num % 5 == 0:
                autoDelete(os.path.join(root_dir, 'model', method), cur_task, epoch_num)
        

        if cur_task == parser.end_task:
            #validation(retinanet, 'Val', 2, 50, 1)
            break
        
        #add new neuron on the classification subnet
        origin_num = dataset_train.num_classes()
        dataset_train.next_round()
        new_class_num = dataset_train.num_classes() - origin_num
        
        retinanet.increase_class(new_class_num, w_distillation)
        print('new classes num is {}'.format(new_class_num))

        #reset optimizer and scheduler
        prev_model = retinanet.prev_model
        retinanet.prev_model = None

    
        optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)
        retinanet.prev_model = prev_model
        
        if rehearsal_method != None and dataset_replay.cur_task != None and ((cur_task == 1) ^ (len(dataset_replay.image_ids) != 0)):
            dataset_replay.next_round()
            sampler = AspectRatioBasedSampler(dataset_replay, batch_size = 3, drop_last=False)
            dataloader_rehearsal = DataLoader(dataset_replay, num_workers=2, collate_fn=collater, batch_sampler=sampler)
       
        #MAS
        if mas_penalty:
            mas = MAS.MAS(retinanet, dataset_train)
            mas.load_importance(os.path.join(root_dir, 'model', method, 'round{}'.format(cur_task), data_split))
        #change the training data
        sampler = AspectRatioBasedSampler(dataset_train, batch_size = parser.batch_size, drop_last=False)
        dataloader_train = DataLoader(dataset_train, num_workers=2, collate_fn=collater, batch_sampler=sampler)




def cal_loss(self, img_batch, annotations, ):
        """ calculate the loss of model
            Args"
                img_batch: tensor, shape = (batch_size, channel, height, width)
                annotations: the annotations of img_batch, shape = (batch_size, ground_truth_num, 5)
        """

        if self.training == False:
            raise RuntimeError("This model is not trainable!")

        if 

        if self.distill_feature:
            return (features, regression, classification)
        elif self.training:
            
            #no distill loss
            if not self.distill_loss:
                if self.prev_model:
                    if not self.enhance_error:
                        class_loss, reg_loss = self.focalLoss(classification, regression, anchors, annotations, special_alpha=self.special_alpha, enhance_error=self.enhance_error, pre_class_num=self.prev_model.num_classes, each_cat_loss = self.each_cat_loss)
                    else:
                        class_loss, reg_loss, enhance_loss = self.focalLoss(classification, regression, anchors, annotations, special_alpha=self.special_alpha, enhance_error=self.enhance_error, pre_class_num=self.prev_model.num_classes, each_cat_loss = self.each_cat_loss)
                else:
                    class_loss, reg_loss = self.focalLoss(classification, regression, anchors, annotations, special_alpha=self.special_alpha, enhance_error=self.enhance_error, each_cat_loss = self.each_cat_loss)
                    
                if not self.enhance_error:
                    return (class_loss, reg_loss)
                else:
                    return (class_loss, reg_loss, enhance_loss)
            #use distill loss
            else: 
                assert self.prev_model != None
                self.prev_model.eval()
                self.prev_model.training = False
                
                #use label
                if self.classificationModel.enable_act:
                        losses = self.focalLoss(classification, regression, anchors, annotations, self.distill_loss, self.prev_model.num_classes, special_alpha=self.special_alpha, decrease_positive=self.decrease_positive, decrease_new=True, enhance_error=self.enhance_error)
                        
                #use logits
                else:
                    classification_label = nn.Sigmoid()(classification)
                    losses = self.focalLoss(classification_label, regression, anchors, annotations, self.distill_loss, self.prev_model.num_classes, special_alpha=self.special_alpha, decrease_positive=self.decrease_positive, decrease_new=True, enhance_error=self.enhance_error)
                    
 
                dist_class_loss = (torch.pow(prev_classification - classification[:,:,:self.prev_model.num_classes], 2) * old_label).sum() / (old_label > 0.5).sum()
                #dist_class_loss /= greater.sum
                #dist_class_loss = nn.MSELoss()(prev_classification[greater], classification[:,:,:self.prev_model.num_classes][greater])
                dist_reg_loss = smoothL1Loss(prev_regression[greater.any(dim=2)], regression[greater.any(dim=2)])
                
    
                #dist_class_loss = smoothL1Loss(prev_classification, classification[:,:,:self.prev_model.num_classes])
        
        
#                 dist_class_loss = nn.MSELoss()(prev_classification, classification[:,:,:self.prev_model.num_classes])
#                 dist_reg_loss = smoothL1Loss(prev_regression, regression)

                
                ################################
                #          Ignore GD           #           
                ################################
#                 negative = torch.flatten(negative)

# #                 dist_class_loss = nn.MSELoss(reduction='sum')(prev_classification.view(-1, self.prev_model.num_classes)[negative,:], classification[:,:,:self.prev_model.num_classes].view(-1, self.prev_model.num_classes)[negative, :])
                
#                 dist_class_loss = nn.MSELoss()(prev_classification.view(-1, self.prev_model.num_classes)[negative,:], classification.view(-1, self.num_classes)[negative, :self.prev_model.num_classes])

#                 #dist_class_loss /= len(prev_classification.shape[s])
#                 dist_reg_loss = smoothL1Loss(prev_regression.view(-1,4)[negative,:], regression.view(-1,4)[negative,:])


                ################################
                #        Origin Baseline       #           
                ################################
#                 greater = torch.ge(prev_classification, 0.05)
#                 if greater.sum() != 0:
#                     dist_class_loss = nn.MSELoss()(prev_classification[greater], classification[:,:,:self.prev_model.num_classes][greater])
#                     dist_reg_loss = smoothL1Loss(prev_regression[greater.any(dim = 2)], regression[greater.any(dim = 2)])
#                 else:
#                     dist_class_loss = torch.tensor(0).float().cuda()
#                     dist_reg_loss = torch.tensor(0).float().cuda()
                
                
                ################################
                #         change ratio         #           
                ################################
#                 ratio = 1
#                 class_loss *= ratio
#                 reg_loss *= ratio
#                 dist_class_loss *= ratio
#                 dist_reg_loss *= ratio
#                 dist_feat_loss *= ratio

                if self.enhance_error:
                    return (class_loss, reg_loss, dist_class_loss, dist_reg_loss, dist_feat_loss, enhance_loss)
                else:
                    return (class_loss, reg_loss, dist_class_loss, dist_reg_loss, dist_feat_loss)
