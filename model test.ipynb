{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T14:41:46.850942Z",
     "start_time": "2021-05-28T14:40:11.595225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "! git remote add origin https://github.com/EonianCoda/continual_learning_forOD.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T14:42:02.976037Z",
     "start_time": "2021-05-28T14:42:02.857439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: unknown option `EonianCoda'\r\n",
      "usage: git push [<options>] [<repository> [<refspec>...]]\r\n",
      "\r\n",
      "    -v, --verbose         be more verbose\r\n",
      "    -q, --quiet           be more quiet\r\n",
      "    --repo <repository>   repository\r\n",
      "    --all                 push all refs\r\n",
      "    --mirror              mirror all refs\r\n",
      "    -d, --delete          delete refs\r\n",
      "    --tags                push tags (can't be used with --all or --mirror)\r\n",
      "    -n, --dry-run         dry run\r\n",
      "    --porcelain           machine-readable output\r\n",
      "    -f, --force           force updates\r\n",
      "    --force-with-lease[=<refname>:<expect>]\r\n",
      "                          require old value of ref to be at this value\r\n",
      "    --recurse-submodules (check|on-demand|no)\r\n",
      "                          control recursive pushing of submodules\r\n",
      "    --thin                use thin pack\r\n",
      "    --receive-pack <receive-pack>\r\n",
      "                          receive pack program\r\n",
      "    --exec <receive-pack>\r\n",
      "                          receive pack program\r\n",
      "    -u, --set-upstream    set upstream for git pull/status\r\n",
      "    --progress            force progress reporting\r\n",
      "    --prune               prune locally removed refs\r\n",
      "    --no-verify           bypass pre-push hook\r\n",
      "    --follow-tags         push missing but relevant tags\r\n",
      "    --signed[=(yes|no|if-asked)]\r\n",
      "                          GPG sign the push\r\n",
      "    --atomic              request atomic transaction on remote side\r\n",
      "    -o, --push-option <server-specific>\r\n",
      "                          option to transmit\r\n",
      "    -4, --ipv4            use IPv4 addresses only\r\n",
      "    -6, --ipv6            use IPv6 addresses only\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! git push -u origin main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T14:36:58.144062Z",
     "start_time": "2021-05-28T14:36:57.909106Z"
    }
   },
   "outputs": [],
   "source": [
    "! git config user.name \"EonianCoda\"\n",
    "! git config user.email \"textuseful15900@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:26:33.424068Z",
     "start_time": "2021-05-27T13:26:29.047227Z"
    },
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "{'id': [[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 15, 16, 17, 18, 19], [6], [20], [14], [12], [13]], 'name': [['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person'], ['train'], ['sheep'], ['sofa'], ['pottedplant'], ['tvmonitor']]}\n",
      "dataloader class_num = 15\n",
      "readcheckpoint at Round1 Epoch50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from retinanet import model\n",
    "from retinanet import coco_eval\n",
    "from retinanet.dataloader import CocoDataset_inOrder, collater, Resizer, AspectRatioBasedSampler, Augmenter, Normalizer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import torch\n",
    "root_path = '/home/deeplab307/Documents/Anaconda/Shiang/CL/'\n",
    "method = 'w_distillation'\n",
    "data_split = '15+1'\n",
    "start_round = 1\n",
    "batch_size = 1\n",
    "\n",
    "checkpoint_epcoh = 50\n",
    "\n",
    "def checkDir(path):\n",
    "    \"\"\"check whether directory exists or not.If not, then create it \n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "def get_checkpoint_path(method, now_round, epoch):\n",
    "    global root_path\n",
    "    global data_split\n",
    "    \n",
    "    checkDir(os.path.join(root_path, 'model', method, 'round{}'.format(now_round)))\n",
    "    checkDir(os.path.join(root_path, 'model', method, 'round{}'.format(now_round), data_split))\n",
    "    \n",
    "    path = os.path.join(root_path, 'model', method, 'round{}'.format(now_round), data_split,'voc_retinanet_{}_checkpoint.pt'.format(epoch))\n",
    "    return path\n",
    "\n",
    "\n",
    "def readCheckpoint(method, now_round, epoch, retinanet, optimizer = None, scheduler = None):\n",
    "    print('readcheckpoint at Round{} Epoch{}'.format(now_round, epoch))\n",
    "    prev_checkpoint = torch.load(get_checkpoint_path(method, now_round, epoch))\n",
    "    retinanet.load_state_dict(prev_checkpoint['model_state_dict'])\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(prev_checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(prev_checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "\n",
    "\n",
    "coco_path = '/home/deeplab307/Documents/Anaconda/Shiang/CL/DataSet/VOC2012'\n",
    "\n",
    "\n",
    "\n",
    "# dataset_train = CocoDataset_inOrder(coco_path, set_name='TrainVoc2012', dataset = 'voc',\n",
    "#                                     transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]),\n",
    "#                                    data_split=data_split, start_round=start_round)\n",
    "\n",
    "# dataset_val = CocoDataset_inOrder(os.path.join(root_path, 'DataSet', 'VOC2012'), set_name=\"ValVoc2012\", dataset = 'voc', \n",
    "#                 transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "#                 start_round=1, data_split = \"20\")\n",
    "dataset_train = CocoDataset_inOrder(coco_path, set_name='TrainVoc2012', dataset = 'voc',\n",
    "                                    transform=transforms.Compose([Normalizer(), Resizer()]),\n",
    "                                   data_split=data_split, start_round=start_round)\n",
    "\n",
    "# sampler = AspectRatioBasedSampler(dataset_train, batch_size = batch_size, drop_last=False)\n",
    "# dataloader_train = DataLoader(dataset_train, num_workers=2, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "retinanet.cuda()\n",
    "\n",
    "optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "readCheckpoint(method, start_round, checkpoint_epcoh, retinanet , optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:43:56.995758Z",
     "start_time": "2021-05-27T13:35:11.543629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 0 | Classification loss: 0.71616 | Regression loss: 0.46356 | Running loss: 0.63894 | Spend Time:0.07s\n",
      "Epoch: 50 | Iteration: 1 | Classification loss: 0.45599 | Regression loss: 0.27079 | Running loss: 0.63963 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2 | Classification loss: 0.61635 | Regression loss: 0.38911 | Running loss: 0.64246 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3 | Classification loss: 0.39104 | Regression loss: 0.33380 | Running loss: 0.64310 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4 | Classification loss: 0.72215 | Regression loss: 0.38564 | Running loss: 0.64665 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5 | Classification loss: 0.13401 | Regression loss: 0.32161 | Running loss: 0.64520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 6 | Classification loss: 0.23653 | Regression loss: 0.23852 | Running loss: 0.64392 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 7 | Classification loss: 0.25578 | Regression loss: 0.39032 | Running loss: 0.64394 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 8 | Classification loss: 0.11753 | Regression loss: 0.16931 | Running loss: 0.64129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 9 | Classification loss: 0.03284 | Regression loss: 0.21301 | Running loss: 0.63838 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 10 | Classification loss: 0.98644 | Regression loss: 0.21448 | Running loss: 0.64249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 11 | Classification loss: 0.01766 | Regression loss: 0.12359 | Running loss: 0.63886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 12 | Classification loss: 0.06001 | Regression loss: 0.15425 | Running loss: 0.63580 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 13 | Classification loss: 0.26069 | Regression loss: 0.38769 | Running loss: 0.63589 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 14 | Classification loss: 0.49374 | Regression loss: 0.52110 | Running loss: 0.63858 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 15 | Classification loss: 0.00320 | Regression loss: 0.19134 | Running loss: 0.63545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 16 | Classification loss: 0.21032 | Regression loss: 0.34763 | Running loss: 0.63491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 17 | Classification loss: 0.56770 | Regression loss: 0.55870 | Running loss: 0.63832 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 18 | Classification loss: 0.01885 | Regression loss: 0.15847 | Running loss: 0.63514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 19 | Classification loss: 0.30631 | Regression loss: 0.28778 | Running loss: 0.63486 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 20 | Classification loss: 0.01311 | Regression loss: 0.15607 | Running loss: 0.63170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 21 | Classification loss: 0.03523 | Regression loss: 0.12664 | Running loss: 0.62852 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 22 | Classification loss: 0.00077 | Regression loss: 0.09164 | Running loss: 0.62492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 23 | Classification loss: 0.42747 | Regression loss: 0.27218 | Running loss: 0.62542 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 24 | Classification loss: 1.53906 | Regression loss: 0.49655 | Running loss: 0.63476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 25 | Classification loss: 0.08607 | Regression loss: 0.24240 | Running loss: 0.63274 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 26 | Classification loss: 0.28797 | Regression loss: 0.24279 | Running loss: 0.63208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 27 | Classification loss: 1.37939 | Regression loss: 0.46031 | Running loss: 0.63992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 28 | Classification loss: 0.43177 | Regression loss: 0.55078 | Running loss: 0.64213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 29 | Classification loss: 0.01236 | Regression loss: 0.06744 | Running loss: 0.63853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 30 | Classification loss: 0.10388 | Regression loss: 0.12379 | Running loss: 0.63591 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 31 | Classification loss: 1.00433 | Regression loss: 0.64560 | Running loss: 0.64233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 32 | Classification loss: 0.27238 | Regression loss: 0.42075 | Running loss: 0.64265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 33 | Classification loss: 0.30168 | Regression loss: 0.60273 | Running loss: 0.64428 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 34 | Classification loss: 0.35987 | Regression loss: 0.25637 | Running loss: 0.64411 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 35 | Classification loss: 0.17476 | Regression loss: 0.36356 | Running loss: 0.64346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 36 | Classification loss: 0.07463 | Regression loss: 0.31411 | Running loss: 0.64189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 37 | Classification loss: 0.47218 | Regression loss: 0.26248 | Running loss: 0.64246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 38 | Classification loss: 0.63497 | Regression loss: 0.40705 | Running loss: 0.64488 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 39 | Classification loss: 0.00018 | Regression loss: 0.03219 | Running loss: 0.64119 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 40 | Classification loss: 0.51372 | Regression loss: 0.39033 | Running loss: 0.64276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 41 | Classification loss: 0.28166 | Regression loss: 0.20385 | Running loss: 0.64183 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 42 | Classification loss: 0.00336 | Regression loss: 0.17465 | Running loss: 0.63908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 43 | Classification loss: 0.19052 | Regression loss: 0.20716 | Running loss: 0.63766 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 44 | Classification loss: 0.02931 | Regression loss: 0.18771 | Running loss: 0.63520 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 45 | Classification loss: 0.08970 | Regression loss: 0.05781 | Running loss: 0.63237 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 46 | Classification loss: 1.50710 | Regression loss: 0.59944 | Running loss: 0.64089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 47 | Classification loss: 1.92607 | Regression loss: 0.72216 | Running loss: 0.65243 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 48 | Classification loss: 0.22926 | Regression loss: 0.20592 | Running loss: 0.65118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 49 | Classification loss: 0.19189 | Regression loss: 0.24179 | Running loss: 0.64995 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 50 | Classification loss: 0.21088 | Regression loss: 0.24385 | Running loss: 0.64885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 51 | Classification loss: 0.91527 | Regression loss: 0.39089 | Running loss: 0.65254 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 52 | Classification loss: 0.27153 | Regression loss: 0.28384 | Running loss: 0.65200 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 53 | Classification loss: 0.15779 | Regression loss: 0.11728 | Running loss: 0.64990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 54 | Classification loss: 0.32622 | Regression loss: 0.46522 | Running loss: 0.65068 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 55 | Classification loss: 0.30791 | Regression loss: 0.19157 | Running loss: 0.64985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 56 | Classification loss: 0.04256 | Regression loss: 0.22930 | Running loss: 0.64779 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 57 | Classification loss: 0.37000 | Regression loss: 0.27680 | Running loss: 0.64778 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 58 | Classification loss: 0.12573 | Regression loss: 0.19863 | Running loss: 0.64603 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 59 | Classification loss: 0.00230 | Regression loss: 0.02379 | Running loss: 0.64270 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 60 | Classification loss: 0.20905 | Regression loss: 0.20555 | Running loss: 0.64148 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 61 | Classification loss: 0.34608 | Regression loss: 0.31898 | Running loss: 0.64161 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 62 | Classification loss: 0.37437 | Regression loss: 0.58584 | Running loss: 0.64329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 63 | Classification loss: 0.10666 | Regression loss: 0.25302 | Running loss: 0.64180 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 64 | Classification loss: 0.00325 | Regression loss: 0.04593 | Running loss: 0.63870 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 65 | Classification loss: 1.20984 | Regression loss: 0.44589 | Running loss: 0.64399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 66 | Classification loss: 0.52883 | Regression loss: 0.56520 | Running loss: 0.64633 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 67 | Classification loss: 0.10868 | Regression loss: 0.07885 | Running loss: 0.64396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 68 | Classification loss: 0.58997 | Regression loss: 0.66985 | Running loss: 0.64712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 69 | Classification loss: 0.00520 | Regression loss: 0.04752 | Running loss: 0.64409 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 70 | Classification loss: 0.01528 | Regression loss: 0.14548 | Running loss: 0.64163 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 71 | Classification loss: 1.00784 | Regression loss: 0.47335 | Running loss: 0.64587 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 72 | Classification loss: 0.14759 | Regression loss: 0.30031 | Running loss: 0.64488 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 73 | Classification loss: 0.10783 | Regression loss: 0.24007 | Running loss: 0.64339 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 74 | Classification loss: 0.77485 | Regression loss: 0.73906 | Running loss: 0.64772 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 75 | Classification loss: 0.18366 | Regression loss: 0.27833 | Running loss: 0.64680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 76 | Classification loss: 0.00282 | Regression loss: 0.08393 | Running loss: 0.64405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 77 | Classification loss: 0.55558 | Regression loss: 0.42717 | Running loss: 0.64571 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 78 | Classification loss: 0.19247 | Regression loss: 0.30301 | Running loss: 0.64497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 79 | Classification loss: 0.57440 | Regression loss: 0.41897 | Running loss: 0.64666 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 80 | Classification loss: 0.20893 | Regression loss: 0.36046 | Running loss: 0.64629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 81 | Classification loss: 0.04466 | Regression loss: 0.30641 | Running loss: 0.64487 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 82 | Classification loss: 0.29510 | Regression loss: 0.29561 | Running loss: 0.64461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 83 | Classification loss: 0.29208 | Regression loss: 0.52631 | Running loss: 0.64544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 84 | Classification loss: 0.02874 | Regression loss: 0.13225 | Running loss: 0.64314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 85 | Classification loss: 0.04936 | Regression loss: 0.34965 | Running loss: 0.64199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 86 | Classification loss: 0.54782 | Regression loss: 0.63923 | Running loss: 0.64455 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 87 | Classification loss: 0.60358 | Regression loss: 0.53842 | Running loss: 0.64688 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 88 | Classification loss: 0.01027 | Regression loss: 0.15559 | Running loss: 0.64464 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 89 | Classification loss: 0.12274 | Regression loss: 0.21722 | Running loss: 0.64323 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 90 | Classification loss: 0.07394 | Regression loss: 0.18806 | Running loss: 0.64147 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 91 | Classification loss: 1.00969 | Regression loss: 0.63733 | Running loss: 0.64608 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 92 | Classification loss: 0.90590 | Regression loss: 0.52029 | Running loss: 0.64965 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 93 | Classification loss: 0.23885 | Regression loss: 0.22138 | Running loss: 0.64879 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 94 | Classification loss: 0.05338 | Regression loss: 0.12772 | Running loss: 0.64667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 95 | Classification loss: 0.07838 | Regression loss: 0.19230 | Running loss: 0.64498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 96 | Classification loss: 0.24266 | Regression loss: 0.25229 | Running loss: 0.64430 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 97 | Classification loss: 0.59092 | Regression loss: 0.72323 | Running loss: 0.64729 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 98 | Classification loss: 0.59355 | Regression loss: 0.36714 | Running loss: 0.64869 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 99 | Classification loss: 0.13918 | Regression loss: 0.31910 | Running loss: 0.64784 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 100 | Classification loss: 0.07138 | Regression loss: 0.27318 | Running loss: 0.64651 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 101 | Classification loss: 0.25524 | Regression loss: 0.26352 | Running loss: 0.64595 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 102 | Classification loss: 0.01650 | Regression loss: 0.09722 | Running loss: 0.64362 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 103 | Classification loss: 0.44383 | Regression loss: 0.29880 | Running loss: 0.64405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 104 | Classification loss: 0.09231 | Regression loss: 0.22188 | Running loss: 0.64263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 105 | Classification loss: 0.13381 | Regression loss: 0.42079 | Running loss: 0.64225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 106 | Classification loss: 2.07064 | Regression loss: 0.56485 | Running loss: 0.65080 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 107 | Classification loss: 0.22976 | Regression loss: 0.20486 | Running loss: 0.64988 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 108 | Classification loss: 0.00678 | Regression loss: 0.13161 | Running loss: 0.64770 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 109 | Classification loss: 0.74194 | Regression loss: 0.52179 | Running loss: 0.65031 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 110 | Classification loss: 0.01205 | Regression loss: 0.17893 | Running loss: 0.64837 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 111 | Classification loss: 0.04525 | Regression loss: 0.20872 | Running loss: 0.64672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 112 | Classification loss: 0.77061 | Regression loss: 0.62379 | Running loss: 0.64984 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 113 | Classification loss: 0.06233 | Regression loss: 0.18101 | Running loss: 0.64815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 114 | Classification loss: 1.02721 | Regression loss: 0.46818 | Running loss: 0.65167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 115 | Classification loss: 0.00257 | Regression loss: 0.10151 | Running loss: 0.64940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 116 | Classification loss: 1.83712 | Regression loss: 0.37170 | Running loss: 0.65582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 117 | Classification loss: 0.10390 | Regression loss: 0.39016 | Running loss: 0.65516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 118 | Classification loss: 0.75061 | Regression loss: 0.31587 | Running loss: 0.65684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 119 | Classification loss: 0.01755 | Regression loss: 0.21139 | Running loss: 0.65510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 120 | Classification loss: 0.36086 | Regression loss: 0.46312 | Running loss: 0.65578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 121 | Classification loss: 0.06946 | Regression loss: 0.13434 | Running loss: 0.65396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 122 | Classification loss: 0.01073 | Regression loss: 0.12194 | Running loss: 0.65186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 123 | Classification loss: 0.03697 | Regression loss: 0.10888 | Running loss: 0.64984 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 124 | Classification loss: 0.31235 | Regression loss: 0.25347 | Running loss: 0.64951 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 125 | Classification loss: 0.29383 | Regression loss: 0.25350 | Running loss: 0.64910 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 126 | Classification loss: 0.06372 | Regression loss: 0.19698 | Running loss: 0.64756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 127 | Classification loss: 0.84806 | Regression loss: 0.42911 | Running loss: 0.65004 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 128 | Classification loss: 0.32435 | Regression loss: 0.41164 | Running loss: 0.65038 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 129 | Classification loss: 0.00429 | Regression loss: 0.12867 | Running loss: 0.64836 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 130 | Classification loss: 1.10876 | Regression loss: 0.59535 | Running loss: 0.65247 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 131 | Classification loss: 0.01874 | Regression loss: 0.13745 | Running loss: 0.65054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 132 | Classification loss: 0.50545 | Regression loss: 0.52541 | Running loss: 0.65201 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 133 | Classification loss: 0.83939 | Regression loss: 0.45150 | Running loss: 0.65447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 134 | Classification loss: 0.48121 | Regression loss: 0.71059 | Running loss: 0.65653 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 135 | Classification loss: 0.46765 | Regression loss: 0.80592 | Running loss: 0.65888 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 136 | Classification loss: 0.09267 | Regression loss: 0.15720 | Running loss: 0.65733 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 137 | Classification loss: 0.56349 | Regression loss: 0.58697 | Running loss: 0.65920 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 138 | Classification loss: 1.00943 | Regression loss: 0.17001 | Running loss: 0.66116 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 139 | Classification loss: 0.89338 | Regression loss: 0.29375 | Running loss: 0.66314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 140 | Classification loss: 0.12699 | Regression loss: 0.29161 | Running loss: 0.66222 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 141 | Classification loss: 0.24782 | Regression loss: 0.38431 | Running loss: 0.66211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 142 | Classification loss: 0.18289 | Regression loss: 0.38264 | Running loss: 0.66175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 143 | Classification loss: 0.85680 | Regression loss: 0.55284 | Running loss: 0.66452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 144 | Classification loss: 0.12799 | Regression loss: 0.31970 | Running loss: 0.66372 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 145 | Classification loss: 0.84012 | Regression loss: 0.44307 | Running loss: 0.66600 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 146 | Classification loss: 0.28053 | Regression loss: 0.53232 | Running loss: 0.66653 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 147 | Classification loss: 0.09155 | Regression loss: 0.14031 | Running loss: 0.66495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 148 | Classification loss: 0.00036 | Regression loss: 0.04574 | Running loss: 0.66270 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 149 | Classification loss: 0.31647 | Regression loss: 0.37197 | Running loss: 0.66279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 150 | Classification loss: 0.50410 | Regression loss: 0.43894 | Running loss: 0.66380 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 151 | Classification loss: 0.08880 | Regression loss: 0.24020 | Running loss: 0.66260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 152 | Classification loss: 0.40422 | Regression loss: 0.62548 | Running loss: 0.66391 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 153 | Classification loss: 0.01426 | Regression loss: 0.07479 | Running loss: 0.66186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 154 | Classification loss: 0.42158 | Regression loss: 0.29209 | Running loss: 0.66205 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 155 | Classification loss: 0.00109 | Regression loss: 0.03786 | Running loss: 0.65984 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 156 | Classification loss: 1.87037 | Regression loss: 0.13396 | Running loss: 0.66459 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 157 | Classification loss: 0.68278 | Regression loss: 0.37566 | Running loss: 0.66597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 158 | Classification loss: 0.02520 | Regression loss: 0.17905 | Running loss: 0.66435 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 159 | Classification loss: 1.05434 | Regression loss: 0.42069 | Running loss: 0.66719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 160 | Classification loss: 0.61031 | Regression loss: 0.40784 | Running loss: 0.66841 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 161 | Classification loss: 0.37743 | Regression loss: 0.35342 | Running loss: 0.66863 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 162 | Classification loss: 0.77934 | Regression loss: 0.27978 | Running loss: 0.66998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 163 | Classification loss: 0.12222 | Regression loss: 0.53759 | Running loss: 0.66994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 164 | Classification loss: 0.26241 | Regression loss: 0.40405 | Running loss: 0.66993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 165 | Classification loss: 0.52322 | Regression loss: 0.43628 | Running loss: 0.67092 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 166 | Classification loss: 0.37358 | Regression loss: 0.43330 | Running loss: 0.67139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 167 | Classification loss: 1.12356 | Regression loss: 0.48687 | Running loss: 0.67458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 168 | Classification loss: 0.30431 | Regression loss: 0.23108 | Running loss: 0.67411 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 169 | Classification loss: 0.07660 | Regression loss: 0.37457 | Running loss: 0.67336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 170 | Classification loss: 0.00714 | Regression loss: 0.12445 | Running loss: 0.67153 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 171 | Classification loss: 0.00013 | Regression loss: 0.06163 | Running loss: 0.66949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 172 | Classification loss: 0.07029 | Regression loss: 0.29897 | Running loss: 0.66848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 173 | Classification loss: 0.98068 | Regression loss: 0.46570 | Running loss: 0.67108 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 174 | Classification loss: 0.06384 | Regression loss: 0.16107 | Running loss: 0.66959 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 175 | Classification loss: 0.20248 | Regression loss: 0.57254 | Running loss: 0.66994 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 176 | Classification loss: 1.28381 | Regression loss: 0.44722 | Running loss: 0.67344 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 177 | Classification loss: 1.43199 | Regression loss: 0.53737 | Running loss: 0.67771 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 178 | Classification loss: 0.01284 | Regression loss: 0.13635 | Running loss: 0.67597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 179 | Classification loss: 1.04017 | Regression loss: 0.42902 | Running loss: 0.67857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 180 | Classification loss: 0.00467 | Regression loss: 0.04193 | Running loss: 0.67651 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 181 | Classification loss: 0.13540 | Regression loss: 0.27850 | Running loss: 0.67566 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 182 | Classification loss: 0.31834 | Regression loss: 0.52463 | Running loss: 0.67620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 183 | Classification loss: 0.21998 | Regression loss: 0.36401 | Running loss: 0.67590 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 184 | Classification loss: 0.17919 | Regression loss: 0.62820 | Running loss: 0.67632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 185 | Classification loss: 0.43129 | Regression loss: 0.16747 | Running loss: 0.67607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 186 | Classification loss: 0.19939 | Regression loss: 0.27834 | Running loss: 0.67544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 187 | Classification loss: 0.29982 | Regression loss: 0.46823 | Running loss: 0.67573 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 188 | Classification loss: 0.06210 | Regression loss: 0.21633 | Running loss: 0.67447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 189 | Classification loss: 0.15048 | Regression loss: 0.37573 | Running loss: 0.67400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 190 | Classification loss: 0.10219 | Regression loss: 0.46444 | Running loss: 0.67367 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 191 | Classification loss: 0.40858 | Regression loss: 0.28895 | Running loss: 0.67374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 192 | Classification loss: 0.32228 | Regression loss: 0.40735 | Running loss: 0.67392 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 193 | Classification loss: 0.09294 | Regression loss: 0.21151 | Running loss: 0.67276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 194 | Classification loss: 0.99518 | Regression loss: 0.69402 | Running loss: 0.67593 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 195 | Classification loss: 0.00811 | Regression loss: 0.13390 | Running loss: 0.67427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 196 | Classification loss: 0.95156 | Regression loss: 0.59858 | Running loss: 0.67698 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 197 | Classification loss: 0.01273 | Regression loss: 0.16771 | Running loss: 0.67545 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 198 | Classification loss: 1.22954 | Regression loss: 0.70820 | Running loss: 0.67933 | Spend Time:0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 199 | Classification loss: 0.74801 | Regression loss: 0.33133 | Running loss: 0.68056 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 200 | Classification loss: 0.06931 | Regression loss: 0.31044 | Running loss: 0.67964 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 201 | Classification loss: 0.47120 | Regression loss: 0.55139 | Running loss: 0.68069 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 202 | Classification loss: 0.37667 | Regression loss: 0.15387 | Running loss: 0.68023 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 203 | Classification loss: 0.01574 | Regression loss: 0.13492 | Running loss: 0.67862 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 204 | Classification loss: 0.04353 | Regression loss: 0.28333 | Running loss: 0.67756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 205 | Classification loss: 0.05397 | Regression loss: 0.21832 | Running loss: 0.67634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 206 | Classification loss: 0.21791 | Regression loss: 0.17229 | Running loss: 0.67548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 207 | Classification loss: 0.00094 | Regression loss: 0.05453 | Running loss: 0.67363 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 208 | Classification loss: 0.05110 | Regression loss: 0.25478 | Running loss: 0.67253 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 209 | Classification loss: 0.45495 | Regression loss: 0.26735 | Running loss: 0.67268 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 210 | Classification loss: 0.77148 | Regression loss: 0.71127 | Running loss: 0.67508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 211 | Classification loss: 0.01111 | Regression loss: 0.25286 | Running loss: 0.67386 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 212 | Classification loss: 0.49453 | Regression loss: 0.45033 | Running loss: 0.67466 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 213 | Classification loss: 0.01143 | Regression loss: 0.04028 | Running loss: 0.67283 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 214 | Classification loss: 0.76306 | Regression loss: 0.56147 | Running loss: 0.67474 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 215 | Classification loss: 0.25624 | Regression loss: 0.41867 | Running loss: 0.67474 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 216 | Classification loss: 0.43516 | Regression loss: 0.43724 | Running loss: 0.67532 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 217 | Classification loss: 0.22545 | Regression loss: 0.34412 | Running loss: 0.67501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 218 | Classification loss: 0.00281 | Regression loss: 0.09320 | Running loss: 0.67333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 219 | Classification loss: 0.02730 | Regression loss: 0.11763 | Running loss: 0.67181 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 220 | Classification loss: 0.61915 | Regression loss: 0.34036 | Running loss: 0.67263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 221 | Classification loss: 0.03649 | Regression loss: 0.11852 | Running loss: 0.67115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 222 | Classification loss: 0.28926 | Regression loss: 0.41173 | Running loss: 0.67123 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 223 | Classification loss: 0.04540 | Regression loss: 0.16851 | Running loss: 0.66993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 224 | Classification loss: 0.42371 | Regression loss: 0.52333 | Running loss: 0.67072 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 225 | Classification loss: 0.44708 | Regression loss: 0.47939 | Running loss: 0.67144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 226 | Classification loss: 0.19943 | Regression loss: 0.32054 | Running loss: 0.67101 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 227 | Classification loss: 0.04864 | Regression loss: 0.13610 | Running loss: 0.66964 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 228 | Classification loss: 0.04458 | Regression loss: 0.09877 | Running loss: 0.66816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 229 | Classification loss: 0.24666 | Regression loss: 0.21781 | Running loss: 0.66758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 230 | Classification loss: 0.71908 | Regression loss: 0.60662 | Running loss: 0.66943 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 231 | Classification loss: 0.63013 | Regression loss: 0.51528 | Running loss: 0.67076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 232 | Classification loss: 1.56750 | Regression loss: 0.74325 | Running loss: 0.67533 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 233 | Classification loss: 0.01745 | Regression loss: 0.17506 | Running loss: 0.67398 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 234 | Classification loss: 0.04270 | Regression loss: 0.26431 | Running loss: 0.67297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 235 | Classification loss: 0.16491 | Regression loss: 0.28845 | Running loss: 0.67236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 236 | Classification loss: 0.26354 | Regression loss: 0.28342 | Running loss: 0.67202 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 237 | Classification loss: 0.10650 | Regression loss: 0.16257 | Running loss: 0.67091 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 238 | Classification loss: 1.18150 | Regression loss: 0.49850 | Running loss: 0.67367 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 239 | Classification loss: 1.67892 | Regression loss: 0.57074 | Running loss: 0.67798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 240 | Classification loss: 1.24190 | Regression loss: 0.44647 | Running loss: 0.68073 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 241 | Classification loss: 0.89082 | Regression loss: 0.41095 | Running loss: 0.68242 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 242 | Classification loss: 0.32120 | Regression loss: 0.40933 | Running loss: 0.68255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 243 | Classification loss: 0.73207 | Regression loss: 0.33001 | Running loss: 0.68358 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 244 | Classification loss: 1.26133 | Regression loss: 0.58266 | Running loss: 0.68670 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 245 | Classification loss: 0.42501 | Regression loss: 0.74811 | Running loss: 0.68801 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 246 | Classification loss: 1.55932 | Regression loss: 0.61972 | Running loss: 0.69201 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 247 | Classification loss: 0.05265 | Regression loss: 0.14020 | Running loss: 0.69067 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 248 | Classification loss: 0.03239 | Regression loss: 0.16349 | Running loss: 0.68936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 249 | Classification loss: 0.51905 | Regression loss: 0.37498 | Running loss: 0.68990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 250 | Classification loss: 0.66401 | Regression loss: 0.18914 | Running loss: 0.69033 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 251 | Classification loss: 0.00572 | Regression loss: 0.07952 | Running loss: 0.68873 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 252 | Classification loss: 0.50858 | Regression loss: 0.51416 | Running loss: 0.68961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 253 | Classification loss: 0.01098 | Regression loss: 0.17775 | Running loss: 0.68829 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 254 | Classification loss: 0.03278 | Regression loss: 0.12931 | Running loss: 0.68691 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 255 | Classification loss: 0.01652 | Regression loss: 0.12168 | Running loss: 0.68548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 256 | Classification loss: 0.12864 | Regression loss: 0.36790 | Running loss: 0.68498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 257 | Classification loss: 0.05410 | Regression loss: 0.21105 | Running loss: 0.68389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 258 | Classification loss: 0.02075 | Regression loss: 0.09109 | Running loss: 0.68240 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 259 | Classification loss: 0.75292 | Regression loss: 0.42488 | Running loss: 0.68369 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 260 | Classification loss: 0.41005 | Regression loss: 0.47154 | Running loss: 0.68420 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 261 | Classification loss: 0.03626 | Regression loss: 0.11477 | Running loss: 0.68283 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 262 | Classification loss: 0.09291 | Regression loss: 0.17026 | Running loss: 0.68175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 263 | Classification loss: 0.00250 | Regression loss: 0.10408 | Running loss: 0.68027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 264 | Classification loss: 1.17078 | Regression loss: 0.60119 | Running loss: 0.68306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 265 | Classification loss: 0.01473 | Regression loss: 0.13562 | Running loss: 0.68170 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 266 | Classification loss: 0.45834 | Regression loss: 0.21516 | Running loss: 0.68168 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 267 | Classification loss: 0.07545 | Regression loss: 0.14438 | Running loss: 0.68051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 268 | Classification loss: 0.34222 | Regression loss: 0.33847 | Running loss: 0.68051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 269 | Classification loss: 1.92543 | Regression loss: 0.68848 | Running loss: 0.68539 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 270 | Classification loss: 0.40768 | Regression loss: 0.45705 | Running loss: 0.68585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 271 | Classification loss: 0.41575 | Regression loss: 0.46420 | Running loss: 0.68633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 272 | Classification loss: 0.79178 | Regression loss: 0.38642 | Running loss: 0.68757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 273 | Classification loss: 0.03389 | Regression loss: 0.08709 | Running loss: 0.68615 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 274 | Classification loss: 0.06017 | Regression loss: 0.13542 | Running loss: 0.68493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 275 | Classification loss: 0.09978 | Regression loss: 0.16979 | Running loss: 0.68389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 276 | Classification loss: 0.40582 | Regression loss: 0.18427 | Running loss: 0.68366 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 277 | Classification loss: 1.73432 | Regression loss: 0.66457 | Running loss: 0.68791 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 278 | Classification loss: 0.47485 | Regression loss: 0.29414 | Running loss: 0.68811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 279 | Classification loss: 0.32345 | Regression loss: 0.58309 | Running loss: 0.68864 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 280 | Classification loss: 0.41199 | Regression loss: 0.55516 | Running loss: 0.68933 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 281 | Classification loss: 0.00538 | Regression loss: 0.10185 | Running loss: 0.68790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 282 | Classification loss: 0.35308 | Regression loss: 0.33646 | Running loss: 0.68791 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 283 | Classification loss: 1.35225 | Regression loss: 0.50108 | Running loss: 0.69075 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 284 | Classification loss: 0.02174 | Regression loss: 0.19661 | Running loss: 0.68960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 285 | Classification loss: 0.10629 | Regression loss: 0.29718 | Running loss: 0.68890 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 286 | Classification loss: 0.30988 | Regression loss: 0.35903 | Running loss: 0.68886 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 287 | Classification loss: 0.70095 | Regression loss: 0.36994 | Running loss: 0.68978 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 288 | Classification loss: 0.35227 | Regression loss: 0.47837 | Running loss: 0.69012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 289 | Classification loss: 0.35852 | Regression loss: 0.44943 | Running loss: 0.69040 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 290 | Classification loss: 0.27549 | Regression loss: 0.34702 | Running loss: 0.69024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 291 | Classification loss: 0.21166 | Regression loss: 0.38113 | Running loss: 0.69001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 292 | Classification loss: 0.00008 | Regression loss: 0.05459 | Running loss: 0.68849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 293 | Classification loss: 0.23280 | Regression loss: 0.55318 | Running loss: 0.68872 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 294 | Classification loss: 0.98376 | Regression loss: 0.16168 | Running loss: 0.68981 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 295 | Classification loss: 0.29397 | Regression loss: 0.30398 | Running loss: 0.68959 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 296 | Classification loss: 0.26146 | Regression loss: 0.44021 | Running loss: 0.68962 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 297 | Classification loss: 0.14659 | Regression loss: 0.10780 | Running loss: 0.68859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 298 | Classification loss: 0.42742 | Regression loss: 0.35955 | Running loss: 0.68882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 299 | Classification loss: 2.27916 | Regression loss: 0.74965 | Running loss: 0.69432 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 300 | Classification loss: 1.23772 | Regression loss: 0.48992 | Running loss: 0.69674 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 301 | Classification loss: 0.28319 | Regression loss: 0.26917 | Running loss: 0.69640 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 302 | Classification loss: 0.23378 | Regression loss: 0.32175 | Running loss: 0.69607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 303 | Classification loss: 0.29199 | Regression loss: 0.33963 | Running loss: 0.69592 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 304 | Classification loss: 0.03540 | Regression loss: 0.28259 | Running loss: 0.69504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 305 | Classification loss: 1.21607 | Regression loss: 0.46935 | Running loss: 0.69734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 306 | Classification loss: 0.00043 | Regression loss: 0.13343 | Running loss: 0.69603 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 307 | Classification loss: 0.31787 | Regression loss: 0.34044 | Running loss: 0.69595 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 308 | Classification loss: 0.13956 | Regression loss: 0.28019 | Running loss: 0.69531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 309 | Classification loss: 0.13707 | Regression loss: 0.26521 | Running loss: 0.69464 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 310 | Classification loss: 0.15444 | Regression loss: 0.33136 | Running loss: 0.69416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 311 | Classification loss: 0.58234 | Regression loss: 0.48650 | Running loss: 0.69502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 312 | Classification loss: 0.46322 | Regression loss: 0.48154 | Running loss: 0.69559 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 313 | Classification loss: 0.45038 | Regression loss: 0.50474 | Running loss: 0.69618 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 314 | Classification loss: 0.42959 | Regression loss: 0.39393 | Running loss: 0.69647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 315 | Classification loss: 0.21335 | Regression loss: 0.33517 | Running loss: 0.69613 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 316 | Classification loss: 0.03283 | Regression loss: 0.15364 | Running loss: 0.69498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 317 | Classification loss: 0.61323 | Regression loss: 0.45449 | Running loss: 0.69582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 318 | Classification loss: 0.05613 | Regression loss: 0.18999 | Running loss: 0.69481 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 319 | Classification loss: 0.30426 | Regression loss: 0.38346 | Running loss: 0.69479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 320 | Classification loss: 0.26962 | Regression loss: 0.46886 | Running loss: 0.69489 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 321 | Classification loss: 0.01562 | Regression loss: 0.07339 | Running loss: 0.69354 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 322 | Classification loss: 0.05413 | Regression loss: 0.09848 | Running loss: 0.69233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 323 | Classification loss: 0.76409 | Regression loss: 0.49086 | Running loss: 0.69358 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 324 | Classification loss: 0.27831 | Regression loss: 0.25376 | Running loss: 0.69323 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 325 | Classification loss: 0.25835 | Regression loss: 0.43905 | Running loss: 0.69324 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 326 | Classification loss: 0.40926 | Regression loss: 0.49017 | Running loss: 0.69369 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 327 | Classification loss: 0.02933 | Regression loss: 0.22664 | Running loss: 0.69273 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 328 | Classification loss: 0.02146 | Regression loss: 0.16706 | Running loss: 0.69162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 329 | Classification loss: 0.00035 | Regression loss: 0.22935 | Running loss: 0.69061 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 330 | Classification loss: 0.26433 | Regression loss: 0.19411 | Running loss: 0.69010 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 331 | Classification loss: 0.00158 | Regression loss: 0.20638 | Running loss: 0.68904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 332 | Classification loss: 0.19386 | Regression loss: 0.48345 | Running loss: 0.68902 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 333 | Classification loss: 0.53909 | Regression loss: 0.73933 | Running loss: 0.69030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 334 | Classification loss: 0.03951 | Regression loss: 0.08993 | Running loss: 0.68908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 335 | Classification loss: 0.18100 | Regression loss: 0.14211 | Running loss: 0.68829 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 336 | Classification loss: 0.35869 | Regression loss: 0.28774 | Running loss: 0.68820 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 337 | Classification loss: 0.87547 | Regression loss: 0.65754 | Running loss: 0.69002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 338 | Classification loss: 1.55838 | Regression loss: 0.62537 | Running loss: 0.69323 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 339 | Classification loss: 0.00016 | Regression loss: 0.15778 | Running loss: 0.69209 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 340 | Classification loss: 0.04695 | Regression loss: 0.17827 | Running loss: 0.69109 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 341 | Classification loss: 0.01641 | Regression loss: 0.15609 | Running loss: 0.68998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 342 | Classification loss: 0.07462 | Regression loss: 0.19364 | Running loss: 0.68908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 343 | Classification loss: 0.11873 | Regression loss: 0.28129 | Running loss: 0.68846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 344 | Classification loss: 0.52001 | Regression loss: 0.46919 | Running loss: 0.68910 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 345 | Classification loss: 0.03281 | Regression loss: 0.08157 | Running loss: 0.68788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 346 | Classification loss: 0.89954 | Regression loss: 0.92836 | Running loss: 0.69029 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 347 | Classification loss: 1.62549 | Regression loss: 0.50755 | Running loss: 0.69334 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 348 | Classification loss: 0.25801 | Regression loss: 0.37548 | Running loss: 0.69321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 349 | Classification loss: 0.05122 | Regression loss: 0.31032 | Running loss: 0.69252 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 350 | Classification loss: 0.48051 | Regression loss: 0.65105 | Running loss: 0.69344 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 351 | Classification loss: 0.49963 | Regression loss: 0.33880 | Running loss: 0.69374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 352 | Classification loss: 0.29595 | Regression loss: 0.39168 | Running loss: 0.69373 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 353 | Classification loss: 0.62612 | Regression loss: 0.48828 | Running loss: 0.69460 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 354 | Classification loss: 1.62145 | Regression loss: 0.55832 | Running loss: 0.69769 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 355 | Classification loss: 0.30938 | Regression loss: 0.31110 | Running loss: 0.69753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 356 | Classification loss: 1.30267 | Regression loss: 0.66482 | Running loss: 0.70016 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 357 | Classification loss: 0.02255 | Regression loss: 0.07076 | Running loss: 0.69891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 358 | Classification loss: 0.03803 | Regression loss: 0.22856 | Running loss: 0.69801 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 359 | Classification loss: 0.00563 | Regression loss: 0.04867 | Running loss: 0.69669 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 360 | Classification loss: 0.25879 | Regression loss: 0.31012 | Running loss: 0.69643 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 361 | Classification loss: 3.90968 | Regression loss: 1.09455 | Running loss: 0.70525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 362 | Classification loss: 0.08166 | Regression loss: 0.24689 | Running loss: 0.70448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 363 | Classification loss: 0.03254 | Regression loss: 0.10218 | Running loss: 0.70332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 364 | Classification loss: 0.00042 | Regression loss: 0.06490 | Running loss: 0.70202 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 365 | Classification loss: 0.12797 | Regression loss: 0.40960 | Running loss: 0.70169 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 366 | Classification loss: 1.56096 | Regression loss: 0.81414 | Running loss: 0.70508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 367 | Classification loss: 0.56807 | Regression loss: 0.42751 | Running loss: 0.70567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 368 | Classification loss: 1.11707 | Regression loss: 0.58286 | Running loss: 0.70768 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 369 | Classification loss: 1.14089 | Regression loss: 0.40755 | Running loss: 0.70937 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 370 | Classification loss: 0.04981 | Regression loss: 0.14283 | Running loss: 0.70833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 371 | Classification loss: 0.58025 | Regression loss: 0.53192 | Running loss: 0.70915 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 372 | Classification loss: 0.00048 | Regression loss: 0.10973 | Running loss: 0.70795 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 373 | Classification loss: 1.67544 | Regression loss: 0.48710 | Running loss: 0.71085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 374 | Classification loss: 0.20809 | Regression loss: 0.44603 | Running loss: 0.70980 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 375 | Classification loss: 0.18118 | Regression loss: 0.32238 | Running loss: 0.70845 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 376 | Classification loss: 0.25713 | Regression loss: 0.33606 | Running loss: 0.70728 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 377 | Classification loss: 1.19001 | Regression loss: 0.67340 | Running loss: 0.70865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 378 | Classification loss: 0.37245 | Regression loss: 0.44374 | Running loss: 0.70882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 379 | Classification loss: 0.01948 | Regression loss: 0.21738 | Running loss: 0.70729 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 380 | Classification loss: 0.37001 | Regression loss: 0.29955 | Running loss: 0.70718 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 381 | Classification loss: 0.30050 | Regression loss: 0.26394 | Running loss: 0.70609 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 382 | Classification loss: 0.26943 | Regression loss: 0.30863 | Running loss: 0.70633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 383 | Classification loss: 0.32116 | Regression loss: 0.41500 | Running loss: 0.70686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 384 | Classification loss: 1.07951 | Regression loss: 0.60158 | Running loss: 0.70893 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 385 | Classification loss: 0.70258 | Regression loss: 0.54283 | Running loss: 0.71084 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 386 | Classification loss: 2.99653 | Regression loss: 0.23269 | Running loss: 0.71681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 387 | Classification loss: 0.22036 | Regression loss: 0.18999 | Running loss: 0.71523 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 388 | Classification loss: 0.19326 | Regression loss: 0.34015 | Running loss: 0.71601 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 389 | Classification loss: 0.69359 | Regression loss: 0.56293 | Running loss: 0.71810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 390 | Classification loss: 0.29127 | Regression loss: 0.32486 | Running loss: 0.71803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 391 | Classification loss: 1.52022 | Regression loss: 0.47047 | Running loss: 0.71999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 392 | Classification loss: 0.35457 | Regression loss: 0.49362 | Running loss: 0.72129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 393 | Classification loss: 0.39546 | Regression loss: 0.36094 | Running loss: 0.72169 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 394 | Classification loss: 0.53047 | Regression loss: 0.46642 | Running loss: 0.72143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 395 | Classification loss: 0.04245 | Regression loss: 0.23788 | Running loss: 0.72164 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 396 | Classification loss: 0.96487 | Regression loss: 0.59407 | Running loss: 0.72357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 397 | Classification loss: 0.53809 | Regression loss: 0.52113 | Running loss: 0.72535 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 398 | Classification loss: 0.00000 | Regression loss: 0.03662 | Running loss: 0.72306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 399 | Classification loss: 1.90644 | Regression loss: 0.68999 | Running loss: 0.72680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 400 | Classification loss: 0.06359 | Regression loss: 0.24969 | Running loss: 0.72541 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 401 | Classification loss: 0.02093 | Regression loss: 0.14958 | Running loss: 0.72431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 402 | Classification loss: 1.42882 | Regression loss: 0.57600 | Running loss: 0.72610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 403 | Classification loss: 0.42568 | Regression loss: 0.38108 | Running loss: 0.72680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 404 | Classification loss: 0.16726 | Regression loss: 0.36683 | Running loss: 0.72692 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 405 | Classification loss: 0.14175 | Regression loss: 0.27819 | Running loss: 0.72647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 406 | Classification loss: 0.00535 | Regression loss: 0.08120 | Running loss: 0.72607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 407 | Classification loss: 0.74048 | Regression loss: 0.30387 | Running loss: 0.72766 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 408 | Classification loss: 0.11524 | Regression loss: 0.33953 | Running loss: 0.72617 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 409 | Classification loss: 0.01339 | Regression loss: 0.14173 | Running loss: 0.72620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 410 | Classification loss: 0.46195 | Regression loss: 0.35600 | Running loss: 0.72741 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 411 | Classification loss: 0.19163 | Regression loss: 0.38647 | Running loss: 0.72727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 412 | Classification loss: 0.02983 | Regression loss: 0.17077 | Running loss: 0.72564 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 413 | Classification loss: 0.08311 | Regression loss: 0.09768 | Running loss: 0.72561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 414 | Classification loss: 0.79503 | Regression loss: 0.55134 | Running loss: 0.72719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 415 | Classification loss: 0.10196 | Regression loss: 0.24346 | Running loss: 0.72563 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 416 | Classification loss: 0.25024 | Regression loss: 0.22231 | Running loss: 0.72622 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 417 | Classification loss: 0.29141 | Regression loss: 0.37293 | Running loss: 0.72519 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 418 | Classification loss: 0.27578 | Regression loss: 0.21023 | Running loss: 0.72470 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 419 | Classification loss: 0.41647 | Regression loss: 0.47394 | Running loss: 0.72447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 420 | Classification loss: 0.10331 | Regression loss: 0.09305 | Running loss: 0.72342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 421 | Classification loss: 1.10382 | Regression loss: 0.79010 | Running loss: 0.72499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 422 | Classification loss: 2.81314 | Regression loss: 0.38589 | Running loss: 0.73048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 423 | Classification loss: 0.03285 | Regression loss: 0.22049 | Running loss: 0.73003 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 424 | Classification loss: 1.31161 | Regression loss: 0.54046 | Running loss: 0.73244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 425 | Classification loss: 0.00129 | Regression loss: 0.06599 | Running loss: 0.73201 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 426 | Classification loss: 0.07388 | Regression loss: 0.20335 | Running loss: 0.73207 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 427 | Classification loss: 0.43062 | Regression loss: 0.39624 | Running loss: 0.73132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 428 | Classification loss: 0.02151 | Regression loss: 0.18288 | Running loss: 0.73145 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 429 | Classification loss: 0.53103 | Regression loss: 0.42135 | Running loss: 0.73292 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 430 | Classification loss: 0.48128 | Regression loss: 0.47018 | Running loss: 0.73353 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 431 | Classification loss: 0.50729 | Regression loss: 0.29100 | Running loss: 0.73310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 432 | Classification loss: 0.17345 | Regression loss: 0.11047 | Running loss: 0.73327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 433 | Classification loss: 0.01316 | Regression loss: 0.11864 | Running loss: 0.73242 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 434 | Classification loss: 0.13008 | Regression loss: 0.21524 | Running loss: 0.73086 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 435 | Classification loss: 0.01437 | Regression loss: 0.38598 | Running loss: 0.73131 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 436 | Classification loss: 0.42258 | Regression loss: 0.46621 | Running loss: 0.73190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 437 | Classification loss: 0.01510 | Regression loss: 0.20033 | Running loss: 0.72997 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 438 | Classification loss: 0.47732 | Regression loss: 0.41637 | Running loss: 0.73030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 439 | Classification loss: 0.01003 | Regression loss: 0.07718 | Running loss: 0.72846 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 440 | Classification loss: 0.00080 | Regression loss: 0.00000 | Running loss: 0.72702 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 441 | Classification loss: 0.01609 | Regression loss: 0.22474 | Running loss: 0.72528 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 442 | Classification loss: 1.09973 | Regression loss: 0.47094 | Running loss: 0.72751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 443 | Classification loss: 1.02032 | Regression loss: 0.41306 | Running loss: 0.72943 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 444 | Classification loss: 0.56000 | Regression loss: 0.38949 | Running loss: 0.73004 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 445 | Classification loss: 1.06114 | Regression loss: 0.62362 | Running loss: 0.73283 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 446 | Classification loss: 0.37078 | Regression loss: 0.43975 | Running loss: 0.73396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 447 | Classification loss: 0.00224 | Regression loss: 0.19191 | Running loss: 0.73195 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 448 | Classification loss: 0.01188 | Regression loss: 0.25513 | Running loss: 0.73220 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 449 | Classification loss: 0.18330 | Regression loss: 0.63669 | Running loss: 0.73341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 450 | Classification loss: 0.62762 | Regression loss: 0.54470 | Running loss: 0.73446 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 451 | Classification loss: 0.52877 | Regression loss: 0.53054 | Running loss: 0.73455 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 452 | Classification loss: 0.41503 | Regression loss: 0.40744 | Running loss: 0.73580 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 453 | Classification loss: 0.17413 | Regression loss: 0.35328 | Running loss: 0.73574 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 454 | Classification loss: 0.49113 | Regression loss: 0.38865 | Running loss: 0.73525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 455 | Classification loss: 0.18358 | Regression loss: 0.38007 | Running loss: 0.73602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 456 | Classification loss: 0.00380 | Regression loss: 0.19839 | Running loss: 0.73524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 457 | Classification loss: 0.09331 | Regression loss: 0.21706 | Running loss: 0.73552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 458 | Classification loss: 0.98597 | Regression loss: 0.40149 | Running loss: 0.73594 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 459 | Classification loss: 0.13258 | Regression loss: 0.10270 | Running loss: 0.73495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 460 | Classification loss: 0.01513 | Regression loss: 0.08233 | Running loss: 0.73314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 461 | Classification loss: 0.08164 | Regression loss: 0.47861 | Running loss: 0.73281 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 462 | Classification loss: 0.03318 | Regression loss: 0.16391 | Running loss: 0.73099 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 463 | Classification loss: 0.18244 | Regression loss: 0.25875 | Running loss: 0.73096 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 464 | Classification loss: 0.10098 | Regression loss: 0.18320 | Running loss: 0.73058 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 465 | Classification loss: 0.45718 | Regression loss: 0.33466 | Running loss: 0.73087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 466 | Classification loss: 0.09733 | Regression loss: 0.21221 | Running loss: 0.73091 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 467 | Classification loss: 1.79461 | Regression loss: 0.70775 | Running loss: 0.73543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 468 | Classification loss: 0.89935 | Regression loss: 0.61735 | Running loss: 0.73606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 469 | Classification loss: 0.26545 | Regression loss: 0.32234 | Running loss: 0.73695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 470 | Classification loss: 0.30577 | Regression loss: 0.34273 | Running loss: 0.73782 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 471 | Classification loss: 0.45251 | Regression loss: 0.40484 | Running loss: 0.73824 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 472 | Classification loss: 0.35838 | Regression loss: 0.55039 | Running loss: 0.73802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 473 | Classification loss: 2.74863 | Regression loss: 0.93607 | Running loss: 0.74500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 474 | Classification loss: 0.13008 | Regression loss: 0.15862 | Running loss: 0.74447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 475 | Classification loss: 0.55467 | Regression loss: 0.38187 | Running loss: 0.74409 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 476 | Classification loss: 1.69930 | Regression loss: 0.64083 | Running loss: 0.74841 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 477 | Classification loss: 0.50464 | Regression loss: 0.58593 | Running loss: 0.74940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 478 | Classification loss: 1.41885 | Regression loss: 0.56616 | Running loss: 0.75304 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 479 | Classification loss: 0.63365 | Regression loss: 0.61419 | Running loss: 0.75317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 480 | Classification loss: 0.29727 | Regression loss: 0.55333 | Running loss: 0.75342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 481 | Classification loss: 1.04813 | Regression loss: 0.46321 | Running loss: 0.75443 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 482 | Classification loss: 0.57117 | Regression loss: 0.51618 | Running loss: 0.75516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 483 | Classification loss: 0.75616 | Regression loss: 0.70861 | Running loss: 0.75587 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 484 | Classification loss: 1.78669 | Regression loss: 0.70442 | Running loss: 0.75994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 485 | Classification loss: 0.04939 | Regression loss: 0.19107 | Running loss: 0.75947 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 486 | Classification loss: 0.26634 | Regression loss: 0.33446 | Running loss: 0.75938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 487 | Classification loss: 0.01420 | Regression loss: 0.14199 | Running loss: 0.75912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 488 | Classification loss: 1.64356 | Regression loss: 0.58369 | Running loss: 0.76308 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 489 | Classification loss: 0.40670 | Regression loss: 0.50275 | Running loss: 0.76250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 490 | Classification loss: 2.52120 | Regression loss: 0.65213 | Running loss: 0.76856 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 491 | Classification loss: 0.23282 | Regression loss: 0.25654 | Running loss: 0.76912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 492 | Classification loss: 0.32226 | Regression loss: 0.23041 | Running loss: 0.76892 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 493 | Classification loss: 0.99361 | Regression loss: 0.52846 | Running loss: 0.76994 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 494 | Classification loss: 0.01102 | Regression loss: 0.09443 | Running loss: 0.76976 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 495 | Classification loss: 0.07582 | Regression loss: 0.12149 | Running loss: 0.76904 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 496 | Classification loss: 0.09369 | Regression loss: 0.25856 | Running loss: 0.76749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 497 | Classification loss: 1.16843 | Regression loss: 0.59531 | Running loss: 0.77066 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 498 | Classification loss: 0.69456 | Regression loss: 0.42633 | Running loss: 0.77172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 499 | Classification loss: 0.08877 | Regression loss: 0.19041 | Running loss: 0.77194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 500 | Classification loss: 0.04197 | Regression loss: 0.13444 | Running loss: 0.76993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 501 | Classification loss: 0.33553 | Regression loss: 0.42994 | Running loss: 0.77001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 502 | Classification loss: 0.05345 | Regression loss: 0.10698 | Running loss: 0.76832 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 503 | Classification loss: 0.67709 | Regression loss: 0.49409 | Running loss: 0.76921 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 504 | Classification loss: 0.54266 | Regression loss: 0.57447 | Running loss: 0.76923 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 505 | Classification loss: 0.36255 | Regression loss: 0.42466 | Running loss: 0.76989 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 506 | Classification loss: 0.56520 | Regression loss: 0.30319 | Running loss: 0.77068 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 507 | Classification loss: 0.05511 | Regression loss: 0.20406 | Running loss: 0.76990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 508 | Classification loss: 0.42360 | Regression loss: 0.53228 | Running loss: 0.77124 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 509 | Classification loss: 0.37252 | Regression loss: 0.29665 | Running loss: 0.77209 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 510 | Classification loss: 0.04058 | Regression loss: 0.24478 | Running loss: 0.77026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 511 | Classification loss: 0.28762 | Regression loss: 0.40106 | Running loss: 0.77135 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 512 | Classification loss: 0.04061 | Regression loss: 0.19008 | Running loss: 0.77139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 513 | Classification loss: 0.00445 | Regression loss: 0.13637 | Running loss: 0.77037 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 514 | Classification loss: 0.09795 | Regression loss: 0.28514 | Running loss: 0.76911 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 515 | Classification loss: 0.71317 | Regression loss: 0.65456 | Running loss: 0.77145 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 516 | Classification loss: 0.82209 | Regression loss: 0.44255 | Running loss: 0.77287 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 517 | Classification loss: 0.08786 | Regression loss: 0.16831 | Running loss: 0.77113 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 518 | Classification loss: 0.86448 | Regression loss: 0.69306 | Running loss: 0.77389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 519 | Classification loss: 0.18642 | Regression loss: 0.32347 | Running loss: 0.77372 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 520 | Classification loss: 0.05290 | Regression loss: 0.14575 | Running loss: 0.77378 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 521 | Classification loss: 0.00244 | Regression loss: 0.07000 | Running loss: 0.77360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 522 | Classification loss: 0.57170 | Regression loss: 0.31744 | Running loss: 0.77519 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 523 | Classification loss: 0.00619 | Regression loss: 0.16272 | Running loss: 0.77413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 524 | Classification loss: 0.01129 | Regression loss: 0.09726 | Running loss: 0.77028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 525 | Classification loss: 0.63522 | Regression loss: 0.48398 | Running loss: 0.77186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 526 | Classification loss: 0.04616 | Regression loss: 0.20922 | Running loss: 0.77131 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 527 | Classification loss: 0.03793 | Regression loss: 0.22947 | Running loss: 0.76816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 528 | Classification loss: 0.71757 | Regression loss: 0.43525 | Running loss: 0.76850 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 529 | Classification loss: 0.00064 | Regression loss: 0.07975 | Running loss: 0.76850 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 530 | Classification loss: 0.06062 | Regression loss: 0.21648 | Running loss: 0.76860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 531 | Classification loss: 0.35658 | Regression loss: 0.43987 | Running loss: 0.76690 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 532 | Classification loss: 0.81676 | Regression loss: 0.06450 | Running loss: 0.76727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 533 | Classification loss: 0.22906 | Regression loss: 0.28746 | Running loss: 0.76650 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 534 | Classification loss: 0.01618 | Regression loss: 0.14560 | Running loss: 0.76559 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 535 | Classification loss: 0.53888 | Regression loss: 0.55185 | Running loss: 0.76669 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 536 | Classification loss: 0.13849 | Regression loss: 0.25428 | Running loss: 0.76670 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 537 | Classification loss: 0.16532 | Regression loss: 0.36894 | Running loss: 0.76630 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 538 | Classification loss: 0.38167 | Regression loss: 0.42912 | Running loss: 0.76584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 539 | Classification loss: 1.10260 | Regression loss: 0.18616 | Running loss: 0.76835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 540 | Classification loss: 1.18389 | Regression loss: 0.30824 | Running loss: 0.76953 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 541 | Classification loss: 0.02520 | Regression loss: 0.21633 | Running loss: 0.76904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 542 | Classification loss: 0.26284 | Regression loss: 0.17930 | Running loss: 0.76957 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 543 | Classification loss: 0.38248 | Regression loss: 0.23750 | Running loss: 0.77001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 544 | Classification loss: 0.02004 | Regression loss: 0.17215 | Running loss: 0.76996 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 545 | Classification loss: 0.35313 | Regression loss: 0.42114 | Running loss: 0.77122 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 546 | Classification loss: 0.26796 | Regression loss: 0.40504 | Running loss: 0.76835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 547 | Classification loss: 0.08833 | Regression loss: 0.17038 | Running loss: 0.76357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 548 | Classification loss: 0.01704 | Regression loss: 0.14552 | Running loss: 0.76302 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 549 | Classification loss: 0.43327 | Regression loss: 0.74142 | Running loss: 0.76451 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 550 | Classification loss: 0.11214 | Regression loss: 0.35362 | Running loss: 0.76453 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 551 | Classification loss: 0.86309 | Regression loss: 0.60333 | Running loss: 0.76485 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 552 | Classification loss: 0.09458 | Regression loss: 0.18882 | Running loss: 0.76430 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 553 | Classification loss: 0.07575 | Regression loss: 0.31844 | Running loss: 0.76454 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 554 | Classification loss: 0.73021 | Regression loss: 0.41252 | Running loss: 0.76525 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 555 | Classification loss: 0.08072 | Regression loss: 0.13007 | Running loss: 0.76467 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 556 | Classification loss: 0.96851 | Regression loss: 0.59609 | Running loss: 0.76725 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 557 | Classification loss: 0.41157 | Regression loss: 0.34632 | Running loss: 0.76748 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 558 | Classification loss: 0.38105 | Regression loss: 0.26238 | Running loss: 0.76811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 559 | Classification loss: 0.97420 | Regression loss: 0.56320 | Running loss: 0.77114 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 560 | Classification loss: 0.25828 | Regression loss: 0.50929 | Running loss: 0.77184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 561 | Classification loss: 0.75634 | Regression loss: 0.32247 | Running loss: 0.77267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 562 | Classification loss: 0.36999 | Regression loss: 0.38061 | Running loss: 0.77225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 563 | Classification loss: 0.16602 | Regression loss: 0.34594 | Running loss: 0.77256 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 564 | Classification loss: 0.06832 | Regression loss: 0.15042 | Running loss: 0.77289 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 565 | Classification loss: 0.09352 | Regression loss: 0.23691 | Running loss: 0.77024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 566 | Classification loss: 0.06951 | Regression loss: 0.28885 | Running loss: 0.76877 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 567 | Classification loss: 0.02353 | Regression loss: 0.11832 | Running loss: 0.76868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 568 | Classification loss: 0.35351 | Regression loss: 0.20174 | Running loss: 0.76727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 569 | Classification loss: 0.01675 | Regression loss: 0.05647 | Running loss: 0.76731 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 570 | Classification loss: 0.03946 | Regression loss: 0.15249 | Running loss: 0.76738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 571 | Classification loss: 0.29288 | Regression loss: 0.55272 | Running loss: 0.76610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 572 | Classification loss: 0.38752 | Regression loss: 0.48211 | Running loss: 0.76695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 573 | Classification loss: 0.67258 | Regression loss: 0.48911 | Running loss: 0.76858 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 574 | Classification loss: 0.11177 | Regression loss: 0.21324 | Running loss: 0.76620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 575 | Classification loss: 0.74476 | Regression loss: 0.54257 | Running loss: 0.76785 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 576 | Classification loss: 0.55818 | Regression loss: 0.49844 | Running loss: 0.76979 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 577 | Classification loss: 0.17889 | Regression loss: 0.36560 | Running loss: 0.76891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 578 | Classification loss: 0.00333 | Regression loss: 0.11730 | Running loss: 0.76816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 579 | Classification loss: 0.00160 | Regression loss: 0.11226 | Running loss: 0.76640 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 580 | Classification loss: 0.60083 | Regression loss: 0.24260 | Running loss: 0.76695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 581 | Classification loss: 0.02562 | Regression loss: 0.09066 | Running loss: 0.76648 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 582 | Classification loss: 0.28956 | Regression loss: 0.22074 | Running loss: 0.76632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 583 | Classification loss: 0.07303 | Regression loss: 0.16154 | Running loss: 0.76515 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 584 | Classification loss: 0.57348 | Regression loss: 0.37237 | Running loss: 0.76672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 585 | Classification loss: 0.09522 | Regression loss: 0.30395 | Running loss: 0.76672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 586 | Classification loss: 2.15894 | Regression loss: 0.74329 | Running loss: 0.77015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 587 | Classification loss: 0.24835 | Regression loss: 0.23176 | Running loss: 0.76883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 588 | Classification loss: 1.57925 | Regression loss: 0.53225 | Running loss: 0.77272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 589 | Classification loss: 0.02465 | Regression loss: 0.13741 | Running loss: 0.77236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 590 | Classification loss: 0.37535 | Regression loss: 0.47095 | Running loss: 0.77353 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 591 | Classification loss: 1.50494 | Regression loss: 0.68257 | Running loss: 0.77461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 592 | Classification loss: 0.37603 | Regression loss: 0.48162 | Running loss: 0.77348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 593 | Classification loss: 0.00290 | Regression loss: 0.15665 | Running loss: 0.77288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 594 | Classification loss: 0.67544 | Regression loss: 0.56216 | Running loss: 0.77499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 595 | Classification loss: 0.19398 | Regression loss: 0.31890 | Running loss: 0.77547 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 596 | Classification loss: 0.00737 | Regression loss: 0.04404 | Running loss: 0.77459 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 597 | Classification loss: 0.00699 | Regression loss: 0.16042 | Running loss: 0.77229 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 598 | Classification loss: 0.04003 | Regression loss: 0.27925 | Running loss: 0.77101 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 599 | Classification loss: 0.37926 | Regression loss: 0.55328 | Running loss: 0.77196 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 600 | Classification loss: 0.28008 | Regression loss: 0.26387 | Running loss: 0.77236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 601 | Classification loss: 0.12034 | Regression loss: 0.28539 | Running loss: 0.77213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 602 | Classification loss: 0.22153 | Regression loss: 0.34721 | Running loss: 0.77304 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 603 | Classification loss: 0.20732 | Regression loss: 0.32046 | Running loss: 0.77261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 604 | Classification loss: 0.42163 | Regression loss: 0.16307 | Running loss: 0.77315 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 605 | Classification loss: 0.59574 | Regression loss: 0.73866 | Running loss: 0.77471 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 606 | Classification loss: 0.00555 | Regression loss: 0.17070 | Running loss: 0.76979 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 607 | Classification loss: 0.51525 | Regression loss: 0.43708 | Running loss: 0.77083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 608 | Classification loss: 0.01657 | Regression loss: 0.13482 | Running loss: 0.77085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 609 | Classification loss: 0.29601 | Regression loss: 0.46700 | Running loss: 0.76985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 610 | Classification loss: 1.19250 | Regression loss: 0.62252 | Running loss: 0.77310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 611 | Classification loss: 0.02401 | Regression loss: 0.19685 | Running loss: 0.77304 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 612 | Classification loss: 0.56743 | Regression loss: 0.49068 | Running loss: 0.77236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 613 | Classification loss: 0.66373 | Regression loss: 0.42178 | Running loss: 0.77405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 614 | Classification loss: 0.02538 | Regression loss: 0.15514 | Running loss: 0.77142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 615 | Classification loss: 0.88256 | Regression loss: 0.31621 | Running loss: 0.77361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 616 | Classification loss: 0.40858 | Regression loss: 0.48036 | Running loss: 0.77097 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 617 | Classification loss: 1.16285 | Regression loss: 0.52215 | Running loss: 0.77335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 618 | Classification loss: 0.14258 | Regression loss: 0.62225 | Running loss: 0.77275 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 619 | Classification loss: 0.16109 | Regression loss: 0.16072 | Running loss: 0.77293 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 620 | Classification loss: 0.31409 | Regression loss: 0.40536 | Running loss: 0.77272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 621 | Classification loss: 0.01684 | Regression loss: 0.20461 | Running loss: 0.77276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 622 | Classification loss: 0.07108 | Regression loss: 0.06714 | Running loss: 0.77277 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 623 | Classification loss: 0.43354 | Regression loss: 0.64296 | Running loss: 0.77463 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 624 | Classification loss: 0.95902 | Regression loss: 0.45025 | Running loss: 0.77632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 625 | Classification loss: 0.01711 | Regression loss: 0.17846 | Running loss: 0.77561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 626 | Classification loss: 0.30983 | Regression loss: 0.32212 | Running loss: 0.77636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 627 | Classification loss: 0.04080 | Regression loss: 0.25918 | Running loss: 0.77440 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 628 | Classification loss: 0.01077 | Regression loss: 0.15868 | Running loss: 0.77327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 629 | Classification loss: 0.41578 | Regression loss: 0.26409 | Running loss: 0.77436 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 630 | Classification loss: 1.18477 | Regression loss: 0.56802 | Running loss: 0.77446 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 631 | Classification loss: 0.00140 | Regression loss: 0.09001 | Running loss: 0.77433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 632 | Classification loss: 0.28425 | Regression loss: 0.21461 | Running loss: 0.77327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 633 | Classification loss: 0.20997 | Regression loss: 0.27776 | Running loss: 0.77166 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 634 | Classification loss: 0.03161 | Regression loss: 0.18801 | Running loss: 0.76972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 635 | Classification loss: 0.89368 | Regression loss: 0.50588 | Running loss: 0.76997 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 636 | Classification loss: 0.54100 | Regression loss: 0.68210 | Running loss: 0.77191 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 637 | Classification loss: 0.01010 | Regression loss: 0.18045 | Running loss: 0.76999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 638 | Classification loss: 0.07214 | Regression loss: 0.16172 | Running loss: 0.76810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 639 | Classification loss: 0.14212 | Regression loss: 0.28614 | Running loss: 0.76658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 640 | Classification loss: 0.70600 | Regression loss: 0.39332 | Running loss: 0.76795 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 641 | Classification loss: 1.23955 | Regression loss: 0.15809 | Running loss: 0.76948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 642 | Classification loss: 0.69810 | Regression loss: 0.39518 | Running loss: 0.77053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 643 | Classification loss: 0.05650 | Regression loss: 0.26979 | Running loss: 0.76837 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 644 | Classification loss: 0.00149 | Regression loss: 0.19567 | Running loss: 0.76787 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 645 | Classification loss: 0.64164 | Regression loss: 0.51439 | Running loss: 0.76761 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 646 | Classification loss: 0.57056 | Regression loss: 0.64660 | Running loss: 0.76842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 647 | Classification loss: 0.03241 | Regression loss: 0.07394 | Running loss: 0.76817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 648 | Classification loss: 0.11089 | Regression loss: 0.22084 | Running loss: 0.76874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 649 | Classification loss: 0.07948 | Regression loss: 0.42146 | Running loss: 0.76836 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 650 | Classification loss: 1.58429 | Regression loss: 0.76127 | Running loss: 0.77117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 651 | Classification loss: 0.46371 | Regression loss: 0.27571 | Running loss: 0.77199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 652 | Classification loss: 0.25403 | Regression loss: 0.39599 | Running loss: 0.77123 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 653 | Classification loss: 0.92849 | Regression loss: 0.55008 | Running loss: 0.77401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 654 | Classification loss: 0.03290 | Regression loss: 0.23045 | Running loss: 0.77311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 655 | Classification loss: 0.25473 | Regression loss: 0.35692 | Running loss: 0.77426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 656 | Classification loss: 0.59138 | Regression loss: 0.46263 | Running loss: 0.77235 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 657 | Classification loss: 0.53991 | Regression loss: 0.58185 | Running loss: 0.77248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 658 | Classification loss: 0.09114 | Regression loss: 0.16106 | Running loss: 0.77258 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 659 | Classification loss: 0.25880 | Regression loss: 0.40187 | Running loss: 0.77095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 660 | Classification loss: 0.17112 | Regression loss: 0.32349 | Running loss: 0.76990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 661 | Classification loss: 0.11172 | Regression loss: 0.03077 | Running loss: 0.76872 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 662 | Classification loss: 0.22372 | Regression loss: 0.23617 | Running loss: 0.76753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 663 | Classification loss: 0.11096 | Regression loss: 0.25592 | Running loss: 0.76694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 664 | Classification loss: 0.16130 | Regression loss: 0.24677 | Running loss: 0.76642 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 665 | Classification loss: 0.10111 | Regression loss: 0.27985 | Running loss: 0.76527 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 666 | Classification loss: 0.80010 | Regression loss: 0.35541 | Running loss: 0.76596 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 667 | Classification loss: 0.00067 | Regression loss: 0.03999 | Running loss: 0.76282 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 668 | Classification loss: 0.18409 | Regression loss: 0.18573 | Running loss: 0.76249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 669 | Classification loss: 1.35961 | Regression loss: 0.50613 | Running loss: 0.76532 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 670 | Classification loss: 0.15264 | Regression loss: 0.15225 | Running loss: 0.76567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 671 | Classification loss: 1.08353 | Regression loss: 0.60356 | Running loss: 0.76892 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 672 | Classification loss: 0.57810 | Regression loss: 0.59596 | Running loss: 0.77053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 673 | Classification loss: 0.00884 | Regression loss: 0.13026 | Running loss: 0.76791 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 674 | Classification loss: 0.19796 | Regression loss: 0.20282 | Running loss: 0.76827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 675 | Classification loss: 0.05510 | Regression loss: 0.19001 | Running loss: 0.76721 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 676 | Classification loss: 0.04181 | Regression loss: 0.13577 | Running loss: 0.76410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 677 | Classification loss: 0.96991 | Regression loss: 0.43239 | Running loss: 0.76297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 678 | Classification loss: 0.10959 | Regression loss: 0.32986 | Running loss: 0.76355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 679 | Classification loss: 0.17972 | Regression loss: 0.34106 | Running loss: 0.76165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 680 | Classification loss: 0.37567 | Regression loss: 0.43608 | Running loss: 0.76318 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 681 | Classification loss: 0.27687 | Regression loss: 0.28741 | Running loss: 0.76348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 682 | Classification loss: 0.50734 | Regression loss: 0.61715 | Running loss: 0.76404 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 683 | Classification loss: 0.00106 | Regression loss: 0.03871 | Running loss: 0.76295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 684 | Classification loss: 0.63943 | Regression loss: 0.41554 | Running loss: 0.76345 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 685 | Classification loss: 0.81244 | Regression loss: 0.56904 | Running loss: 0.76502 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 686 | Classification loss: 0.55562 | Regression loss: 0.66196 | Running loss: 0.76649 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 687 | Classification loss: 0.14925 | Regression loss: 0.29070 | Running loss: 0.76584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 688 | Classification loss: 0.26400 | Regression loss: 0.45840 | Running loss: 0.76673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 689 | Classification loss: 0.04259 | Regression loss: 0.08959 | Running loss: 0.76594 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 690 | Classification loss: 0.82139 | Regression loss: 0.84111 | Running loss: 0.76813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 691 | Classification loss: 0.14547 | Regression loss: 0.27066 | Running loss: 0.76757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 692 | Classification loss: 0.53453 | Regression loss: 0.54410 | Running loss: 0.76827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 693 | Classification loss: 0.34178 | Regression loss: 0.27966 | Running loss: 0.76890 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 694 | Classification loss: 2.75699 | Regression loss: 0.62632 | Running loss: 0.77229 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 695 | Classification loss: 0.04915 | Regression loss: 0.09659 | Running loss: 0.77230 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 696 | Classification loss: 0.20009 | Regression loss: 0.39343 | Running loss: 0.77038 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 697 | Classification loss: 0.20590 | Regression loss: 0.26315 | Running loss: 0.77096 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 698 | Classification loss: 0.19101 | Regression loss: 0.24166 | Running loss: 0.76795 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 699 | Classification loss: 0.00724 | Regression loss: 0.11114 | Running loss: 0.76603 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 700 | Classification loss: 0.35866 | Regression loss: 0.55402 | Running loss: 0.76709 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 701 | Classification loss: 0.00857 | Regression loss: 0.08126 | Running loss: 0.76523 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 702 | Classification loss: 0.31601 | Regression loss: 0.30284 | Running loss: 0.76540 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 703 | Classification loss: 0.04457 | Regression loss: 0.16877 | Running loss: 0.76553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 704 | Classification loss: 0.09414 | Regression loss: 0.22991 | Running loss: 0.76552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 705 | Classification loss: 0.04489 | Regression loss: 0.13904 | Running loss: 0.76535 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 706 | Classification loss: 0.34702 | Regression loss: 0.38083 | Running loss: 0.76602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 707 | Classification loss: 0.17945 | Regression loss: 0.52528 | Running loss: 0.76732 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 708 | Classification loss: 0.05221 | Regression loss: 0.22237 | Running loss: 0.76726 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 709 | Classification loss: 0.13588 | Regression loss: 0.29264 | Running loss: 0.76667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 710 | Classification loss: 0.50626 | Regression loss: 0.68948 | Running loss: 0.76610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 711 | Classification loss: 0.09830 | Regression loss: 0.13993 | Running loss: 0.76605 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 712 | Classification loss: 0.06560 | Regression loss: 0.23512 | Running loss: 0.76476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 713 | Classification loss: 0.99633 | Regression loss: 0.63811 | Running loss: 0.76792 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 714 | Classification loss: 0.08309 | Regression loss: 0.26531 | Running loss: 0.76597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 715 | Classification loss: 0.47229 | Regression loss: 0.20234 | Running loss: 0.76597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 716 | Classification loss: 0.06657 | Regression loss: 0.29792 | Running loss: 0.76495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 717 | Classification loss: 0.15643 | Regression loss: 0.39849 | Running loss: 0.76492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 718 | Classification loss: 0.04502 | Regression loss: 0.12083 | Running loss: 0.76506 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 719 | Classification loss: 0.00249 | Regression loss: 0.05114 | Running loss: 0.76488 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 720 | Classification loss: 0.26435 | Regression loss: 0.34917 | Running loss: 0.76419 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 721 | Classification loss: 0.72196 | Regression loss: 0.53470 | Running loss: 0.76639 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 722 | Classification loss: 0.34408 | Regression loss: 0.32793 | Running loss: 0.76634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 723 | Classification loss: 0.99616 | Regression loss: 0.45260 | Running loss: 0.76880 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 724 | Classification loss: 0.70472 | Regression loss: 0.53303 | Running loss: 0.76939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 725 | Classification loss: 0.17533 | Regression loss: 0.39804 | Running loss: 0.76868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 726 | Classification loss: 0.16027 | Regression loss: 0.28208 | Running loss: 0.76852 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 727 | Classification loss: 0.00923 | Regression loss: 0.15379 | Running loss: 0.76848 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 728 | Classification loss: 1.98612 | Regression loss: 0.40647 | Running loss: 0.77298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 729 | Classification loss: 0.04423 | Regression loss: 0.18968 | Running loss: 0.77252 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 730 | Classification loss: 0.87015 | Regression loss: 0.55216 | Running loss: 0.77271 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 731 | Classification loss: 0.28871 | Regression loss: 0.59140 | Running loss: 0.77218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 732 | Classification loss: 0.39686 | Regression loss: 0.49404 | Running loss: 0.76934 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 733 | Classification loss: 0.31827 | Regression loss: 0.46206 | Running loss: 0.77052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 734 | Classification loss: 0.15305 | Regression loss: 0.29639 | Running loss: 0.77080 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 735 | Classification loss: 0.15653 | Regression loss: 0.24875 | Running loss: 0.77071 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 736 | Classification loss: 0.41348 | Regression loss: 0.52499 | Running loss: 0.77149 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 737 | Classification loss: 0.08245 | Regression loss: 0.27390 | Running loss: 0.77166 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 738 | Classification loss: 0.00398 | Regression loss: 0.07635 | Running loss: 0.76846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 739 | Classification loss: 0.32205 | Regression loss: 0.25022 | Running loss: 0.76511 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 740 | Classification loss: 0.31828 | Regression loss: 0.37878 | Running loss: 0.76313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 741 | Classification loss: 0.05024 | Regression loss: 0.13470 | Running loss: 0.76089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 742 | Classification loss: 0.13789 | Regression loss: 0.20017 | Running loss: 0.76011 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 743 | Classification loss: 0.37456 | Regression loss: 0.35958 | Running loss: 0.75945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 744 | Classification loss: 0.68405 | Regression loss: 0.30608 | Running loss: 0.75774 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 745 | Classification loss: 0.49657 | Regression loss: 0.50240 | Running loss: 0.75740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 746 | Classification loss: 0.37935 | Regression loss: 0.55059 | Running loss: 0.75490 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 747 | Classification loss: 0.04131 | Regression loss: 0.24046 | Running loss: 0.75508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 748 | Classification loss: 0.60671 | Regression loss: 0.28613 | Running loss: 0.75647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 749 | Classification loss: 0.09997 | Regression loss: 0.28165 | Running loss: 0.75544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 750 | Classification loss: 0.18243 | Regression loss: 0.19034 | Running loss: 0.75448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 751 | Classification loss: 0.01886 | Regression loss: 0.21003 | Running loss: 0.75477 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 752 | Classification loss: 0.02708 | Regression loss: 0.24509 | Running loss: 0.75327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 753 | Classification loss: 0.11461 | Regression loss: 0.43864 | Running loss: 0.75400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 754 | Classification loss: 0.37412 | Regression loss: 0.40794 | Running loss: 0.75524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 755 | Classification loss: 0.13479 | Regression loss: 0.32460 | Running loss: 0.75588 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 756 | Classification loss: 0.36038 | Regression loss: 0.17638 | Running loss: 0.75596 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 757 | Classification loss: 0.61948 | Regression loss: 0.50303 | Running loss: 0.75768 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 758 | Classification loss: 0.00808 | Regression loss: 0.03748 | Running loss: 0.75754 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 759 | Classification loss: 0.00011 | Regression loss: 0.01249 | Running loss: 0.75521 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 760 | Classification loss: 0.40993 | Regression loss: 0.57576 | Running loss: 0.75542 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 761 | Classification loss: 1.18670 | Regression loss: 0.52024 | Running loss: 0.75853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 762 | Classification loss: 0.81409 | Regression loss: 0.62568 | Running loss: 0.76089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 763 | Classification loss: 0.06989 | Regression loss: 0.34447 | Running loss: 0.76150 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 764 | Classification loss: 0.22968 | Regression loss: 0.28233 | Running loss: 0.75898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 765 | Classification loss: 1.67179 | Regression loss: 0.32112 | Running loss: 0.76267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 766 | Classification loss: 1.13354 | Regression loss: 0.58426 | Running loss: 0.76476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 767 | Classification loss: 0.03603 | Regression loss: 0.14431 | Running loss: 0.76468 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 768 | Classification loss: 1.12852 | Regression loss: 0.49449 | Running loss: 0.76656 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 769 | Classification loss: 0.08180 | Regression loss: 0.24278 | Running loss: 0.76198 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 770 | Classification loss: 0.01836 | Regression loss: 0.11541 | Running loss: 0.76052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 771 | Classification loss: 0.31809 | Regression loss: 0.22608 | Running loss: 0.75985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 772 | Classification loss: 0.73943 | Regression loss: 0.63762 | Running loss: 0.76025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 773 | Classification loss: 0.02330 | Regression loss: 0.26972 | Running loss: 0.76059 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 774 | Classification loss: 0.25178 | Regression loss: 0.22929 | Running loss: 0.76116 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 775 | Classification loss: 0.14129 | Regression loss: 0.27408 | Running loss: 0.76145 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 776 | Classification loss: 0.00036 | Regression loss: 0.06112 | Running loss: 0.76040 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 777 | Classification loss: 0.51664 | Regression loss: 0.48269 | Running loss: 0.75760 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 778 | Classification loss: 0.59237 | Regression loss: 0.38934 | Running loss: 0.75802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 779 | Classification loss: 0.09018 | Regression loss: 0.17073 | Running loss: 0.75673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 780 | Classification loss: 0.01080 | Regression loss: 0.09034 | Running loss: 0.75500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 781 | Classification loss: 0.00013 | Regression loss: 0.02352 | Running loss: 0.75483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 782 | Classification loss: 0.45566 | Regression loss: 0.09760 | Running loss: 0.75456 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 783 | Classification loss: 0.22380 | Regression loss: 0.35901 | Running loss: 0.75202 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 784 | Classification loss: 0.50769 | Regression loss: 0.50731 | Running loss: 0.75361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 785 | Classification loss: 0.06318 | Regression loss: 0.31671 | Running loss: 0.75357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 786 | Classification loss: 0.34238 | Regression loss: 0.12152 | Running loss: 0.75316 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 787 | Classification loss: 0.96872 | Regression loss: 0.36906 | Running loss: 0.75369 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 788 | Classification loss: 1.81055 | Regression loss: 0.53104 | Running loss: 0.75671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 789 | Classification loss: 0.21664 | Regression loss: 0.30394 | Running loss: 0.75614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 790 | Classification loss: 0.00770 | Regression loss: 0.11508 | Running loss: 0.75514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 791 | Classification loss: 0.70987 | Regression loss: 0.31332 | Running loss: 0.75600 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 792 | Classification loss: 0.52546 | Regression loss: 0.47096 | Running loss: 0.75788 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 793 | Classification loss: 0.90302 | Regression loss: 0.42612 | Running loss: 0.75897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 794 | Classification loss: 0.12042 | Regression loss: 0.30920 | Running loss: 0.75754 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 795 | Classification loss: 0.23334 | Regression loss: 0.33073 | Running loss: 0.75747 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 796 | Classification loss: 0.37095 | Regression loss: 0.37299 | Running loss: 0.75755 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 797 | Classification loss: 0.05605 | Regression loss: 0.16640 | Running loss: 0.75749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 798 | Classification loss: 0.15337 | Regression loss: 0.12397 | Running loss: 0.75647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 799 | Classification loss: 0.06118 | Regression loss: 0.17491 | Running loss: 0.75088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 800 | Classification loss: 0.05758 | Regression loss: 0.09943 | Running loss: 0.74774 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 801 | Classification loss: 0.08221 | Regression loss: 0.26818 | Running loss: 0.74734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 802 | Classification loss: 0.01409 | Regression loss: 0.16486 | Running loss: 0.74659 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 803 | Classification loss: 0.65770 | Regression loss: 0.39766 | Running loss: 0.74743 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 804 | Classification loss: 0.02879 | Regression loss: 0.16851 | Running loss: 0.74719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 805 | Classification loss: 0.25058 | Regression loss: 0.21690 | Running loss: 0.74476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 806 | Classification loss: 0.02465 | Regression loss: 0.08871 | Running loss: 0.74472 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 807 | Classification loss: 0.13053 | Regression loss: 0.57062 | Running loss: 0.74480 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 808 | Classification loss: 0.57331 | Regression loss: 0.51070 | Running loss: 0.74613 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 809 | Classification loss: 0.25780 | Regression loss: 0.24746 | Running loss: 0.74634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 810 | Classification loss: 0.16114 | Regression loss: 0.06954 | Running loss: 0.74582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 811 | Classification loss: 0.51425 | Regression loss: 0.46693 | Running loss: 0.74565 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 812 | Classification loss: 0.52520 | Regression loss: 0.16309 | Running loss: 0.74514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 813 | Classification loss: 0.36077 | Regression loss: 0.53700 | Running loss: 0.74502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 814 | Classification loss: 0.28951 | Regression loss: 0.35670 | Running loss: 0.74467 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 815 | Classification loss: 0.52969 | Regression loss: 0.48777 | Running loss: 0.74561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 816 | Classification loss: 0.81989 | Regression loss: 0.66867 | Running loss: 0.74821 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 817 | Classification loss: 0.69276 | Regression loss: 0.46875 | Running loss: 0.74840 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 818 | Classification loss: 0.43508 | Regression loss: 0.42482 | Running loss: 0.74962 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 819 | Classification loss: 0.06823 | Regression loss: 0.24285 | Running loss: 0.74887 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 820 | Classification loss: 0.21320 | Regression loss: 0.27295 | Running loss: 0.74837 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 821 | Classification loss: 0.01636 | Regression loss: 0.28681 | Running loss: 0.74879 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 822 | Classification loss: 0.02654 | Regression loss: 0.26432 | Running loss: 0.74907 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 823 | Classification loss: 0.50746 | Regression loss: 0.41195 | Running loss: 0.74840 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 824 | Classification loss: 0.78007 | Regression loss: 0.35894 | Running loss: 0.74961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 825 | Classification loss: 0.03432 | Regression loss: 0.17918 | Running loss: 0.74865 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 826 | Classification loss: 0.53440 | Regression loss: 0.36434 | Running loss: 0.74865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 827 | Classification loss: 1.06374 | Regression loss: 0.19996 | Running loss: 0.75066 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 828 | Classification loss: 0.07193 | Regression loss: 0.30930 | Running loss: 0.75105 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 829 | Classification loss: 0.03653 | Regression loss: 0.10794 | Running loss: 0.75088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 830 | Classification loss: 0.10324 | Regression loss: 0.28607 | Running loss: 0.75074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 831 | Classification loss: 1.10034 | Regression loss: 0.51376 | Running loss: 0.75355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 832 | Classification loss: 0.25933 | Regression loss: 0.32682 | Running loss: 0.75337 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 833 | Classification loss: 0.09944 | Regression loss: 0.13404 | Running loss: 0.75128 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 834 | Classification loss: 0.64828 | Regression loss: 0.47406 | Running loss: 0.75326 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 835 | Classification loss: 0.58841 | Regression loss: 0.55900 | Running loss: 0.75491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 836 | Classification loss: 0.06810 | Regression loss: 0.12189 | Running loss: 0.75400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 837 | Classification loss: 0.01674 | Regression loss: 0.16122 | Running loss: 0.75129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 838 | Classification loss: 0.00078 | Regression loss: 0.03838 | Running loss: 0.74700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 839 | Classification loss: 0.04599 | Regression loss: 0.28075 | Running loss: 0.74734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 840 | Classification loss: 0.92695 | Regression loss: 0.39126 | Running loss: 0.74952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 841 | Classification loss: 0.04233 | Regression loss: 0.11625 | Running loss: 0.74950 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 842 | Classification loss: 0.03818 | Regression loss: 0.24059 | Running loss: 0.74952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 843 | Classification loss: 0.35345 | Regression loss: 0.37562 | Running loss: 0.75017 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 844 | Classification loss: 0.14570 | Regression loss: 0.19723 | Running loss: 0.74888 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 845 | Classification loss: 0.15525 | Regression loss: 0.22066 | Running loss: 0.74940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 846 | Classification loss: 1.53648 | Regression loss: 0.56362 | Running loss: 0.74995 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 847 | Classification loss: 0.01121 | Regression loss: 0.11751 | Running loss: 0.74594 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 848 | Classification loss: 1.25664 | Regression loss: 0.71446 | Running loss: 0.74862 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 849 | Classification loss: 0.19978 | Regression loss: 0.34147 | Running loss: 0.74898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 850 | Classification loss: 0.20494 | Regression loss: 0.11341 | Running loss: 0.74735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 851 | Classification loss: 0.69014 | Regression loss: 0.62105 | Running loss: 0.74829 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 852 | Classification loss: 0.15463 | Regression loss: 0.22953 | Running loss: 0.74769 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 853 | Classification loss: 0.09795 | Regression loss: 0.27210 | Running loss: 0.74620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 854 | Classification loss: 0.21967 | Regression loss: 0.36215 | Running loss: 0.74300 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 855 | Classification loss: 0.87292 | Regression loss: 0.60745 | Running loss: 0.74472 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 856 | Classification loss: 0.02805 | Regression loss: 0.06607 | Running loss: 0.74098 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 857 | Classification loss: 0.30213 | Regression loss: 0.35959 | Running loss: 0.74211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 858 | Classification loss: 0.27613 | Regression loss: 0.49617 | Running loss: 0.74312 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 859 | Classification loss: 0.10962 | Regression loss: 0.17755 | Running loss: 0.74359 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 860 | Classification loss: 1.12903 | Regression loss: 0.66160 | Running loss: 0.74603 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 861 | Classification loss: 0.25394 | Regression loss: 0.42565 | Running loss: 0.73738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 862 | Classification loss: 0.00620 | Regression loss: 0.10467 | Running loss: 0.73695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 863 | Classification loss: 0.36420 | Regression loss: 0.24654 | Running loss: 0.73790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 864 | Classification loss: 0.04047 | Regression loss: 0.09963 | Running loss: 0.73805 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 865 | Classification loss: 0.06993 | Regression loss: 0.25924 | Running loss: 0.73763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 866 | Classification loss: 1.16096 | Regression loss: 0.57313 | Running loss: 0.73635 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 867 | Classification loss: 0.45448 | Regression loss: 0.33030 | Running loss: 0.73593 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 868 | Classification loss: 1.61956 | Regression loss: 0.77529 | Running loss: 0.73732 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 869 | Classification loss: 0.30336 | Regression loss: 0.39157 | Running loss: 0.73561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 870 | Classification loss: 0.47258 | Regression loss: 0.46223 | Running loss: 0.73710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 871 | Classification loss: 1.96177 | Regression loss: 0.75742 | Running loss: 0.74031 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 872 | Classification loss: 1.58596 | Regression loss: 0.61070 | Running loss: 0.74448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 873 | Classification loss: 0.60362 | Regression loss: 0.55678 | Running loss: 0.74248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 874 | Classification loss: 0.04382 | Regression loss: 0.08228 | Running loss: 0.74142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 875 | Classification loss: 0.19063 | Regression loss: 0.20592 | Running loss: 0.74121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 876 | Classification loss: 0.00291 | Regression loss: 0.02980 | Running loss: 0.74009 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 877 | Classification loss: 0.33427 | Regression loss: 0.27438 | Running loss: 0.73758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 878 | Classification loss: 0.03909 | Regression loss: 0.07530 | Running loss: 0.73618 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 879 | Classification loss: 0.43039 | Regression loss: 0.32304 | Running loss: 0.73721 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 880 | Classification loss: 0.48956 | Regression loss: 0.49430 | Running loss: 0.73784 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 881 | Classification loss: 0.41170 | Regression loss: 0.65342 | Running loss: 0.73884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 882 | Classification loss: 0.63122 | Regression loss: 0.33355 | Running loss: 0.73961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 883 | Classification loss: 2.31456 | Regression loss: 0.63020 | Running loss: 0.74403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 884 | Classification loss: 0.19519 | Regression loss: 0.32522 | Running loss: 0.74171 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 885 | Classification loss: 0.29325 | Regression loss: 0.33775 | Running loss: 0.74048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 886 | Classification loss: 0.06449 | Regression loss: 0.07611 | Running loss: 0.73430 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 887 | Classification loss: 0.14856 | Regression loss: 0.27875 | Running loss: 0.73434 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 888 | Classification loss: 0.20077 | Regression loss: 0.37754 | Running loss: 0.73443 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 889 | Classification loss: 0.02867 | Regression loss: 0.11505 | Running loss: 0.73220 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 890 | Classification loss: 0.01085 | Regression loss: 0.13819 | Running loss: 0.73127 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 891 | Classification loss: 0.32368 | Regression loss: 0.42040 | Running loss: 0.72877 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 892 | Classification loss: 0.19412 | Regression loss: 0.36762 | Running loss: 0.72820 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 893 | Classification loss: 0.00885 | Regression loss: 0.21604 | Running loss: 0.72714 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 894 | Classification loss: 0.04679 | Regression loss: 0.33710 | Running loss: 0.72591 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 895 | Classification loss: 0.01868 | Regression loss: 0.09462 | Running loss: 0.72558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 896 | Classification loss: 0.75220 | Regression loss: 0.37728 | Running loss: 0.72472 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 897 | Classification loss: 0.10542 | Regression loss: 0.10618 | Running loss: 0.72302 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 898 | Classification loss: 0.39217 | Regression loss: 0.31140 | Running loss: 0.72436 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 899 | Classification loss: 0.87066 | Regression loss: 0.44393 | Running loss: 0.72179 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 900 | Classification loss: 0.11615 | Regression loss: 0.41164 | Running loss: 0.72222 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 901 | Classification loss: 0.54747 | Regression loss: 0.26131 | Running loss: 0.72350 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 902 | Classification loss: 0.67806 | Regression loss: 0.52490 | Running loss: 0.72189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 903 | Classification loss: 0.00167 | Regression loss: 0.00000 | Running loss: 0.72028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 904 | Classification loss: 1.35966 | Regression loss: 0.67931 | Running loss: 0.72329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 905 | Classification loss: 0.25157 | Regression loss: 0.26417 | Running loss: 0.72349 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 906 | Classification loss: 0.03573 | Regression loss: 0.05085 | Running loss: 0.72349 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 907 | Classification loss: 0.04786 | Regression loss: 0.27112 | Running loss: 0.72203 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 908 | Classification loss: 0.59037 | Regression loss: 0.32653 | Running loss: 0.72296 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 909 | Classification loss: 0.31907 | Regression loss: 0.31674 | Running loss: 0.72392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 910 | Classification loss: 0.00218 | Regression loss: 0.08235 | Running loss: 0.72245 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 911 | Classification loss: 0.84354 | Regression loss: 0.58791 | Running loss: 0.72416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 912 | Classification loss: 0.98646 | Regression loss: 0.55438 | Running loss: 0.72684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 913 | Classification loss: 0.11873 | Regression loss: 0.32005 | Running loss: 0.72736 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 914 | Classification loss: 0.04214 | Regression loss: 0.10415 | Running loss: 0.72496 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 915 | Classification loss: 0.30773 | Regression loss: 0.49729 | Running loss: 0.72588 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 916 | Classification loss: 0.84405 | Regression loss: 0.03914 | Running loss: 0.72670 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 917 | Classification loss: 0.01156 | Regression loss: 0.22207 | Running loss: 0.72584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 918 | Classification loss: 0.30486 | Regression loss: 0.31667 | Running loss: 0.72611 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 919 | Classification loss: 0.44433 | Regression loss: 0.58461 | Running loss: 0.72638 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 920 | Classification loss: 0.00085 | Regression loss: 0.06817 | Running loss: 0.72613 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 921 | Classification loss: 0.08996 | Regression loss: 0.22609 | Running loss: 0.72297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 922 | Classification loss: 0.00833 | Regression loss: 0.15920 | Running loss: 0.71691 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 923 | Classification loss: 0.52314 | Regression loss: 0.27731 | Running loss: 0.71800 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 924 | Classification loss: 0.16663 | Regression loss: 0.30160 | Running loss: 0.71524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 925 | Classification loss: 0.02496 | Regression loss: 0.16632 | Running loss: 0.71549 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 926 | Classification loss: 0.08470 | Regression loss: 0.26425 | Running loss: 0.71563 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 927 | Classification loss: 0.07484 | Regression loss: 0.16407 | Running loss: 0.71445 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 928 | Classification loss: 0.07524 | Regression loss: 0.22097 | Running loss: 0.71464 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 929 | Classification loss: 0.05441 | Regression loss: 0.25544 | Running loss: 0.71335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 930 | Classification loss: 0.05176 | Regression loss: 0.12041 | Running loss: 0.71179 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 931 | Classification loss: 0.18815 | Regression loss: 0.42300 | Running loss: 0.71142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 932 | Classification loss: 1.01764 | Regression loss: 0.49713 | Running loss: 0.71388 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 933 | Classification loss: 0.58803 | Regression loss: 0.55711 | Running loss: 0.71591 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 934 | Classification loss: 0.18736 | Regression loss: 0.11907 | Running loss: 0.71583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 935 | Classification loss: 0.18963 | Regression loss: 0.42255 | Running loss: 0.71625 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 936 | Classification loss: 0.00025 | Regression loss: 0.04844 | Running loss: 0.71457 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 937 | Classification loss: 0.56684 | Regression loss: 0.33684 | Running loss: 0.71595 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 938 | Classification loss: 0.40488 | Regression loss: 0.29132 | Running loss: 0.71555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 939 | Classification loss: 0.27225 | Regression loss: 0.31381 | Running loss: 0.71655 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 940 | Classification loss: 0.14795 | Regression loss: 0.33125 | Running loss: 0.71751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 941 | Classification loss: 0.64797 | Regression loss: 0.48950 | Running loss: 0.71930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 942 | Classification loss: 0.84178 | Regression loss: 0.61606 | Running loss: 0.71908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 943 | Classification loss: 0.98535 | Regression loss: 0.60579 | Running loss: 0.71939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 944 | Classification loss: 1.20865 | Regression loss: 0.65972 | Running loss: 0.72123 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 945 | Classification loss: 0.47276 | Regression loss: 0.16042 | Running loss: 0.71913 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 946 | Classification loss: 0.12807 | Regression loss: 0.41273 | Running loss: 0.71859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 947 | Classification loss: 0.13646 | Regression loss: 0.19075 | Running loss: 0.71885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 948 | Classification loss: 0.02219 | Regression loss: 0.16078 | Running loss: 0.71868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 949 | Classification loss: 0.28041 | Regression loss: 0.14715 | Running loss: 0.71790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 950 | Classification loss: 0.32362 | Regression loss: 0.24642 | Running loss: 0.71670 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 951 | Classification loss: 0.44921 | Regression loss: 0.62022 | Running loss: 0.71672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 952 | Classification loss: 0.00068 | Regression loss: 0.17627 | Running loss: 0.71542 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 953 | Classification loss: 1.92596 | Regression loss: 0.77787 | Running loss: 0.71978 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 954 | Classification loss: 0.54579 | Regression loss: 0.44048 | Running loss: 0.71999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 955 | Classification loss: 1.10128 | Regression loss: 0.49290 | Running loss: 0.72205 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 956 | Classification loss: 0.22936 | Regression loss: 0.21473 | Running loss: 0.72254 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 957 | Classification loss: 0.30047 | Regression loss: 0.38644 | Running loss: 0.72329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 958 | Classification loss: 0.14278 | Regression loss: 0.20678 | Running loss: 0.72121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 959 | Classification loss: 0.15871 | Regression loss: 0.38141 | Running loss: 0.72182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 960 | Classification loss: 0.32424 | Regression loss: 0.37717 | Running loss: 0.72303 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 961 | Classification loss: 0.03162 | Regression loss: 0.11157 | Running loss: 0.72220 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 962 | Classification loss: 0.00960 | Regression loss: 0.16887 | Running loss: 0.72216 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 963 | Classification loss: 0.02522 | Regression loss: 0.12569 | Running loss: 0.72158 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 964 | Classification loss: 0.02734 | Regression loss: 0.23602 | Running loss: 0.72154 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 965 | Classification loss: 0.01807 | Regression loss: 0.24229 | Running loss: 0.72047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 966 | Classification loss: 0.37995 | Regression loss: 0.32462 | Running loss: 0.72126 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 967 | Classification loss: 1.27233 | Regression loss: 0.57595 | Running loss: 0.71996 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 968 | Classification loss: 0.28918 | Regression loss: 0.42354 | Running loss: 0.71835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 969 | Classification loss: 0.33553 | Regression loss: 0.28929 | Running loss: 0.71842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 970 | Classification loss: 0.12821 | Regression loss: 0.30595 | Running loss: 0.71799 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 971 | Classification loss: 0.05333 | Regression loss: 0.13381 | Running loss: 0.71665 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 972 | Classification loss: 0.68998 | Regression loss: 0.40603 | Running loss: 0.71703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 973 | Classification loss: 0.01733 | Regression loss: 0.13682 | Running loss: 0.70997 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 974 | Classification loss: 0.78155 | Regression loss: 0.23282 | Running loss: 0.71142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 975 | Classification loss: 0.09117 | Regression loss: 0.16187 | Running loss: 0.71005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 976 | Classification loss: 0.24079 | Regression loss: 0.55682 | Running loss: 0.70697 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 977 | Classification loss: 0.30240 | Regression loss: 0.44426 | Running loss: 0.70628 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 978 | Classification loss: 0.09978 | Regression loss: 0.18137 | Running loss: 0.70287 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 979 | Classification loss: 0.16086 | Regression loss: 0.29853 | Running loss: 0.70129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 980 | Classification loss: 0.70087 | Regression loss: 0.42352 | Running loss: 0.70184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 981 | Classification loss: 0.26860 | Regression loss: 0.33184 | Running loss: 0.70002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 982 | Classification loss: 0.28679 | Regression loss: 0.40134 | Running loss: 0.69922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 983 | Classification loss: 0.65269 | Regression loss: 0.45939 | Running loss: 0.69851 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 984 | Classification loss: 0.29038 | Regression loss: 0.43750 | Running loss: 0.69499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 985 | Classification loss: 0.87266 | Regression loss: 0.58211 | Running loss: 0.69742 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 986 | Classification loss: 0.43649 | Regression loss: 0.36213 | Running loss: 0.69781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 987 | Classification loss: 0.02325 | Regression loss: 0.29264 | Running loss: 0.69813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 988 | Classification loss: 0.28675 | Regression loss: 0.41417 | Running loss: 0.69508 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 989 | Classification loss: 1.55447 | Regression loss: 0.57026 | Running loss: 0.69751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 990 | Classification loss: 0.48198 | Regression loss: 0.29776 | Running loss: 0.69272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 991 | Classification loss: 1.54883 | Regression loss: 0.65716 | Running loss: 0.69616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 992 | Classification loss: 0.25997 | Regression loss: 0.41717 | Running loss: 0.69640 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 993 | Classification loss: 0.42989 | Regression loss: 0.53304 | Running loss: 0.69529 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 994 | Classification loss: 0.45732 | Regression loss: 0.25031 | Running loss: 0.69649 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 995 | Classification loss: 0.14719 | Regression loss: 0.07789 | Running loss: 0.69655 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 996 | Classification loss: 0.01908 | Regression loss: 0.14801 | Running loss: 0.69618 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 997 | Classification loss: 0.27685 | Regression loss: 0.31104 | Running loss: 0.69382 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 998 | Classification loss: 0.35642 | Regression loss: 0.39702 | Running loss: 0.69309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 999 | Classification loss: 0.01041 | Regression loss: 0.14843 | Running loss: 0.69285 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1000 | Classification loss: 0.01232 | Regression loss: 0.11964 | Running loss: 0.69276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1001 | Classification loss: 0.08363 | Regression loss: 0.13165 | Running loss: 0.69166 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1002 | Classification loss: 0.12573 | Regression loss: 0.15745 | Running loss: 0.69190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1003 | Classification loss: 0.03676 | Regression loss: 0.15163 | Running loss: 0.68994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1004 | Classification loss: 0.55349 | Regression loss: 0.63029 | Running loss: 0.69007 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1005 | Classification loss: 0.92109 | Regression loss: 0.65132 | Running loss: 0.69164 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1006 | Classification loss: 0.08930 | Regression loss: 0.14518 | Running loss: 0.69038 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1007 | Classification loss: 0.04278 | Regression loss: 0.34879 | Running loss: 0.69064 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1008 | Classification loss: 1.27894 | Regression loss: 0.56999 | Running loss: 0.69243 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1009 | Classification loss: 0.36132 | Regression loss: 0.37122 | Running loss: 0.69255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1010 | Classification loss: 0.01496 | Regression loss: 0.15139 | Running loss: 0.69231 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1011 | Classification loss: 1.38787 | Regression loss: 0.57157 | Running loss: 0.69486 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1012 | Classification loss: 0.27172 | Regression loss: 0.40974 | Running loss: 0.69576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1013 | Classification loss: 0.04374 | Regression loss: 0.29607 | Running loss: 0.69616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1014 | Classification loss: 0.00326 | Regression loss: 0.08922 | Running loss: 0.69557 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1015 | Classification loss: 0.30893 | Regression loss: 0.13113 | Running loss: 0.69372 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1016 | Classification loss: 0.29385 | Regression loss: 0.16544 | Running loss: 0.69211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1017 | Classification loss: 0.11344 | Regression loss: 0.30802 | Running loss: 0.69244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1018 | Classification loss: 0.09925 | Regression loss: 0.37007 | Running loss: 0.69026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1019 | Classification loss: 0.32337 | Regression loss: 0.59725 | Running loss: 0.69108 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1020 | Classification loss: 0.05949 | Regression loss: 0.36254 | Running loss: 0.69153 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1021 | Classification loss: 0.63473 | Regression loss: 0.57081 | Running loss: 0.69380 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1022 | Classification loss: 1.12215 | Regression loss: 0.67818 | Running loss: 0.69562 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1023 | Classification loss: 0.07609 | Regression loss: 0.24697 | Running loss: 0.69593 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1024 | Classification loss: 0.57188 | Regression loss: 0.36356 | Running loss: 0.69758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1025 | Classification loss: 2.00947 | Regression loss: 0.75553 | Running loss: 0.70087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1026 | Classification loss: 0.16311 | Regression loss: 0.33877 | Running loss: 0.70137 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1027 | Classification loss: 0.68272 | Regression loss: 0.70001 | Running loss: 0.70360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1028 | Classification loss: 0.07724 | Regression loss: 0.31482 | Running loss: 0.70208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1029 | Classification loss: 0.04475 | Regression loss: 0.11784 | Running loss: 0.70224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1030 | Classification loss: 0.85198 | Regression loss: 0.26334 | Running loss: 0.70392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1031 | Classification loss: 0.84942 | Regression loss: 0.39041 | Running loss: 0.70480 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1032 | Classification loss: 0.44810 | Regression loss: 0.33357 | Running loss: 0.70460 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1033 | Classification loss: 0.10109 | Regression loss: 0.13273 | Running loss: 0.70404 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1034 | Classification loss: 0.79674 | Regression loss: 0.49058 | Running loss: 0.70629 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1035 | Classification loss: 0.23028 | Regression loss: 0.26851 | Running loss: 0.70511 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1036 | Classification loss: 0.07072 | Regression loss: 0.12242 | Running loss: 0.70471 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1037 | Classification loss: 1.19107 | Regression loss: 0.60386 | Running loss: 0.70723 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1038 | Classification loss: 1.11510 | Regression loss: 0.69134 | Running loss: 0.70922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1039 | Classification loss: 0.41824 | Regression loss: 0.50730 | Running loss: 0.70849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1040 | Classification loss: 0.13587 | Regression loss: 0.21622 | Running loss: 0.70621 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1041 | Classification loss: 1.01221 | Regression loss: 0.58001 | Running loss: 0.70891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1042 | Classification loss: 0.04250 | Regression loss: 0.26741 | Running loss: 0.70865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1043 | Classification loss: 0.00389 | Regression loss: 0.15221 | Running loss: 0.70772 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1044 | Classification loss: 0.03621 | Regression loss: 0.25824 | Running loss: 0.70793 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1045 | Classification loss: 0.12739 | Regression loss: 0.23624 | Running loss: 0.70710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1046 | Classification loss: 3.30044 | Regression loss: 0.89505 | Running loss: 0.71415 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1047 | Classification loss: 0.35371 | Regression loss: 0.47090 | Running loss: 0.71528 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1048 | Classification loss: 0.68286 | Regression loss: 0.47597 | Running loss: 0.71727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1049 | Classification loss: 0.21559 | Regression loss: 0.21782 | Running loss: 0.71579 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1050 | Classification loss: 0.01339 | Regression loss: 0.10564 | Running loss: 0.71510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1051 | Classification loss: 0.08079 | Regression loss: 0.20601 | Running loss: 0.71274 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1052 | Classification loss: 0.42241 | Regression loss: 0.60459 | Running loss: 0.71423 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1053 | Classification loss: 1.93478 | Regression loss: 0.70068 | Running loss: 0.71871 | Spend Time:0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1054 | Classification loss: 0.00214 | Regression loss: 0.12904 | Running loss: 0.71669 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1055 | Classification loss: 0.36347 | Regression loss: 0.39534 | Running loss: 0.71778 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1056 | Classification loss: 0.20008 | Regression loss: 0.24626 | Running loss: 0.71554 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1057 | Classification loss: 0.28825 | Regression loss: 0.44304 | Running loss: 0.71549 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1058 | Classification loss: 0.42349 | Regression loss: 0.28298 | Running loss: 0.71562 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1059 | Classification loss: 0.05351 | Regression loss: 0.12012 | Running loss: 0.71289 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1060 | Classification loss: 0.12440 | Regression loss: 0.32878 | Running loss: 0.71226 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1061 | Classification loss: 0.62604 | Regression loss: 0.36408 | Running loss: 0.71208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1062 | Classification loss: 0.33398 | Regression loss: 0.49143 | Running loss: 0.71223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1063 | Classification loss: 0.22274 | Regression loss: 0.26043 | Running loss: 0.71218 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1064 | Classification loss: 0.12900 | Regression loss: 0.17171 | Running loss: 0.71234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1065 | Classification loss: 0.00041 | Regression loss: 0.01872 | Running loss: 0.71172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1066 | Classification loss: 0.18169 | Regression loss: 0.13507 | Running loss: 0.71163 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1067 | Classification loss: 0.52653 | Regression loss: 0.63766 | Running loss: 0.71368 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1068 | Classification loss: 1.48008 | Regression loss: 0.49585 | Running loss: 0.71652 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1069 | Classification loss: 0.01521 | Regression loss: 0.13871 | Running loss: 0.71668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1070 | Classification loss: 0.72207 | Regression loss: 0.45371 | Running loss: 0.71865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1071 | Classification loss: 0.06298 | Regression loss: 0.15783 | Running loss: 0.71740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1072 | Classification loss: 0.45337 | Regression loss: 0.35284 | Running loss: 0.71727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1073 | Classification loss: 0.05735 | Regression loss: 0.11446 | Running loss: 0.71529 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1074 | Classification loss: 0.04317 | Regression loss: 0.18102 | Running loss: 0.71509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1075 | Classification loss: 0.88955 | Regression loss: 0.62315 | Running loss: 0.71554 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1076 | Classification loss: 0.00223 | Regression loss: 0.07070 | Running loss: 0.71357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1077 | Classification loss: 0.02507 | Regression loss: 0.18052 | Running loss: 0.71290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1078 | Classification loss: 0.10799 | Regression loss: 0.26105 | Running loss: 0.71339 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1079 | Classification loss: 0.00067 | Regression loss: 0.06020 | Running loss: 0.71329 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1080 | Classification loss: 0.01398 | Regression loss: 0.26200 | Running loss: 0.71215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1081 | Classification loss: 1.49374 | Regression loss: 0.66767 | Running loss: 0.71624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1082 | Classification loss: 0.32346 | Regression loss: 0.31124 | Running loss: 0.71649 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1083 | Classification loss: 1.70992 | Regression loss: 0.64691 | Running loss: 0.72074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1084 | Classification loss: 1.02640 | Regression loss: 0.22622 | Running loss: 0.72135 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1085 | Classification loss: 0.27250 | Regression loss: 0.38190 | Running loss: 0.72186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1086 | Classification loss: 0.88057 | Regression loss: 0.57180 | Running loss: 0.71896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1087 | Classification loss: 0.34013 | Regression loss: 0.23201 | Running loss: 0.71914 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1088 | Classification loss: 0.00595 | Regression loss: 0.08311 | Running loss: 0.71510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1089 | Classification loss: 0.03543 | Regression loss: 0.10808 | Running loss: 0.71506 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1090 | Classification loss: 0.04219 | Regression loss: 0.13170 | Running loss: 0.71372 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1091 | Classification loss: 0.11639 | Regression loss: 0.33679 | Running loss: 0.71025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1092 | Classification loss: 0.15034 | Regression loss: 0.20145 | Running loss: 0.70924 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1093 | Classification loss: 0.35167 | Regression loss: 0.41506 | Running loss: 0.71045 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1094 | Classification loss: 0.28691 | Regression loss: 0.49613 | Running loss: 0.70954 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1095 | Classification loss: 2.51315 | Regression loss: 0.73437 | Running loss: 0.71501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1096 | Classification loss: 0.07845 | Regression loss: 0.17434 | Running loss: 0.71542 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1097 | Classification loss: 0.03518 | Regression loss: 0.18931 | Running loss: 0.71553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1098 | Classification loss: 0.03818 | Regression loss: 0.21052 | Running loss: 0.71539 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1099 | Classification loss: 0.45035 | Regression loss: 0.44501 | Running loss: 0.71531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1100 | Classification loss: 0.08398 | Regression loss: 0.09163 | Running loss: 0.71458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1101 | Classification loss: 0.98677 | Regression loss: 0.18835 | Running loss: 0.71612 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1102 | Classification loss: 0.07795 | Regression loss: 0.18642 | Running loss: 0.71551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1103 | Classification loss: 0.22728 | Regression loss: 0.22746 | Running loss: 0.71536 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1104 | Classification loss: 0.14757 | Regression loss: 0.25704 | Running loss: 0.71500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1105 | Classification loss: 0.06409 | Regression loss: 0.32502 | Running loss: 0.71311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1106 | Classification loss: 0.04808 | Regression loss: 0.17853 | Running loss: 0.71321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1107 | Classification loss: 2.89016 | Regression loss: 0.75336 | Running loss: 0.71859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1108 | Classification loss: 0.01976 | Regression loss: 0.20593 | Running loss: 0.71874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1109 | Classification loss: 0.15872 | Regression loss: 0.51610 | Running loss: 0.71857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1110 | Classification loss: 0.11215 | Regression loss: 0.11896 | Running loss: 0.71540 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1111 | Classification loss: 0.26449 | Regression loss: 0.24734 | Running loss: 0.71598 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1112 | Classification loss: 0.30303 | Regression loss: 0.26503 | Running loss: 0.71500 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1113 | Classification loss: 0.62082 | Regression loss: 0.24990 | Running loss: 0.71457 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1114 | Classification loss: 0.25081 | Regression loss: 0.26755 | Running loss: 0.71525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1115 | Classification loss: 0.14435 | Regression loss: 0.26506 | Running loss: 0.71367 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1116 | Classification loss: 0.01850 | Regression loss: 0.22442 | Running loss: 0.71237 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1117 | Classification loss: 0.02168 | Regression loss: 0.14796 | Running loss: 0.70934 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1118 | Classification loss: 0.11936 | Regression loss: 0.28257 | Running loss: 0.70862 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1119 | Classification loss: 0.00265 | Regression loss: 0.13264 | Running loss: 0.70825 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1120 | Classification loss: 0.38407 | Regression loss: 0.48463 | Running loss: 0.70854 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1121 | Classification loss: 0.00465 | Regression loss: 0.05224 | Running loss: 0.70821 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1122 | Classification loss: 0.00286 | Regression loss: 0.08012 | Running loss: 0.70810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1123 | Classification loss: 0.10899 | Regression loss: 0.14032 | Running loss: 0.70645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1124 | Classification loss: 0.95854 | Regression loss: 0.58440 | Running loss: 0.70672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1125 | Classification loss: 0.36675 | Regression loss: 0.29212 | Running loss: 0.70764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1126 | Classification loss: 0.76615 | Regression loss: 0.32925 | Running loss: 0.70857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1127 | Classification loss: 1.06310 | Regression loss: 0.49534 | Running loss: 0.71109 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1128 | Classification loss: 0.53294 | Regression loss: 0.53460 | Running loss: 0.71288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1129 | Classification loss: 0.02685 | Regression loss: 0.21759 | Running loss: 0.71201 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1130 | Classification loss: 0.09130 | Regression loss: 0.13878 | Running loss: 0.70897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1131 | Classification loss: 0.15649 | Regression loss: 0.50782 | Running loss: 0.71011 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1132 | Classification loss: 0.04578 | Regression loss: 0.17010 | Running loss: 0.70955 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1133 | Classification loss: 0.56401 | Regression loss: 0.40246 | Running loss: 0.71050 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1134 | Classification loss: 0.10224 | Regression loss: 0.27934 | Running loss: 0.71083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1135 | Classification loss: 0.00161 | Regression loss: 0.05713 | Running loss: 0.70815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1136 | Classification loss: 0.12625 | Regression loss: 0.24867 | Running loss: 0.70645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1137 | Classification loss: 0.01237 | Regression loss: 0.21725 | Running loss: 0.70653 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1138 | Classification loss: 0.13462 | Regression loss: 0.32351 | Running loss: 0.70698 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1139 | Classification loss: 3.92600 | Regression loss: 0.75536 | Running loss: 0.71548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1140 | Classification loss: 0.00072 | Regression loss: 0.06989 | Running loss: 0.71343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1141 | Classification loss: 0.21575 | Regression loss: 0.36981 | Running loss: 0.71180 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1142 | Classification loss: 0.26574 | Regression loss: 0.04143 | Running loss: 0.71023 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1143 | Classification loss: 0.23137 | Regression loss: 0.49319 | Running loss: 0.71103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1144 | Classification loss: 0.62475 | Regression loss: 0.54254 | Running loss: 0.71297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1145 | Classification loss: 0.01209 | Regression loss: 0.08738 | Running loss: 0.71085 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1146 | Classification loss: 0.02172 | Regression loss: 0.25717 | Running loss: 0.70898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1147 | Classification loss: 0.98499 | Regression loss: 0.36255 | Running loss: 0.71146 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1148 | Classification loss: 0.66913 | Regression loss: 0.23496 | Running loss: 0.71260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1149 | Classification loss: 0.02067 | Regression loss: 0.19667 | Running loss: 0.71204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1150 | Classification loss: 0.06146 | Regression loss: 0.13440 | Running loss: 0.70774 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1151 | Classification loss: 0.37434 | Regression loss: 0.31224 | Running loss: 0.70763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1152 | Classification loss: 0.03656 | Regression loss: 0.17173 | Running loss: 0.70675 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1153 | Classification loss: 1.74108 | Regression loss: 0.36420 | Running loss: 0.70800 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1154 | Classification loss: 0.78600 | Regression loss: 0.59985 | Running loss: 0.71025 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1155 | Classification loss: 0.25544 | Regression loss: 0.31340 | Running loss: 0.71016 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1156 | Classification loss: 0.38207 | Regression loss: 0.61932 | Running loss: 0.71006 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1157 | Classification loss: 0.52435 | Regression loss: 0.54892 | Running loss: 0.70996 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1158 | Classification loss: 0.46359 | Regression loss: 0.39189 | Running loss: 0.71117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1159 | Classification loss: 0.70456 | Regression loss: 0.28028 | Running loss: 0.71181 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1160 | Classification loss: 0.30610 | Regression loss: 0.51424 | Running loss: 0.71247 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1161 | Classification loss: 0.00097 | Regression loss: 0.07370 | Running loss: 0.71233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1162 | Classification loss: 0.00016 | Regression loss: 0.02288 | Running loss: 0.71146 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1163 | Classification loss: 0.08410 | Regression loss: 0.14741 | Running loss: 0.71119 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1164 | Classification loss: 0.68147 | Regression loss: 0.52315 | Running loss: 0.71278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1165 | Classification loss: 0.46437 | Regression loss: 0.37142 | Running loss: 0.71369 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1166 | Classification loss: 0.04650 | Regression loss: 0.25367 | Running loss: 0.71198 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1167 | Classification loss: 0.06182 | Regression loss: 0.15302 | Running loss: 0.71233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1168 | Classification loss: 0.15765 | Regression loss: 0.26305 | Running loss: 0.71243 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1169 | Classification loss: 0.00168 | Regression loss: 0.06757 | Running loss: 0.70883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1170 | Classification loss: 0.82055 | Regression loss: 0.56389 | Running loss: 0.71099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1171 | Classification loss: 0.06358 | Regression loss: 0.21344 | Running loss: 0.70817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1172 | Classification loss: 1.34071 | Regression loss: 0.54856 | Running loss: 0.70960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1173 | Classification loss: 0.51451 | Regression loss: 0.41328 | Running loss: 0.71118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1174 | Classification loss: 0.13673 | Regression loss: 0.19862 | Running loss: 0.71105 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1175 | Classification loss: 0.04303 | Regression loss: 0.17575 | Running loss: 0.71100 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1176 | Classification loss: 0.10288 | Regression loss: 0.20240 | Running loss: 0.71125 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1177 | Classification loss: 0.78035 | Regression loss: 0.67578 | Running loss: 0.71136 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1178 | Classification loss: 0.28259 | Regression loss: 0.37210 | Running loss: 0.71179 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1179 | Classification loss: 0.01421 | Regression loss: 0.14800 | Running loss: 0.71107 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1180 | Classification loss: 0.19562 | Regression loss: 0.35710 | Running loss: 0.71056 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1181 | Classification loss: 0.00587 | Regression loss: 0.09291 | Running loss: 0.70962 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1182 | Classification loss: 0.47896 | Regression loss: 0.37939 | Running loss: 0.70909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1183 | Classification loss: 0.00265 | Regression loss: 0.13030 | Running loss: 0.70928 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1184 | Classification loss: 0.13016 | Regression loss: 0.39508 | Running loss: 0.70822 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1185 | Classification loss: 0.09269 | Regression loss: 0.21051 | Running loss: 0.70606 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1186 | Classification loss: 0.27104 | Regression loss: 0.49420 | Running loss: 0.70516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1187 | Classification loss: 0.92518 | Regression loss: 0.63018 | Running loss: 0.70739 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1188 | Classification loss: 0.00730 | Regression loss: 0.17759 | Running loss: 0.70631 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1189 | Classification loss: 0.07823 | Regression loss: 0.31488 | Running loss: 0.70684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1190 | Classification loss: 0.06306 | Regression loss: 0.23873 | Running loss: 0.70411 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1191 | Classification loss: 0.04747 | Regression loss: 0.20268 | Running loss: 0.70378 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1192 | Classification loss: 0.19262 | Regression loss: 0.21328 | Running loss: 0.70244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1193 | Classification loss: 0.34854 | Regression loss: 0.58700 | Running loss: 0.70307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1194 | Classification loss: 0.31950 | Regression loss: 0.39985 | Running loss: 0.69774 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1195 | Classification loss: 0.30540 | Regression loss: 0.37665 | Running loss: 0.69881 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1196 | Classification loss: 0.16381 | Regression loss: 0.26547 | Running loss: 0.69848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1197 | Classification loss: 0.75836 | Regression loss: 0.39894 | Running loss: 0.69986 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1198 | Classification loss: 0.66474 | Regression loss: 0.30317 | Running loss: 0.70093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1199 | Classification loss: 0.67411 | Regression loss: 0.55050 | Running loss: 0.70314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1200 | Classification loss: 0.36468 | Regression loss: 0.42072 | Running loss: 0.70289 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1201 | Classification loss: 0.11881 | Regression loss: 0.21171 | Running loss: 0.70337 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1202 | Classification loss: 0.03949 | Regression loss: 0.22148 | Running loss: 0.70265 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1203 | Classification loss: 0.25575 | Regression loss: 0.26939 | Running loss: 0.70328 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1204 | Classification loss: 0.80696 | Regression loss: 0.42262 | Running loss: 0.70509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1205 | Classification loss: 0.32576 | Regression loss: 0.38369 | Running loss: 0.70614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1206 | Classification loss: 0.35277 | Regression loss: 0.68390 | Running loss: 0.70676 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1207 | Classification loss: 0.05954 | Regression loss: 0.32485 | Running loss: 0.70611 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1208 | Classification loss: 0.01572 | Regression loss: 0.22020 | Running loss: 0.70604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1209 | Classification loss: 0.06628 | Regression loss: 0.30079 | Running loss: 0.70591 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1210 | Classification loss: 0.39890 | Regression loss: 0.40673 | Running loss: 0.70513 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1211 | Classification loss: 0.12361 | Regression loss: 0.12913 | Running loss: 0.70516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1212 | Classification loss: 0.10744 | Regression loss: 0.26211 | Running loss: 0.70530 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1213 | Classification loss: 0.86914 | Regression loss: 0.38846 | Running loss: 0.70455 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1214 | Classification loss: 0.00544 | Regression loss: 0.08473 | Running loss: 0.70403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1215 | Classification loss: 1.03774 | Regression loss: 0.59775 | Running loss: 0.70595 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1216 | Classification loss: 0.38773 | Regression loss: 0.16953 | Running loss: 0.70634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1217 | Classification loss: 0.16073 | Regression loss: 0.25821 | Running loss: 0.70607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1218 | Classification loss: 0.00525 | Regression loss: 0.13541 | Running loss: 0.70602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1219 | Classification loss: 0.09848 | Regression loss: 0.25626 | Running loss: 0.70662 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1220 | Classification loss: 0.21962 | Regression loss: 0.17261 | Running loss: 0.70618 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1221 | Classification loss: 1.68449 | Regression loss: 0.50019 | Running loss: 0.70803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1222 | Classification loss: 0.57450 | Regression loss: 0.27371 | Running loss: 0.70838 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1223 | Classification loss: 0.35031 | Regression loss: 0.39191 | Running loss: 0.70697 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1224 | Classification loss: 0.18940 | Regression loss: 0.13404 | Running loss: 0.70514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1225 | Classification loss: 0.00233 | Regression loss: 0.13080 | Running loss: 0.70426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1226 | Classification loss: 0.22985 | Regression loss: 0.48303 | Running loss: 0.70480 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1227 | Classification loss: 1.81180 | Regression loss: 0.49307 | Running loss: 0.70909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1228 | Classification loss: 0.41768 | Regression loss: 0.56581 | Running loss: 0.70627 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1229 | Classification loss: 0.14201 | Regression loss: 0.11609 | Running loss: 0.70632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1230 | Classification loss: 0.79244 | Regression loss: 0.47612 | Running loss: 0.70601 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1231 | Classification loss: 0.37759 | Regression loss: 0.33341 | Running loss: 0.70567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1232 | Classification loss: 0.79894 | Regression loss: 0.59078 | Running loss: 0.70667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1233 | Classification loss: 0.11247 | Regression loss: 0.10298 | Running loss: 0.70554 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1234 | Classification loss: 0.02312 | Regression loss: 0.20812 | Running loss: 0.70510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1235 | Classification loss: 0.10258 | Regression loss: 0.21235 | Running loss: 0.70492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1236 | Classification loss: 0.24206 | Regression loss: 0.39163 | Running loss: 0.70431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1237 | Classification loss: 0.15240 | Regression loss: 0.17159 | Running loss: 0.70425 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1238 | Classification loss: 0.18406 | Regression loss: 0.29052 | Running loss: 0.70504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1239 | Classification loss: 0.04156 | Regression loss: 0.13403 | Running loss: 0.70424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1240 | Classification loss: 0.02239 | Regression loss: 0.17610 | Running loss: 0.70325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1241 | Classification loss: 0.10646 | Regression loss: 0.23265 | Running loss: 0.70355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1242 | Classification loss: 0.01361 | Regression loss: 0.13645 | Running loss: 0.70318 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1243 | Classification loss: 0.00495 | Regression loss: 0.17263 | Running loss: 0.70206 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1244 | Classification loss: 1.86405 | Regression loss: 0.76724 | Running loss: 0.70535 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1245 | Classification loss: 0.17995 | Regression loss: 0.21108 | Running loss: 0.70413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1246 | Classification loss: 0.13805 | Regression loss: 0.26381 | Running loss: 0.70307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1247 | Classification loss: 0.26545 | Regression loss: 0.34755 | Running loss: 0.70374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1248 | Classification loss: 0.01420 | Regression loss: 0.05312 | Running loss: 0.70209 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1249 | Classification loss: 0.44960 | Regression loss: 0.40970 | Running loss: 0.70304 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1250 | Classification loss: 0.03604 | Regression loss: 0.12697 | Running loss: 0.70262 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1251 | Classification loss: 0.63852 | Regression loss: 0.52802 | Running loss: 0.70450 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1252 | Classification loss: 0.18261 | Regression loss: 0.41444 | Running loss: 0.70515 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1253 | Classification loss: 0.10683 | Regression loss: 0.17663 | Running loss: 0.70461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1254 | Classification loss: 0.00473 | Regression loss: 0.06002 | Running loss: 0.70317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1255 | Classification loss: 0.30317 | Regression loss: 0.14784 | Running loss: 0.70316 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1256 | Classification loss: 0.09260 | Regression loss: 0.19463 | Running loss: 0.70266 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1257 | Classification loss: 0.90398 | Regression loss: 0.41351 | Running loss: 0.70305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1258 | Classification loss: 0.08520 | Regression loss: 0.19449 | Running loss: 0.70352 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1259 | Classification loss: 1.59911 | Regression loss: 0.56459 | Running loss: 0.70782 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1260 | Classification loss: 0.45418 | Regression loss: 0.31044 | Running loss: 0.70738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1261 | Classification loss: 0.06006 | Regression loss: 0.33982 | Running loss: 0.70476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1262 | Classification loss: 0.09427 | Regression loss: 0.42592 | Running loss: 0.70292 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1263 | Classification loss: 0.00993 | Regression loss: 0.21902 | Running loss: 0.70255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1264 | Classification loss: 1.11523 | Regression loss: 0.55593 | Running loss: 0.70487 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1265 | Classification loss: 0.54387 | Regression loss: 0.29956 | Running loss: 0.70257 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1266 | Classification loss: 0.17529 | Regression loss: 0.21563 | Running loss: 0.69992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1267 | Classification loss: 0.68303 | Regression loss: 0.13064 | Running loss: 0.70118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1268 | Classification loss: 3.72128 | Regression loss: 1.06952 | Running loss: 0.70752 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1269 | Classification loss: 0.62880 | Regression loss: 0.43639 | Running loss: 0.70900 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1270 | Classification loss: 0.21223 | Regression loss: 0.36889 | Running loss: 0.70990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1271 | Classification loss: 0.63288 | Regression loss: 0.30449 | Running loss: 0.71068 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1272 | Classification loss: 0.86221 | Regression loss: 0.29344 | Running loss: 0.71024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1273 | Classification loss: 0.06038 | Regression loss: 0.08165 | Running loss: 0.70994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1274 | Classification loss: 0.06207 | Regression loss: 0.22886 | Running loss: 0.70956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1275 | Classification loss: 0.84909 | Regression loss: 0.37791 | Running loss: 0.71118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1276 | Classification loss: 0.02248 | Regression loss: 0.27200 | Running loss: 0.71165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1277 | Classification loss: 0.03127 | Regression loss: 0.14922 | Running loss: 0.71001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1278 | Classification loss: 0.02966 | Regression loss: 0.11719 | Running loss: 0.70834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1279 | Classification loss: 0.14857 | Regression loss: 0.25784 | Running loss: 0.70863 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1280 | Classification loss: 0.85009 | Regression loss: 0.55321 | Running loss: 0.71123 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1281 | Classification loss: 0.50983 | Regression loss: 0.54120 | Running loss: 0.71329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1282 | Classification loss: 0.77572 | Regression loss: 0.54236 | Running loss: 0.71482 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1283 | Classification loss: 0.15063 | Regression loss: 0.22965 | Running loss: 0.71441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1284 | Classification loss: 0.19759 | Regression loss: 0.46641 | Running loss: 0.71371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1285 | Classification loss: 0.96027 | Regression loss: 0.41993 | Running loss: 0.71571 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1286 | Classification loss: 2.48686 | Regression loss: 0.57103 | Running loss: 0.72090 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1287 | Classification loss: 0.38877 | Regression loss: 0.46539 | Running loss: 0.71993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1288 | Classification loss: 0.32964 | Regression loss: 0.11953 | Running loss: 0.71615 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1289 | Classification loss: 1.40056 | Regression loss: 0.69440 | Running loss: 0.71930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1290 | Classification loss: 0.44225 | Regression loss: 0.26891 | Running loss: 0.72047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1291 | Classification loss: 0.06424 | Regression loss: 0.27696 | Running loss: 0.71911 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1292 | Classification loss: 0.77615 | Regression loss: 0.42573 | Running loss: 0.71952 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1293 | Classification loss: 0.89377 | Regression loss: 0.17397 | Running loss: 0.71900 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1294 | Classification loss: 0.61692 | Regression loss: 0.44509 | Running loss: 0.72026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1295 | Classification loss: 0.18188 | Regression loss: 0.20158 | Running loss: 0.71990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1296 | Classification loss: 0.10219 | Regression loss: 0.63572 | Running loss: 0.71989 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1297 | Classification loss: 0.02244 | Regression loss: 0.12766 | Running loss: 0.71974 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1298 | Classification loss: 0.11502 | Regression loss: 0.28206 | Running loss: 0.71998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1299 | Classification loss: 1.39433 | Regression loss: 0.38716 | Running loss: 0.72307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1300 | Classification loss: 0.04955 | Regression loss: 0.13672 | Running loss: 0.72313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1301 | Classification loss: 1.22420 | Regression loss: 0.66367 | Running loss: 0.72621 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1302 | Classification loss: 0.07994 | Regression loss: 0.20919 | Running loss: 0.72643 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1303 | Classification loss: 0.11780 | Regression loss: 0.18349 | Running loss: 0.72492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1304 | Classification loss: 0.51622 | Regression loss: 0.67434 | Running loss: 0.72691 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1305 | Classification loss: 0.00528 | Regression loss: 0.09104 | Running loss: 0.72616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1306 | Classification loss: 0.96431 | Regression loss: 0.76356 | Running loss: 0.72939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1307 | Classification loss: 3.08070 | Regression loss: 0.29890 | Running loss: 0.73475 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1308 | Classification loss: 0.06475 | Regression loss: 0.16704 | Running loss: 0.73305 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1309 | Classification loss: 0.03172 | Regression loss: 0.43037 | Running loss: 0.73296 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1310 | Classification loss: 0.81319 | Regression loss: 0.36842 | Running loss: 0.73486 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1311 | Classification loss: 0.26889 | Regression loss: 0.26727 | Running loss: 0.73397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1312 | Classification loss: 0.09000 | Regression loss: 0.25863 | Running loss: 0.73329 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1313 | Classification loss: 0.59561 | Regression loss: 0.45815 | Running loss: 0.73360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1314 | Classification loss: 0.12885 | Regression loss: 0.20249 | Running loss: 0.73297 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1315 | Classification loss: 0.79852 | Regression loss: 0.18513 | Running loss: 0.73291 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1316 | Classification loss: 0.37574 | Regression loss: 0.20877 | Running loss: 0.73110 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1317 | Classification loss: 0.32489 | Regression loss: 0.46646 | Running loss: 0.73036 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1318 | Classification loss: 0.50043 | Regression loss: 0.36216 | Running loss: 0.73036 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1319 | Classification loss: 0.05930 | Regression loss: 0.15475 | Running loss: 0.73017 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1320 | Classification loss: 0.55883 | Regression loss: 0.60159 | Running loss: 0.73152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1321 | Classification loss: 0.20701 | Regression loss: 0.32030 | Running loss: 0.73197 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1322 | Classification loss: 0.10538 | Regression loss: 0.43389 | Running loss: 0.73246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1323 | Classification loss: 0.03542 | Regression loss: 0.21148 | Running loss: 0.73112 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1324 | Classification loss: 0.47524 | Regression loss: 0.58194 | Running loss: 0.73095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1325 | Classification loss: 0.11004 | Regression loss: 0.28951 | Running loss: 0.73133 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1326 | Classification loss: 0.24799 | Regression loss: 0.20421 | Running loss: 0.73043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1327 | Classification loss: 0.77721 | Regression loss: 0.46716 | Running loss: 0.73039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1328 | Classification loss: 0.01391 | Regression loss: 0.15351 | Running loss: 0.72997 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1329 | Classification loss: 1.11578 | Regression loss: 0.52463 | Running loss: 0.73296 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1330 | Classification loss: 2.06695 | Regression loss: 0.27117 | Running loss: 0.73686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1331 | Classification loss: 0.08251 | Regression loss: 0.14461 | Running loss: 0.73408 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1332 | Classification loss: 0.30643 | Regression loss: 0.33740 | Running loss: 0.73420 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1333 | Classification loss: 0.38652 | Regression loss: 0.47354 | Running loss: 0.73545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1334 | Classification loss: 0.03545 | Regression loss: 0.16751 | Running loss: 0.73361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1335 | Classification loss: 0.01007 | Regression loss: 0.12436 | Running loss: 0.73159 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1336 | Classification loss: 0.04672 | Regression loss: 0.13754 | Running loss: 0.73157 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1337 | Classification loss: 0.07860 | Regression loss: 0.31145 | Running loss: 0.73200 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1338 | Classification loss: 0.05602 | Regression loss: 0.25155 | Running loss: 0.73254 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1339 | Classification loss: 1.14077 | Regression loss: 0.58578 | Running loss: 0.73534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1340 | Classification loss: 0.21139 | Regression loss: 0.44532 | Running loss: 0.73401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1341 | Classification loss: 0.57043 | Regression loss: 0.52322 | Running loss: 0.73588 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1342 | Classification loss: 0.00701 | Regression loss: 0.13475 | Running loss: 0.73561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1343 | Classification loss: 0.82410 | Regression loss: 0.52643 | Running loss: 0.73685 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1344 | Classification loss: 0.76628 | Regression loss: 0.39175 | Running loss: 0.73848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1345 | Classification loss: 0.01002 | Regression loss: 0.02902 | Running loss: 0.73781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1346 | Classification loss: 0.01436 | Regression loss: 0.16594 | Running loss: 0.73397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1347 | Classification loss: 0.24312 | Regression loss: 0.47261 | Running loss: 0.73514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1348 | Classification loss: 0.00181 | Regression loss: 0.15485 | Running loss: 0.73151 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1349 | Classification loss: 0.04429 | Regression loss: 0.11861 | Running loss: 0.73076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1350 | Classification loss: 0.18443 | Regression loss: 0.18983 | Running loss: 0.73087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1351 | Classification loss: 0.60613 | Regression loss: 0.64735 | Running loss: 0.73075 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1352 | Classification loss: 0.00857 | Regression loss: 0.12038 | Running loss: 0.73024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1353 | Classification loss: 0.26907 | Regression loss: 0.20684 | Running loss: 0.73045 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1354 | Classification loss: 0.00075 | Regression loss: 0.09372 | Running loss: 0.72948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1355 | Classification loss: 0.21353 | Regression loss: 0.31459 | Running loss: 0.72758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1356 | Classification loss: 0.07596 | Regression loss: 0.17877 | Running loss: 0.72790 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1357 | Classification loss: 0.02509 | Regression loss: 0.08969 | Running loss: 0.72680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1358 | Classification loss: 0.09537 | Regression loss: 0.29953 | Running loss: 0.72605 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1359 | Classification loss: 0.95493 | Regression loss: 0.55909 | Running loss: 0.72850 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1360 | Classification loss: 0.17404 | Regression loss: 0.44316 | Running loss: 0.72615 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1361 | Classification loss: 0.12375 | Regression loss: 0.45766 | Running loss: 0.72596 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1362 | Classification loss: 0.20699 | Regression loss: 0.22601 | Running loss: 0.72660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1363 | Classification loss: 0.83004 | Regression loss: 0.83136 | Running loss: 0.72870 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1364 | Classification loss: 0.07088 | Regression loss: 0.11774 | Running loss: 0.72880 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1365 | Classification loss: 0.06497 | Regression loss: 0.29838 | Running loss: 0.72887 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1366 | Classification loss: 0.01697 | Regression loss: 0.14549 | Running loss: 0.72573 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1367 | Classification loss: 0.06768 | Regression loss: 0.11806 | Running loss: 0.72453 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1368 | Classification loss: 0.93841 | Regression loss: 0.59229 | Running loss: 0.72280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1369 | Classification loss: 0.22639 | Regression loss: 0.44358 | Running loss: 0.72275 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1370 | Classification loss: 0.17903 | Regression loss: 0.31511 | Running loss: 0.72187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1371 | Classification loss: 0.10811 | Regression loss: 0.29199 | Running loss: 0.71723 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1372 | Classification loss: 0.75908 | Regression loss: 0.50521 | Running loss: 0.71537 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1373 | Classification loss: 0.16360 | Regression loss: 0.18022 | Running loss: 0.71373 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1374 | Classification loss: 0.44056 | Regression loss: 0.17781 | Running loss: 0.71472 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1375 | Classification loss: 0.03219 | Regression loss: 0.14104 | Running loss: 0.71427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1376 | Classification loss: 0.06066 | Regression loss: 0.18944 | Running loss: 0.71471 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1377 | Classification loss: 0.24212 | Regression loss: 0.30449 | Running loss: 0.71458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1378 | Classification loss: 0.79061 | Regression loss: 0.50287 | Running loss: 0.71694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1379 | Classification loss: 0.56883 | Regression loss: 0.64036 | Running loss: 0.71785 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1380 | Classification loss: 0.71492 | Regression loss: 0.71499 | Running loss: 0.71874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1381 | Classification loss: 0.42762 | Regression loss: 0.23630 | Running loss: 0.71794 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1382 | Classification loss: 0.42726 | Regression loss: 0.40958 | Running loss: 0.71768 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1383 | Classification loss: 0.05108 | Regression loss: 0.35967 | Running loss: 0.71262 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1384 | Classification loss: 1.56037 | Regression loss: 0.57675 | Running loss: 0.71585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1385 | Classification loss: 0.40508 | Regression loss: 0.30085 | Running loss: 0.71600 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1386 | Classification loss: 0.00187 | Regression loss: 0.16011 | Running loss: 0.71604 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1387 | Classification loss: 1.48959 | Regression loss: 0.64218 | Running loss: 0.71945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1388 | Classification loss: 0.36025 | Regression loss: 0.13182 | Running loss: 0.71928 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1389 | Classification loss: 0.13010 | Regression loss: 0.40004 | Running loss: 0.72005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1390 | Classification loss: 0.10723 | Regression loss: 0.28962 | Running loss: 0.72055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1391 | Classification loss: 0.14892 | Regression loss: 0.32078 | Running loss: 0.72000 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1392 | Classification loss: 0.32574 | Regression loss: 0.59905 | Running loss: 0.72072 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1393 | Classification loss: 0.08631 | Regression loss: 0.15630 | Running loss: 0.72076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1394 | Classification loss: 1.21240 | Regression loss: 0.52115 | Running loss: 0.72346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1395 | Classification loss: 0.61705 | Regression loss: 0.53039 | Running loss: 0.72553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1396 | Classification loss: 1.20036 | Regression loss: 0.58099 | Running loss: 0.72683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1397 | Classification loss: 0.38446 | Regression loss: 0.56842 | Running loss: 0.72831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1398 | Classification loss: 0.01356 | Regression loss: 0.09943 | Running loss: 0.72713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1399 | Classification loss: 0.00327 | Regression loss: 0.22232 | Running loss: 0.72495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1400 | Classification loss: 0.06163 | Regression loss: 0.06799 | Running loss: 0.72416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1401 | Classification loss: 0.09195 | Regression loss: 0.18956 | Running loss: 0.72310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1402 | Classification loss: 0.23395 | Regression loss: 0.46888 | Running loss: 0.72210 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1403 | Classification loss: 0.89952 | Regression loss: 0.60939 | Running loss: 0.72512 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1404 | Classification loss: 0.29973 | Regression loss: 0.51740 | Running loss: 0.72267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1405 | Classification loss: 0.18813 | Regression loss: 0.34279 | Running loss: 0.72270 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1406 | Classification loss: 0.51370 | Regression loss: 0.34033 | Running loss: 0.72424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1407 | Classification loss: 0.07053 | Regression loss: 0.21146 | Running loss: 0.72417 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1408 | Classification loss: 0.76835 | Regression loss: 0.37718 | Running loss: 0.72462 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1409 | Classification loss: 0.03167 | Regression loss: 0.11722 | Running loss: 0.72365 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1410 | Classification loss: 0.61660 | Regression loss: 0.13174 | Running loss: 0.72498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1411 | Classification loss: 0.15344 | Regression loss: 0.38556 | Running loss: 0.72319 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1412 | Classification loss: 1.13916 | Regression loss: 0.41865 | Running loss: 0.72323 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1413 | Classification loss: 1.42902 | Regression loss: 0.51470 | Running loss: 0.72624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1414 | Classification loss: 0.00291 | Regression loss: 0.13010 | Running loss: 0.72621 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1415 | Classification loss: 0.02220 | Regression loss: 0.23783 | Running loss: 0.72512 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1416 | Classification loss: 0.20424 | Regression loss: 0.39379 | Running loss: 0.72455 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1417 | Classification loss: 0.04342 | Regression loss: 0.16370 | Running loss: 0.72450 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1418 | Classification loss: 0.34782 | Regression loss: 0.54546 | Running loss: 0.72504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1419 | Classification loss: 0.79317 | Regression loss: 0.35024 | Running loss: 0.72527 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1420 | Classification loss: 0.14483 | Regression loss: 0.10921 | Running loss: 0.72564 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1421 | Classification loss: 0.56471 | Regression loss: 0.10389 | Running loss: 0.72634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1422 | Classification loss: 0.20929 | Regression loss: 0.16593 | Running loss: 0.72676 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1423 | Classification loss: 0.00169 | Regression loss: 0.11220 | Running loss: 0.72539 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1424 | Classification loss: 0.07222 | Regression loss: 0.26751 | Running loss: 0.72513 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1425 | Classification loss: 0.84538 | Regression loss: 0.32893 | Running loss: 0.72709 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1426 | Classification loss: 0.38791 | Regression loss: 0.58072 | Running loss: 0.72833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1427 | Classification loss: 0.08671 | Regression loss: 0.19722 | Running loss: 0.72842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1428 | Classification loss: 0.00318 | Regression loss: 0.18039 | Running loss: 0.72820 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1429 | Classification loss: 0.14936 | Regression loss: 0.30780 | Running loss: 0.72849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1430 | Classification loss: 0.00407 | Regression loss: 0.04941 | Running loss: 0.72826 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1431 | Classification loss: 0.76510 | Regression loss: 0.56802 | Running loss: 0.72970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1432 | Classification loss: 0.56963 | Regression loss: 0.31325 | Running loss: 0.72844 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1433 | Classification loss: 0.03758 | Regression loss: 0.22529 | Running loss: 0.72667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1434 | Classification loss: 0.23719 | Regression loss: 0.23415 | Running loss: 0.72700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1435 | Classification loss: 0.17639 | Regression loss: 0.28140 | Running loss: 0.72669 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1436 | Classification loss: 0.28924 | Regression loss: 0.49954 | Running loss: 0.72817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1437 | Classification loss: 0.16587 | Regression loss: 0.22087 | Running loss: 0.72714 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1438 | Classification loss: 0.04431 | Regression loss: 0.08157 | Running loss: 0.72600 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1439 | Classification loss: 0.58946 | Regression loss: 0.46541 | Running loss: 0.72694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1440 | Classification loss: 0.07425 | Regression loss: 0.16124 | Running loss: 0.72645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1441 | Classification loss: 0.15374 | Regression loss: 0.30085 | Running loss: 0.72508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1442 | Classification loss: 1.04133 | Regression loss: 0.37489 | Running loss: 0.72500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1443 | Classification loss: 0.02538 | Regression loss: 0.30709 | Running loss: 0.72248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1444 | Classification loss: 0.43695 | Regression loss: 0.50958 | Running loss: 0.72064 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1445 | Classification loss: 0.22633 | Regression loss: 0.42385 | Running loss: 0.72067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1446 | Classification loss: 0.03911 | Regression loss: 0.09630 | Running loss: 0.71986 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1447 | Classification loss: 0.55792 | Regression loss: 0.36927 | Running loss: 0.72106 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1448 | Classification loss: 0.66384 | Regression loss: 0.65529 | Running loss: 0.72333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1449 | Classification loss: 0.86933 | Regression loss: 0.53927 | Running loss: 0.72530 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1450 | Classification loss: 1.18311 | Regression loss: 0.37346 | Running loss: 0.72727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1451 | Classification loss: 1.36613 | Regression loss: 0.75863 | Running loss: 0.72938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1452 | Classification loss: 0.27596 | Regression loss: 0.48373 | Running loss: 0.73055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1453 | Classification loss: 0.00301 | Regression loss: 0.14802 | Running loss: 0.72544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1454 | Classification loss: 0.09106 | Regression loss: 0.16859 | Running loss: 0.72399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1455 | Classification loss: 0.77968 | Regression loss: 0.51039 | Running loss: 0.72338 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1456 | Classification loss: 0.92724 | Regression loss: 0.48621 | Running loss: 0.72532 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1457 | Classification loss: 0.30028 | Regression loss: 0.39988 | Running loss: 0.72534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1458 | Classification loss: 0.03814 | Regression loss: 0.12405 | Running loss: 0.72497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1459 | Classification loss: 0.59750 | Regression loss: 0.32177 | Running loss: 0.72573 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1460 | Classification loss: 0.10474 | Regression loss: 0.16956 | Running loss: 0.72487 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1461 | Classification loss: 0.00832 | Regression loss: 0.14048 | Running loss: 0.72488 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1462 | Classification loss: 0.07937 | Regression loss: 0.18761 | Running loss: 0.72506 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1463 | Classification loss: 0.21682 | Regression loss: 0.37522 | Running loss: 0.72594 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1464 | Classification loss: 1.78687 | Regression loss: 0.63448 | Running loss: 0.73026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1465 | Classification loss: 0.02097 | Regression loss: 0.14072 | Running loss: 0.73006 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1466 | Classification loss: 0.31594 | Regression loss: 0.32570 | Running loss: 0.72994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1467 | Classification loss: 0.03657 | Regression loss: 0.18130 | Running loss: 0.72668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1468 | Classification loss: 0.64170 | Regression loss: 0.43120 | Running loss: 0.72740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1469 | Classification loss: 0.01596 | Regression loss: 0.23002 | Running loss: 0.72664 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1470 | Classification loss: 0.23287 | Regression loss: 0.27839 | Running loss: 0.72679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1471 | Classification loss: 0.20219 | Regression loss: 0.27518 | Running loss: 0.72737 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1472 | Classification loss: 0.38260 | Regression loss: 0.51392 | Running loss: 0.72697 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1473 | Classification loss: 0.43355 | Regression loss: 0.42767 | Running loss: 0.72839 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1474 | Classification loss: 0.14947 | Regression loss: 0.19876 | Running loss: 0.72706 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1475 | Classification loss: 0.06509 | Regression loss: 0.12039 | Running loss: 0.72692 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1476 | Classification loss: 0.11192 | Regression loss: 0.18260 | Running loss: 0.72591 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1477 | Classification loss: 0.34148 | Regression loss: 0.16145 | Running loss: 0.72543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1478 | Classification loss: 0.38914 | Regression loss: 0.40324 | Running loss: 0.72645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1479 | Classification loss: 0.56910 | Regression loss: 0.51636 | Running loss: 0.72770 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1480 | Classification loss: 0.74300 | Regression loss: 0.38607 | Running loss: 0.72771 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1481 | Classification loss: 1.05434 | Regression loss: 0.50527 | Running loss: 0.72963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1482 | Classification loss: 0.13438 | Regression loss: 0.34194 | Running loss: 0.72921 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1483 | Classification loss: 0.48780 | Regression loss: 0.60544 | Running loss: 0.72917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1484 | Classification loss: 0.59211 | Regression loss: 0.49414 | Running loss: 0.72988 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1485 | Classification loss: 0.04080 | Regression loss: 0.17544 | Running loss: 0.72741 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1486 | Classification loss: 0.08573 | Regression loss: 0.31968 | Running loss: 0.72662 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1487 | Classification loss: 0.82612 | Regression loss: 0.61447 | Running loss: 0.72887 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1488 | Classification loss: 0.45860 | Regression loss: 0.61897 | Running loss: 0.72962 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1489 | Classification loss: 0.18571 | Regression loss: 0.30708 | Running loss: 0.72636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1490 | Classification loss: 1.31753 | Regression loss: 0.15875 | Running loss: 0.72775 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1491 | Classification loss: 0.06913 | Regression loss: 0.27100 | Running loss: 0.72402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1492 | Classification loss: 0.74062 | Regression loss: 0.66861 | Running loss: 0.72549 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1493 | Classification loss: 0.14404 | Regression loss: 0.46985 | Running loss: 0.72479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1494 | Classification loss: 0.10151 | Regression loss: 0.27530 | Running loss: 0.72413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1495 | Classification loss: 0.51892 | Regression loss: 0.43291 | Running loss: 0.72558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1496 | Classification loss: 1.34248 | Regression loss: 0.33359 | Running loss: 0.72860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1497 | Classification loss: 0.13512 | Regression loss: 0.14071 | Running loss: 0.72797 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1498 | Classification loss: 0.12267 | Regression loss: 0.16680 | Running loss: 0.72705 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1499 | Classification loss: 0.01749 | Regression loss: 0.12911 | Running loss: 0.72702 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1500 | Classification loss: 0.40146 | Regression loss: 0.44999 | Running loss: 0.72846 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1501 | Classification loss: 1.17765 | Regression loss: 0.39969 | Running loss: 0.73118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1502 | Classification loss: 1.46083 | Regression loss: 0.58052 | Running loss: 0.73470 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1503 | Classification loss: 0.00569 | Regression loss: 0.08795 | Running loss: 0.73451 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1504 | Classification loss: 0.00652 | Regression loss: 0.05755 | Running loss: 0.73227 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1505 | Classification loss: 0.00729 | Regression loss: 0.08082 | Running loss: 0.72930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1506 | Classification loss: 0.06906 | Regression loss: 0.32631 | Running loss: 0.72962 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1507 | Classification loss: 0.02581 | Regression loss: 0.27921 | Running loss: 0.72945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1508 | Classification loss: 0.59543 | Regression loss: 0.38164 | Running loss: 0.72771 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1509 | Classification loss: 0.13854 | Regression loss: 0.21621 | Running loss: 0.72695 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1510 | Classification loss: 0.67665 | Regression loss: 0.27979 | Running loss: 0.72853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1511 | Classification loss: 0.13608 | Regression loss: 0.46101 | Running loss: 0.72581 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1512 | Classification loss: 0.16760 | Regression loss: 0.14580 | Running loss: 0.72507 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1513 | Classification loss: 0.13962 | Regression loss: 0.39492 | Running loss: 0.72546 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1514 | Classification loss: 1.24974 | Regression loss: 0.52099 | Running loss: 0.72882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1515 | Classification loss: 0.03166 | Regression loss: 0.15842 | Running loss: 0.72832 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1516 | Classification loss: 0.57238 | Regression loss: 0.43033 | Running loss: 0.72940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1517 | Classification loss: 0.25376 | Regression loss: 0.52502 | Running loss: 0.73012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1518 | Classification loss: 0.22198 | Regression loss: 0.29974 | Running loss: 0.73022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1519 | Classification loss: 0.64392 | Regression loss: 0.26893 | Running loss: 0.73021 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1520 | Classification loss: 0.09706 | Regression loss: 0.26119 | Running loss: 0.73008 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1521 | Classification loss: 0.01857 | Regression loss: 0.41304 | Running loss: 0.72853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1522 | Classification loss: 0.16662 | Regression loss: 0.39025 | Running loss: 0.72605 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1523 | Classification loss: 0.18303 | Regression loss: 0.35078 | Running loss: 0.72647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1524 | Classification loss: 1.00313 | Regression loss: 0.47611 | Running loss: 0.72756 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1525 | Classification loss: 0.06310 | Regression loss: 0.20969 | Running loss: 0.72257 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1526 | Classification loss: 1.74724 | Regression loss: 0.79531 | Running loss: 0.72665 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1527 | Classification loss: 0.04179 | Regression loss: 0.15339 | Running loss: 0.72428 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1528 | Classification loss: 0.19668 | Regression loss: 0.32050 | Running loss: 0.72453 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1529 | Classification loss: 0.07453 | Regression loss: 0.25661 | Running loss: 0.72486 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1530 | Classification loss: 0.27191 | Regression loss: 0.41183 | Running loss: 0.72400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1531 | Classification loss: 0.10108 | Regression loss: 0.19403 | Running loss: 0.72211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1532 | Classification loss: 0.53635 | Regression loss: 0.31309 | Running loss: 0.72225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1533 | Classification loss: 1.01309 | Regression loss: 0.42874 | Running loss: 0.72466 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1534 | Classification loss: 1.03765 | Regression loss: 0.40322 | Running loss: 0.72497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1535 | Classification loss: 1.06464 | Regression loss: 0.72244 | Running loss: 0.72755 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1536 | Classification loss: 0.00109 | Regression loss: 0.10488 | Running loss: 0.72737 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1537 | Classification loss: 0.56787 | Regression loss: 0.37579 | Running loss: 0.72567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1538 | Classification loss: 0.64061 | Regression loss: 0.33985 | Running loss: 0.72402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1539 | Classification loss: 0.68944 | Regression loss: 0.52737 | Running loss: 0.72460 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1540 | Classification loss: 0.07992 | Regression loss: 0.37100 | Running loss: 0.72480 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1541 | Classification loss: 0.02039 | Regression loss: 0.09349 | Running loss: 0.72184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1542 | Classification loss: 0.03394 | Regression loss: 0.18238 | Running loss: 0.72165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1543 | Classification loss: 0.12767 | Regression loss: 0.43325 | Running loss: 0.72246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1544 | Classification loss: 0.11417 | Regression loss: 0.19038 | Running loss: 0.72248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1545 | Classification loss: 0.00015 | Regression loss: 0.05485 | Running loss: 0.72187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1546 | Classification loss: 0.03151 | Regression loss: 0.18797 | Running loss: 0.71391 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1547 | Classification loss: 0.22768 | Regression loss: 0.46018 | Running loss: 0.71364 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1548 | Classification loss: 0.09114 | Regression loss: 0.33759 | Running loss: 0.71218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1549 | Classification loss: 0.01062 | Regression loss: 0.11871 | Running loss: 0.71157 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1550 | Classification loss: 0.13776 | Regression loss: 0.30637 | Running loss: 0.71222 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1551 | Classification loss: 2.19617 | Regression loss: 0.80506 | Running loss: 0.71765 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1552 | Classification loss: 0.08609 | Regression loss: 0.32645 | Running loss: 0.71642 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1553 | Classification loss: 0.19925 | Regression loss: 0.34443 | Running loss: 0.71224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1554 | Classification loss: 0.03046 | Regression loss: 0.20849 | Running loss: 0.71246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1555 | Classification loss: 0.37313 | Regression loss: 0.53632 | Running loss: 0.71276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1556 | Classification loss: 0.69918 | Regression loss: 0.49530 | Running loss: 0.71425 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1557 | Classification loss: 0.06052 | Regression loss: 0.06782 | Running loss: 0.71305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1558 | Classification loss: 0.22580 | Regression loss: 0.09991 | Running loss: 0.71229 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1559 | Classification loss: 1.08848 | Regression loss: 0.51585 | Running loss: 0.71515 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1560 | Classification loss: 0.01078 | Regression loss: 0.23033 | Running loss: 0.71472 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1561 | Classification loss: 0.85660 | Regression loss: 0.53481 | Running loss: 0.71553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1562 | Classification loss: 0.91901 | Regression loss: 0.48616 | Running loss: 0.71668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1563 | Classification loss: 0.01127 | Regression loss: 0.07879 | Running loss: 0.71590 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1564 | Classification loss: 1.45649 | Regression loss: 0.69491 | Running loss: 0.71960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1565 | Classification loss: 0.45825 | Regression loss: 0.11329 | Running loss: 0.72070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1566 | Classification loss: 0.03651 | Regression loss: 0.13165 | Running loss: 0.72041 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1567 | Classification loss: 0.53952 | Regression loss: 0.34699 | Running loss: 0.71985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1568 | Classification loss: 0.14250 | Regression loss: 0.30539 | Running loss: 0.71680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1569 | Classification loss: 0.02784 | Regression loss: 0.16599 | Running loss: 0.71688 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1570 | Classification loss: 0.03289 | Regression loss: 0.07761 | Running loss: 0.71475 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1571 | Classification loss: 0.71859 | Regression loss: 0.73807 | Running loss: 0.71722 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1572 | Classification loss: 0.35833 | Regression loss: 0.24592 | Running loss: 0.71681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1573 | Classification loss: 0.06040 | Regression loss: 0.15326 | Running loss: 0.71690 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1574 | Classification loss: 0.53181 | Regression loss: 0.17473 | Running loss: 0.71786 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1575 | Classification loss: 0.69390 | Regression loss: 0.75273 | Running loss: 0.71773 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1576 | Classification loss: 0.77761 | Regression loss: 0.50522 | Running loss: 0.72015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1577 | Classification loss: 0.61024 | Regression loss: 0.54110 | Running loss: 0.72204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1578 | Classification loss: 0.00403 | Regression loss: 0.08004 | Running loss: 0.72147 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1579 | Classification loss: 0.60567 | Regression loss: 0.51178 | Running loss: 0.72358 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1580 | Classification loss: 0.10802 | Regression loss: 0.26865 | Running loss: 0.72379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1581 | Classification loss: 0.34930 | Regression loss: 0.41544 | Running loss: 0.72099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1582 | Classification loss: 0.38035 | Regression loss: 0.45297 | Running loss: 0.72139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1583 | Classification loss: 0.10108 | Regression loss: 0.15167 | Running loss: 0.71718 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1584 | Classification loss: 0.01250 | Regression loss: 0.04693 | Running loss: 0.71479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1585 | Classification loss: 0.20522 | Regression loss: 0.35491 | Running loss: 0.71461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1586 | Classification loss: 1.02456 | Regression loss: 0.58440 | Running loss: 0.71492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1587 | Classification loss: 0.98527 | Regression loss: 0.52574 | Running loss: 0.71680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1588 | Classification loss: 0.01988 | Regression loss: 0.23311 | Running loss: 0.71712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1589 | Classification loss: 0.87533 | Regression loss: 0.58538 | Running loss: 0.71976 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1590 | Classification loss: 0.30875 | Regression loss: 0.24587 | Running loss: 0.72052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1591 | Classification loss: 0.35974 | Regression loss: 0.69451 | Running loss: 0.72172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1592 | Classification loss: 0.62723 | Regression loss: 0.36913 | Running loss: 0.72301 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1593 | Classification loss: 0.18029 | Regression loss: 0.62482 | Running loss: 0.72309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1594 | Classification loss: 0.08761 | Regression loss: 0.15157 | Running loss: 0.72200 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1595 | Classification loss: 0.12941 | Regression loss: 0.39314 | Running loss: 0.71655 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1596 | Classification loss: 1.20754 | Regression loss: 0.58366 | Running loss: 0.71963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1597 | Classification loss: 0.01053 | Regression loss: 0.10496 | Running loss: 0.71941 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1598 | Classification loss: 0.01960 | Regression loss: 0.14365 | Running loss: 0.71924 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1599 | Classification loss: 0.00066 | Regression loss: 0.06145 | Running loss: 0.71757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1600 | Classification loss: 0.52782 | Regression loss: 0.75234 | Running loss: 0.71978 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1601 | Classification loss: 0.19706 | Regression loss: 0.14290 | Running loss: 0.71811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1602 | Classification loss: 0.09556 | Regression loss: 0.25127 | Running loss: 0.71828 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1603 | Classification loss: 0.03787 | Regression loss: 0.27047 | Running loss: 0.71798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1604 | Classification loss: 0.36072 | Regression loss: 0.51587 | Running loss: 0.71893 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1605 | Classification loss: 1.80466 | Regression loss: 0.59487 | Running loss: 0.72295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1606 | Classification loss: 0.02108 | Regression loss: 0.08595 | Running loss: 0.72271 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1607 | Classification loss: 0.94803 | Regression loss: 0.63389 | Running loss: 0.71859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1608 | Classification loss: 0.63567 | Regression loss: 0.33512 | Running loss: 0.72008 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1609 | Classification loss: 0.15488 | Regression loss: 0.28513 | Running loss: 0.71961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1610 | Classification loss: 0.31534 | Regression loss: 0.28948 | Running loss: 0.72035 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1611 | Classification loss: 0.21779 | Regression loss: 0.25703 | Running loss: 0.72028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1612 | Classification loss: 1.25547 | Regression loss: 0.56089 | Running loss: 0.72278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1613 | Classification loss: 0.17122 | Regression loss: 0.25234 | Running loss: 0.72188 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1614 | Classification loss: 0.66704 | Regression loss: 0.45880 | Running loss: 0.72310 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1615 | Classification loss: 0.10057 | Regression loss: 0.26376 | Running loss: 0.72301 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1616 | Classification loss: 0.96661 | Regression loss: 0.49816 | Running loss: 0.72545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1617 | Classification loss: 0.01694 | Regression loss: 0.20545 | Running loss: 0.72556 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1618 | Classification loss: 0.10881 | Regression loss: 0.25983 | Running loss: 0.72549 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1619 | Classification loss: 1.45745 | Regression loss: 0.58705 | Running loss: 0.72931 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1620 | Classification loss: 0.05854 | Regression loss: 0.19824 | Running loss: 0.72808 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1621 | Classification loss: 0.03541 | Regression loss: 0.28031 | Running loss: 0.72860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1622 | Classification loss: 0.01241 | Regression loss: 0.24871 | Running loss: 0.72896 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1623 | Classification loss: 0.43843 | Regression loss: 0.53629 | Running loss: 0.73041 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1624 | Classification loss: 0.14703 | Regression loss: 0.12949 | Running loss: 0.72788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1625 | Classification loss: 1.09096 | Regression loss: 0.80183 | Running loss: 0.73034 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1626 | Classification loss: 0.37420 | Regression loss: 0.45982 | Running loss: 0.72982 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1627 | Classification loss: 0.42227 | Regression loss: 0.29192 | Running loss: 0.72813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1628 | Classification loss: 0.00977 | Regression loss: 0.13252 | Running loss: 0.72628 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1629 | Classification loss: 1.47663 | Regression loss: 0.61006 | Running loss: 0.72997 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1630 | Classification loss: 0.76932 | Regression loss: 0.49238 | Running loss: 0.73203 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1631 | Classification loss: 1.16450 | Regression loss: 0.60625 | Running loss: 0.73424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1632 | Classification loss: 0.65792 | Regression loss: 0.48835 | Running loss: 0.73610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1633 | Classification loss: 0.12215 | Regression loss: 0.37022 | Running loss: 0.73516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1634 | Classification loss: 0.29548 | Regression loss: 0.39014 | Running loss: 0.73576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1635 | Classification loss: 0.70597 | Regression loss: 0.49773 | Running loss: 0.73805 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1636 | Classification loss: 1.27444 | Regression loss: 0.48589 | Running loss: 0.74082 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1637 | Classification loss: 0.99634 | Regression loss: 0.55615 | Running loss: 0.74347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1638 | Classification loss: 0.01486 | Regression loss: 0.06283 | Running loss: 0.74271 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1639 | Classification loss: 0.45783 | Regression loss: 0.28225 | Running loss: 0.73483 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1640 | Classification loss: 0.25259 | Regression loss: 0.43697 | Running loss: 0.73606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1641 | Classification loss: 0.05371 | Regression loss: 0.04218 | Running loss: 0.73509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1642 | Classification loss: 1.12663 | Regression loss: 0.51131 | Running loss: 0.73775 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1643 | Classification loss: 0.63267 | Regression loss: 0.42605 | Running loss: 0.73841 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1644 | Classification loss: 1.14542 | Regression loss: 0.19782 | Running loss: 0.73877 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1645 | Classification loss: 0.04207 | Regression loss: 0.08916 | Running loss: 0.73883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1646 | Classification loss: 2.06816 | Regression loss: 0.46101 | Running loss: 0.74333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1647 | Classification loss: 0.00396 | Regression loss: 0.16445 | Running loss: 0.74097 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1648 | Classification loss: 0.38343 | Regression loss: 0.48250 | Running loss: 0.74090 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1649 | Classification loss: 0.09857 | Regression loss: 0.09911 | Running loss: 0.74086 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1650 | Classification loss: 0.12606 | Regression loss: 0.27792 | Running loss: 0.74127 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1651 | Classification loss: 1.55135 | Regression loss: 0.59783 | Running loss: 0.74420 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1652 | Classification loss: 0.44076 | Regression loss: 0.40990 | Running loss: 0.74548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1653 | Classification loss: 0.56452 | Regression loss: 0.38302 | Running loss: 0.74317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1654 | Classification loss: 1.69317 | Regression loss: 0.67586 | Running loss: 0.74513 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1655 | Classification loss: 1.20289 | Regression loss: 0.46975 | Running loss: 0.74734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1656 | Classification loss: 0.92836 | Regression loss: 0.62903 | Running loss: 0.74845 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1657 | Classification loss: 0.97678 | Regression loss: 0.72898 | Running loss: 0.74972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1658 | Classification loss: 0.20764 | Regression loss: 0.25893 | Running loss: 0.74894 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1659 | Classification loss: 0.25461 | Regression loss: 0.21811 | Running loss: 0.74792 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1660 | Classification loss: 0.67251 | Regression loss: 0.50005 | Running loss: 0.74862 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1661 | Classification loss: 0.51783 | Regression loss: 0.41413 | Running loss: 0.75034 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1662 | Classification loss: 0.86980 | Regression loss: 0.15059 | Running loss: 0.75233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1663 | Classification loss: 0.08592 | Regression loss: 0.19995 | Running loss: 0.75244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1664 | Classification loss: 1.16935 | Regression loss: 0.67185 | Running loss: 0.75371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1665 | Classification loss: 0.89782 | Regression loss: 0.47991 | Running loss: 0.75480 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1666 | Classification loss: 0.01947 | Regression loss: 0.09905 | Running loss: 0.75443 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1667 | Classification loss: 0.05288 | Regression loss: 0.06137 | Running loss: 0.75423 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1668 | Classification loss: 1.25779 | Regression loss: 0.57510 | Running loss: 0.75706 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1669 | Classification loss: 0.36695 | Regression loss: 0.19535 | Running loss: 0.75804 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1670 | Classification loss: 0.01317 | Regression loss: 0.13486 | Running loss: 0.75557 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1671 | Classification loss: 0.11232 | Regression loss: 0.18466 | Running loss: 0.75561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1672 | Classification loss: 1.23214 | Regression loss: 0.52435 | Running loss: 0.75534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1673 | Classification loss: 0.19864 | Regression loss: 0.19463 | Running loss: 0.75427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1674 | Classification loss: 0.17842 | Regression loss: 0.32431 | Running loss: 0.75461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1675 | Classification loss: 0.28176 | Regression loss: 0.34759 | Running loss: 0.75543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1676 | Classification loss: 0.27548 | Regression loss: 0.47392 | Running loss: 0.75632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1677 | Classification loss: 0.03362 | Regression loss: 0.19791 | Running loss: 0.75387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1678 | Classification loss: 0.00007 | Regression loss: 0.11351 | Running loss: 0.75279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1679 | Classification loss: 0.01038 | Regression loss: 0.09245 | Running loss: 0.75267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1680 | Classification loss: 0.34793 | Regression loss: 0.35286 | Running loss: 0.75296 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1681 | Classification loss: 1.31904 | Regression loss: 0.46054 | Running loss: 0.75633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1682 | Classification loss: 0.50641 | Regression loss: 0.30472 | Running loss: 0.75623 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1683 | Classification loss: 0.38089 | Regression loss: 0.58174 | Running loss: 0.75789 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1684 | Classification loss: 0.25117 | Regression loss: 0.44925 | Running loss: 0.75824 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1685 | Classification loss: 0.45836 | Regression loss: 0.23776 | Running loss: 0.75903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1686 | Classification loss: 0.99764 | Regression loss: 0.56879 | Running loss: 0.76063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1687 | Classification loss: 0.58218 | Regression loss: 0.51732 | Running loss: 0.75972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1688 | Classification loss: 0.50619 | Regression loss: 0.31615 | Running loss: 0.76099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1689 | Classification loss: 0.27555 | Regression loss: 0.39316 | Running loss: 0.76154 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1690 | Classification loss: 0.00621 | Regression loss: 0.21303 | Running loss: 0.76138 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1691 | Classification loss: 0.91666 | Regression loss: 0.27836 | Running loss: 0.76327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1692 | Classification loss: 0.14612 | Regression loss: 0.19919 | Running loss: 0.76315 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1693 | Classification loss: 0.19045 | Regression loss: 0.11259 | Running loss: 0.76188 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1694 | Classification loss: 0.73128 | Regression loss: 0.36361 | Running loss: 0.76263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1695 | Classification loss: 0.25865 | Regression loss: 0.21163 | Running loss: 0.76221 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1696 | Classification loss: 0.15002 | Regression loss: 0.32360 | Running loss: 0.76230 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1697 | Classification loss: 1.32612 | Regression loss: 0.45601 | Running loss: 0.76355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1698 | Classification loss: 1.31956 | Regression loss: 0.37474 | Running loss: 0.76500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1699 | Classification loss: 0.09000 | Regression loss: 0.16520 | Running loss: 0.76306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1700 | Classification loss: 0.31806 | Regression loss: 0.42442 | Running loss: 0.76298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1701 | Classification loss: 0.44669 | Regression loss: 0.28193 | Running loss: 0.76377 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1702 | Classification loss: 0.10669 | Regression loss: 0.34985 | Running loss: 0.76416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1703 | Classification loss: 0.00060 | Regression loss: 0.17832 | Running loss: 0.76347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1704 | Classification loss: 0.00168 | Regression loss: 0.08272 | Running loss: 0.76118 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1705 | Classification loss: 0.24329 | Regression loss: 0.35289 | Running loss: 0.76095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1706 | Classification loss: 0.10040 | Regression loss: 0.49472 | Running loss: 0.76007 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1707 | Classification loss: 0.77356 | Regression loss: 0.56830 | Running loss: 0.76199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1708 | Classification loss: 0.39861 | Regression loss: 0.44802 | Running loss: 0.76321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1709 | Classification loss: 0.13985 | Regression loss: 0.13426 | Running loss: 0.76302 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1710 | Classification loss: 0.01156 | Regression loss: 0.09652 | Running loss: 0.76163 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1711 | Classification loss: 0.42157 | Regression loss: 0.22062 | Running loss: 0.76241 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1712 | Classification loss: 0.06828 | Regression loss: 0.38489 | Running loss: 0.76257 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1713 | Classification loss: 0.65379 | Regression loss: 0.60977 | Running loss: 0.76258 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1714 | Classification loss: 0.27971 | Regression loss: 0.26681 | Running loss: 0.76350 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1715 | Classification loss: 0.13421 | Regression loss: 0.41269 | Running loss: 0.76132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1716 | Classification loss: 0.01533 | Regression loss: 0.12058 | Running loss: 0.76048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1717 | Classification loss: 0.54643 | Regression loss: 0.65759 | Running loss: 0.76205 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1718 | Classification loss: 3.31987 | Regression loss: 0.96011 | Running loss: 0.77033 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1719 | Classification loss: 0.00218 | Regression loss: 0.05163 | Running loss: 0.76972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1720 | Classification loss: 0.25580 | Regression loss: 0.56026 | Running loss: 0.77057 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1721 | Classification loss: 0.55807 | Regression loss: 0.41198 | Running loss: 0.76814 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1722 | Classification loss: 1.32630 | Regression loss: 0.30215 | Running loss: 0.76970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1723 | Classification loss: 0.42127 | Regression loss: 0.34865 | Running loss: 0.76976 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1724 | Classification loss: 0.62036 | Regression loss: 0.51938 | Running loss: 0.77139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1725 | Classification loss: 0.27344 | Regression loss: 0.40511 | Running loss: 0.77248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1726 | Classification loss: 0.00640 | Regression loss: 0.22477 | Running loss: 0.77152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1727 | Classification loss: 0.02973 | Regression loss: 0.23219 | Running loss: 0.76743 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1728 | Classification loss: 0.77486 | Regression loss: 0.48323 | Running loss: 0.76798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1729 | Classification loss: 0.68219 | Regression loss: 0.49804 | Running loss: 0.76983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1730 | Classification loss: 0.28404 | Regression loss: 0.28479 | Running loss: 0.76843 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1731 | Classification loss: 0.01019 | Regression loss: 0.25039 | Running loss: 0.76753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1732 | Classification loss: 0.20504 | Regression loss: 0.45887 | Running loss: 0.76607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1733 | Classification loss: 0.00461 | Regression loss: 0.13671 | Running loss: 0.76593 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1734 | Classification loss: 0.17259 | Regression loss: 0.58076 | Running loss: 0.76697 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1735 | Classification loss: 0.46713 | Regression loss: 0.51282 | Running loss: 0.76830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1736 | Classification loss: 0.06087 | Regression loss: 0.18489 | Running loss: 0.76752 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1737 | Classification loss: 0.12788 | Regression loss: 0.16485 | Running loss: 0.76746 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1738 | Classification loss: 1.19080 | Regression loss: 0.42929 | Running loss: 0.76975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1739 | Classification loss: 1.59024 | Regression loss: 0.44860 | Running loss: 0.77348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1740 | Classification loss: 0.92308 | Regression loss: 0.61968 | Running loss: 0.77617 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1741 | Classification loss: 0.13083 | Regression loss: 0.34427 | Running loss: 0.77644 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1742 | Classification loss: 0.05261 | Regression loss: 0.04492 | Running loss: 0.77634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1743 | Classification loss: 0.51149 | Regression loss: 0.40327 | Running loss: 0.77781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1744 | Classification loss: 0.86490 | Regression loss: 0.41263 | Running loss: 0.77510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1745 | Classification loss: 0.08139 | Regression loss: 0.38656 | Running loss: 0.77526 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1746 | Classification loss: 1.99683 | Regression loss: 0.76532 | Running loss: 0.77998 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1747 | Classification loss: 2.54010 | Regression loss: 0.69330 | Running loss: 0.78522 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1748 | Classification loss: 0.42099 | Regression loss: 0.37747 | Running loss: 0.78668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1749 | Classification loss: 0.26956 | Regression loss: 0.29620 | Running loss: 0.78609 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1750 | Classification loss: 0.69364 | Regression loss: 0.59650 | Running loss: 0.78835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1751 | Classification loss: 0.67430 | Regression loss: 0.51572 | Running loss: 0.78839 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1752 | Classification loss: 0.04407 | Regression loss: 0.09711 | Running loss: 0.78748 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1753 | Classification loss: 1.77211 | Regression loss: 0.58161 | Running loss: 0.79162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1754 | Classification loss: 0.27217 | Regression loss: 0.37452 | Running loss: 0.79279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1755 | Classification loss: 1.74586 | Regression loss: 0.76362 | Running loss: 0.79690 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1756 | Classification loss: 0.22818 | Regression loss: 0.33238 | Running loss: 0.79745 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1757 | Classification loss: 0.16379 | Regression loss: 0.33123 | Running loss: 0.79581 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1758 | Classification loss: 0.28945 | Regression loss: 0.47396 | Running loss: 0.79677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1759 | Classification loss: 0.15988 | Regression loss: 0.49589 | Running loss: 0.79376 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1760 | Classification loss: 0.19222 | Regression loss: 0.14519 | Running loss: 0.79290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1761 | Classification loss: 0.40714 | Regression loss: 0.55036 | Running loss: 0.79402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1762 | Classification loss: 0.27024 | Regression loss: 0.32325 | Running loss: 0.79416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1763 | Classification loss: 0.88005 | Regression loss: 0.28471 | Running loss: 0.79604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1764 | Classification loss: 0.42562 | Regression loss: 0.45481 | Running loss: 0.79445 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1765 | Classification loss: 0.39489 | Regression loss: 0.48132 | Running loss: 0.79452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1766 | Classification loss: 0.11932 | Regression loss: 0.40037 | Running loss: 0.79478 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1767 | Classification loss: 0.56461 | Regression loss: 0.45065 | Running loss: 0.79518 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1768 | Classification loss: 0.00348 | Regression loss: 0.09632 | Running loss: 0.78580 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1769 | Classification loss: 0.46046 | Regression loss: 0.23858 | Running loss: 0.78507 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1770 | Classification loss: 0.00523 | Regression loss: 0.15413 | Running loss: 0.78422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1771 | Classification loss: 0.46733 | Regression loss: 0.56800 | Running loss: 0.78442 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1772 | Classification loss: 0.09361 | Regression loss: 0.49761 | Running loss: 0.78329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1773 | Classification loss: 0.51583 | Regression loss: 0.24060 | Running loss: 0.78452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1774 | Classification loss: 0.15917 | Regression loss: 0.12943 | Running loss: 0.78451 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1775 | Classification loss: 2.18405 | Regression loss: 0.71270 | Running loss: 0.78785 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1776 | Classification loss: 0.00079 | Regression loss: 0.07207 | Running loss: 0.78741 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1777 | Classification loss: 0.09010 | Regression loss: 0.14493 | Running loss: 0.78752 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1778 | Classification loss: 0.02259 | Regression loss: 0.15246 | Running loss: 0.78758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1779 | Classification loss: 0.13481 | Regression loss: 0.08194 | Running loss: 0.78720 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1780 | Classification loss: 0.11728 | Regression loss: 0.33186 | Running loss: 0.78529 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1781 | Classification loss: 0.54185 | Regression loss: 0.36333 | Running loss: 0.78500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1782 | Classification loss: 0.74988 | Regression loss: 0.40026 | Running loss: 0.78466 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1783 | Classification loss: 1.58392 | Regression loss: 0.48091 | Running loss: 0.78803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1784 | Classification loss: 0.87332 | Regression loss: 0.38551 | Running loss: 0.78922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1785 | Classification loss: 0.00507 | Regression loss: 0.04144 | Running loss: 0.78655 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1786 | Classification loss: 1.50852 | Regression loss: 0.63307 | Running loss: 0.78472 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1787 | Classification loss: 0.10218 | Regression loss: 0.10877 | Running loss: 0.78343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1788 | Classification loss: 0.02068 | Regression loss: 0.22832 | Running loss: 0.78303 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 1789 | Classification loss: 0.62067 | Regression loss: 0.51221 | Running loss: 0.78111 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1790 | Classification loss: 0.22902 | Regression loss: 0.35777 | Running loss: 0.78086 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1791 | Classification loss: 0.75219 | Regression loss: 0.81826 | Running loss: 0.78332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1792 | Classification loss: 0.01978 | Regression loss: 0.10646 | Running loss: 0.78117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1793 | Classification loss: 0.00499 | Regression loss: 0.05377 | Running loss: 0.77915 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1794 | Classification loss: 0.41545 | Regression loss: 0.33493 | Running loss: 0.77853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1795 | Classification loss: 0.00053 | Regression loss: 0.13565 | Running loss: 0.77803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1796 | Classification loss: 0.21540 | Regression loss: 0.45880 | Running loss: 0.77790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1797 | Classification loss: 0.68219 | Regression loss: 0.44463 | Running loss: 0.77986 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1798 | Classification loss: 0.39218 | Regression loss: 0.48735 | Running loss: 0.78082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1799 | Classification loss: 1.06876 | Regression loss: 0.49949 | Running loss: 0.78040 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1800 | Classification loss: 0.55414 | Regression loss: 0.65633 | Running loss: 0.78244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1801 | Classification loss: 0.00782 | Regression loss: 0.09849 | Running loss: 0.77888 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1802 | Classification loss: 0.65900 | Regression loss: 0.46844 | Running loss: 0.78056 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1803 | Classification loss: 0.38197 | Regression loss: 0.57339 | Running loss: 0.78187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1804 | Classification loss: 0.00694 | Regression loss: 0.26901 | Running loss: 0.78004 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1805 | Classification loss: 0.03918 | Regression loss: 0.15587 | Running loss: 0.78023 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1806 | Classification loss: 0.09529 | Regression loss: 0.15124 | Running loss: 0.77727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1807 | Classification loss: 0.00777 | Regression loss: 0.14289 | Running loss: 0.77081 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1808 | Classification loss: 1.57492 | Regression loss: 0.46244 | Running loss: 0.77442 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1809 | Classification loss: 0.63655 | Regression loss: 0.57659 | Running loss: 0.77593 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1810 | Classification loss: 0.73209 | Regression loss: 0.40224 | Running loss: 0.77583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1811 | Classification loss: 0.00602 | Regression loss: 0.03761 | Running loss: 0.77485 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1812 | Classification loss: 0.30122 | Regression loss: 0.43457 | Running loss: 0.77562 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1813 | Classification loss: 0.16751 | Regression loss: 0.17870 | Running loss: 0.77421 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1814 | Classification loss: 0.24351 | Regression loss: 0.37504 | Running loss: 0.77478 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1815 | Classification loss: 0.01161 | Regression loss: 0.35456 | Running loss: 0.77355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1816 | Classification loss: 1.10755 | Regression loss: 0.32971 | Running loss: 0.77525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1817 | Classification loss: 0.88345 | Regression loss: 0.54207 | Running loss: 0.77652 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1818 | Classification loss: 0.64311 | Regression loss: 0.26806 | Running loss: 0.77662 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1819 | Classification loss: 0.28568 | Regression loss: 0.43544 | Running loss: 0.77763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1820 | Classification loss: 0.01835 | Regression loss: 0.09555 | Running loss: 0.77554 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1821 | Classification loss: 0.03788 | Regression loss: 0.18751 | Running loss: 0.77493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1822 | Classification loss: 1.09063 | Regression loss: 0.50192 | Running loss: 0.77704 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1823 | Classification loss: 0.63825 | Regression loss: 0.53899 | Running loss: 0.77890 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1824 | Classification loss: 0.27304 | Regression loss: 0.40170 | Running loss: 0.77814 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1825 | Classification loss: 0.08909 | Regression loss: 0.52744 | Running loss: 0.77857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1826 | Classification loss: 0.70249 | Regression loss: 0.10245 | Running loss: 0.77928 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1827 | Classification loss: 0.02768 | Regression loss: 0.12175 | Running loss: 0.77709 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1828 | Classification loss: 0.05283 | Regression loss: 0.11232 | Running loss: 0.77708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1829 | Classification loss: 0.00102 | Regression loss: 0.10478 | Running loss: 0.77401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1830 | Classification loss: 0.43086 | Regression loss: 0.56369 | Running loss: 0.77132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1831 | Classification loss: 0.67643 | Regression loss: 0.31365 | Running loss: 0.77285 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1832 | Classification loss: 0.25552 | Regression loss: 0.39138 | Running loss: 0.77286 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1833 | Classification loss: 0.46629 | Regression loss: 0.48318 | Running loss: 0.77304 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1834 | Classification loss: 0.01159 | Regression loss: 0.11988 | Running loss: 0.77289 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1835 | Classification loss: 0.46472 | Regression loss: 0.31940 | Running loss: 0.77419 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1836 | Classification loss: 0.07891 | Regression loss: 0.23943 | Running loss: 0.77446 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1837 | Classification loss: 1.14094 | Regression loss: 0.87451 | Running loss: 0.77771 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1838 | Classification loss: 0.00255 | Regression loss: 0.06869 | Running loss: 0.77724 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1839 | Classification loss: 0.70257 | Regression loss: 0.25757 | Running loss: 0.77571 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1840 | Classification loss: 1.01843 | Regression loss: 0.00000 | Running loss: 0.77643 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1841 | Classification loss: 0.34074 | Regression loss: 0.28007 | Running loss: 0.77548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1842 | Classification loss: 1.77870 | Regression loss: 0.27275 | Running loss: 0.77930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1843 | Classification loss: 0.61042 | Regression loss: 0.51673 | Running loss: 0.77886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1844 | Classification loss: 0.51890 | Regression loss: 0.49522 | Running loss: 0.77857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1845 | Classification loss: 0.62397 | Regression loss: 0.60218 | Running loss: 0.78094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1846 | Classification loss: 0.07255 | Regression loss: 0.25875 | Running loss: 0.78124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1847 | Classification loss: 0.48083 | Regression loss: 0.47667 | Running loss: 0.78173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1848 | Classification loss: 0.27055 | Regression loss: 0.30181 | Running loss: 0.78256 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1849 | Classification loss: 0.44992 | Regression loss: 0.43555 | Running loss: 0.78400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1850 | Classification loss: 0.14951 | Regression loss: 0.27282 | Running loss: 0.78410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1851 | Classification loss: 0.12940 | Regression loss: 0.19550 | Running loss: 0.78224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1852 | Classification loss: 0.06396 | Regression loss: 0.25519 | Running loss: 0.78262 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1853 | Classification loss: 0.18581 | Regression loss: 0.33082 | Running loss: 0.78270 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1854 | Classification loss: 0.33591 | Regression loss: 0.30175 | Running loss: 0.78379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1855 | Classification loss: 0.04954 | Regression loss: 0.17932 | Running loss: 0.78319 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1856 | Classification loss: 0.02891 | Regression loss: 0.15448 | Running loss: 0.78305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1857 | Classification loss: 0.04600 | Regression loss: 0.18602 | Running loss: 0.78328 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1858 | Classification loss: 0.00776 | Regression loss: 0.11234 | Running loss: 0.78274 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1859 | Classification loss: 0.14727 | Regression loss: 0.35781 | Running loss: 0.78072 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1860 | Classification loss: 0.66840 | Regression loss: 0.55132 | Running loss: 0.78192 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1861 | Classification loss: 0.70037 | Regression loss: 0.58270 | Running loss: 0.78333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1862 | Classification loss: 0.17750 | Regression loss: 0.15590 | Running loss: 0.78313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1863 | Classification loss: 0.02976 | Regression loss: 0.12997 | Running loss: 0.78012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1864 | Classification loss: 0.09095 | Regression loss: 0.25127 | Running loss: 0.78043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1865 | Classification loss: 0.02531 | Regression loss: 0.22664 | Running loss: 0.78021 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1866 | Classification loss: 1.90397 | Regression loss: 0.46170 | Running loss: 0.78461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1867 | Classification loss: 0.01275 | Regression loss: 0.10710 | Running loss: 0.78448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1868 | Classification loss: 0.09598 | Regression loss: 0.16503 | Running loss: 0.78194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1869 | Classification loss: 0.06584 | Regression loss: 0.12892 | Running loss: 0.78099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1870 | Classification loss: 0.07306 | Regression loss: 0.25935 | Running loss: 0.78067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1871 | Classification loss: 0.06289 | Regression loss: 0.28634 | Running loss: 0.78057 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1872 | Classification loss: 0.59869 | Regression loss: 0.48112 | Running loss: 0.78020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1873 | Classification loss: 0.45513 | Regression loss: 0.47561 | Running loss: 0.78137 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1874 | Classification loss: 0.03481 | Regression loss: 0.15673 | Running loss: 0.78052 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1875 | Classification loss: 0.97107 | Regression loss: 0.50768 | Running loss: 0.78313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1876 | Classification loss: 0.36536 | Regression loss: 0.52696 | Running loss: 0.78441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1877 | Classification loss: 0.16153 | Regression loss: 0.36612 | Running loss: 0.78438 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1878 | Classification loss: 1.18098 | Regression loss: 0.43005 | Running loss: 0.78501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1879 | Classification loss: 1.31849 | Regression loss: 0.45197 | Running loss: 0.78613 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1880 | Classification loss: 0.76044 | Regression loss: 0.27378 | Running loss: 0.78534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1881 | Classification loss: 0.14243 | Regression loss: 0.31359 | Running loss: 0.78493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1882 | Classification loss: 0.70485 | Regression loss: 0.17453 | Running loss: 0.78501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1883 | Classification loss: 2.04201 | Regression loss: 0.94372 | Running loss: 0.79016 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1884 | Classification loss: 1.91113 | Regression loss: 0.42028 | Running loss: 0.79055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1885 | Classification loss: 0.00007 | Regression loss: 0.02938 | Running loss: 0.78920 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1886 | Classification loss: 0.22116 | Regression loss: 0.22672 | Running loss: 0.78977 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1887 | Classification loss: 0.13374 | Regression loss: 0.33284 | Running loss: 0.78644 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1888 | Classification loss: 0.89802 | Regression loss: 0.58655 | Running loss: 0.78842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1889 | Classification loss: 0.57049 | Regression loss: 0.44291 | Running loss: 0.78939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1890 | Classification loss: 0.02348 | Regression loss: 0.04854 | Running loss: 0.78874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1891 | Classification loss: 0.23635 | Regression loss: 0.31937 | Running loss: 0.78891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1892 | Classification loss: 0.59428 | Regression loss: 0.61630 | Running loss: 0.78948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1893 | Classification loss: 0.23671 | Regression loss: 0.42649 | Running loss: 0.79033 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1894 | Classification loss: 0.78446 | Regression loss: 0.34999 | Running loss: 0.78913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1895 | Classification loss: 0.00467 | Regression loss: 0.05767 | Running loss: 0.78696 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1896 | Classification loss: 0.58180 | Regression loss: 0.53209 | Running loss: 0.78562 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1897 | Classification loss: 0.11899 | Regression loss: 0.12510 | Running loss: 0.78420 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1898 | Classification loss: 0.15563 | Regression loss: 0.16728 | Running loss: 0.78462 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1899 | Classification loss: 2.42973 | Regression loss: 0.70989 | Running loss: 0.79045 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1900 | Classification loss: 0.04762 | Regression loss: 0.28440 | Running loss: 0.79086 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1901 | Classification loss: 0.76422 | Regression loss: 0.59507 | Running loss: 0.79301 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1902 | Classification loss: 0.19791 | Regression loss: 0.35869 | Running loss: 0.79272 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1903 | Classification loss: 0.12577 | Regression loss: 0.41310 | Running loss: 0.79078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1904 | Classification loss: 0.49017 | Regression loss: 0.43320 | Running loss: 0.79099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1905 | Classification loss: 1.10400 | Regression loss: 0.43410 | Running loss: 0.79301 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1906 | Classification loss: 0.31774 | Regression loss: 0.24530 | Running loss: 0.79242 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1907 | Classification loss: 0.01738 | Regression loss: 0.08777 | Running loss: 0.79207 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1908 | Classification loss: 0.08782 | Regression loss: 0.13186 | Running loss: 0.79022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1909 | Classification loss: 0.48835 | Regression loss: 0.66470 | Running loss: 0.79223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1910 | Classification loss: 0.09039 | Regression loss: 0.37952 | Running loss: 0.79167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1911 | Classification loss: 0.87334 | Regression loss: 0.65040 | Running loss: 0.79364 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1912 | Classification loss: 0.44299 | Regression loss: 0.46112 | Running loss: 0.79233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1913 | Classification loss: 0.55903 | Regression loss: 0.55106 | Running loss: 0.79067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1914 | Classification loss: 0.12679 | Regression loss: 0.14212 | Running loss: 0.79094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1915 | Classification loss: 0.02471 | Regression loss: 0.09665 | Running loss: 0.79066 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1916 | Classification loss: 0.01498 | Regression loss: 0.21942 | Running loss: 0.78993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1917 | Classification loss: 0.20898 | Regression loss: 0.20846 | Running loss: 0.79035 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1918 | Classification loss: 0.66536 | Regression loss: 0.50648 | Running loss: 0.79091 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1919 | Classification loss: 0.60519 | Regression loss: 0.20501 | Running loss: 0.79024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1920 | Classification loss: 0.03555 | Regression loss: 0.32246 | Running loss: 0.79045 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1921 | Classification loss: 1.14798 | Regression loss: 0.54488 | Running loss: 0.79250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1922 | Classification loss: 0.09794 | Regression loss: 0.20158 | Running loss: 0.79235 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1923 | Classification loss: 0.13634 | Regression loss: 0.19653 | Running loss: 0.79279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1924 | Classification loss: 0.37704 | Regression loss: 0.34351 | Running loss: 0.79355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1925 | Classification loss: 0.81949 | Regression loss: 0.32058 | Running loss: 0.79348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1926 | Classification loss: 0.00140 | Regression loss: 0.09019 | Running loss: 0.79173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1927 | Classification loss: 0.36809 | Regression loss: 0.32885 | Running loss: 0.79255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1928 | Classification loss: 0.08012 | Regression loss: 0.14774 | Running loss: 0.79264 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1929 | Classification loss: 0.10075 | Regression loss: 0.29427 | Running loss: 0.79252 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1930 | Classification loss: 0.26323 | Regression loss: 0.54756 | Running loss: 0.79403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1931 | Classification loss: 0.16819 | Regression loss: 0.14387 | Running loss: 0.79199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1932 | Classification loss: 3.93590 | Regression loss: 0.85670 | Running loss: 0.79981 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1933 | Classification loss: 0.21739 | Regression loss: 0.27686 | Running loss: 0.80027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1934 | Classification loss: 0.02840 | Regression loss: 0.06526 | Running loss: 0.79952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1935 | Classification loss: 0.02190 | Regression loss: 0.35140 | Running loss: 0.79935 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1936 | Classification loss: 0.06713 | Regression loss: 0.10914 | Running loss: 0.79812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1937 | Classification loss: 0.64298 | Regression loss: 0.50468 | Running loss: 0.79964 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1938 | Classification loss: 0.00282 | Regression loss: 0.17195 | Running loss: 0.79974 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1939 | Classification loss: 0.24809 | Regression loss: 0.41004 | Running loss: 0.79895 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1940 | Classification loss: 0.32181 | Regression loss: 0.28545 | Running loss: 0.79969 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1941 | Classification loss: 0.46508 | Regression loss: 0.67316 | Running loss: 0.80106 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1942 | Classification loss: 0.03343 | Regression loss: 0.18860 | Running loss: 0.79867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1943 | Classification loss: 0.04695 | Regression loss: 0.10196 | Running loss: 0.79830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1944 | Classification loss: 0.06838 | Regression loss: 0.40364 | Running loss: 0.79735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1945 | Classification loss: 0.03033 | Regression loss: 0.33022 | Running loss: 0.79678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1946 | Classification loss: 0.39833 | Regression loss: 0.28780 | Running loss: 0.79788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1947 | Classification loss: 0.11138 | Regression loss: 0.16789 | Running loss: 0.79658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1948 | Classification loss: 0.18489 | Regression loss: 0.39595 | Running loss: 0.79510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1949 | Classification loss: 2.42186 | Regression loss: 0.79160 | Running loss: 0.79871 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1950 | Classification loss: 0.08732 | Regression loss: 0.47530 | Running loss: 0.79673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1951 | Classification loss: 0.73339 | Regression loss: 0.64640 | Running loss: 0.79524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1952 | Classification loss: 0.46883 | Regression loss: 0.42713 | Running loss: 0.79551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1953 | Classification loss: 0.00095 | Regression loss: 0.07776 | Running loss: 0.79536 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1954 | Classification loss: 0.00015 | Regression loss: 0.11651 | Running loss: 0.79508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1955 | Classification loss: 0.20292 | Regression loss: 0.16075 | Running loss: 0.79323 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1956 | Classification loss: 0.99915 | Regression loss: 0.51216 | Running loss: 0.79342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1957 | Classification loss: 0.28863 | Regression loss: 0.34505 | Running loss: 0.79329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1958 | Classification loss: 0.25466 | Regression loss: 0.17592 | Running loss: 0.79382 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1959 | Classification loss: 0.53586 | Regression loss: 0.44458 | Running loss: 0.79395 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1960 | Classification loss: 0.00934 | Regression loss: 0.22533 | Running loss: 0.79387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1961 | Classification loss: 0.95365 | Regression loss: 0.36180 | Running loss: 0.79620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1962 | Classification loss: 1.43760 | Regression loss: 0.49660 | Running loss: 0.79954 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1963 | Classification loss: 0.00931 | Regression loss: 0.11345 | Running loss: 0.79860 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 1964 | Classification loss: 1.45465 | Regression loss: 0.56493 | Running loss: 0.79779 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1965 | Classification loss: 0.95382 | Regression loss: 0.75284 | Running loss: 0.80088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1966 | Classification loss: 0.54463 | Regression loss: 0.37432 | Running loss: 0.80144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1967 | Classification loss: 0.10103 | Regression loss: 0.30990 | Running loss: 0.80182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1968 | Classification loss: 0.13218 | Regression loss: 0.12172 | Running loss: 0.80019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1969 | Classification loss: 0.10003 | Regression loss: 0.25411 | Running loss: 0.80040 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1970 | Classification loss: 0.16171 | Regression loss: 0.07860 | Running loss: 0.79986 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1971 | Classification loss: 0.36112 | Regression loss: 0.25577 | Running loss: 0.80014 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1972 | Classification loss: 0.32199 | Regression loss: 0.43998 | Running loss: 0.79987 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1973 | Classification loss: 0.33863 | Regression loss: 0.32492 | Running loss: 0.79948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1974 | Classification loss: 0.12507 | Regression loss: 0.26651 | Running loss: 0.79956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1975 | Classification loss: 0.00024 | Regression loss: 0.07719 | Running loss: 0.79935 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1976 | Classification loss: 0.35543 | Regression loss: 0.59760 | Running loss: 0.80066 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1977 | Classification loss: 0.11972 | Regression loss: 0.29072 | Running loss: 0.80048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1978 | Classification loss: 0.02223 | Regression loss: 0.04496 | Running loss: 0.79903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1979 | Classification loss: 0.15610 | Regression loss: 0.32071 | Running loss: 0.79781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1980 | Classification loss: 0.20007 | Regression loss: 0.33651 | Running loss: 0.79663 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1981 | Classification loss: 0.03839 | Regression loss: 0.29466 | Running loss: 0.79417 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1982 | Classification loss: 0.06772 | Regression loss: 0.16155 | Running loss: 0.79368 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1983 | Classification loss: 0.01814 | Regression loss: 0.16781 | Running loss: 0.79186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1984 | Classification loss: 0.00051 | Regression loss: 0.01274 | Running loss: 0.78972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1985 | Classification loss: 0.02003 | Regression loss: 0.20879 | Running loss: 0.78974 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 1986 | Classification loss: 0.18661 | Regression loss: 0.22046 | Running loss: 0.78975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1987 | Classification loss: 1.39082 | Regression loss: 0.23903 | Running loss: 0.79012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1988 | Classification loss: 2.83432 | Regression loss: 1.08025 | Running loss: 0.79580 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1989 | Classification loss: 0.55358 | Regression loss: 0.57279 | Running loss: 0.79707 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1990 | Classification loss: 1.17729 | Regression loss: 0.29962 | Running loss: 0.79707 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1991 | Classification loss: 0.42332 | Regression loss: 0.40572 | Running loss: 0.79804 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1992 | Classification loss: 0.97506 | Regression loss: 0.79950 | Running loss: 0.79878 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1993 | Classification loss: 0.16607 | Regression loss: 0.24243 | Running loss: 0.79836 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1994 | Classification loss: 1.52157 | Regression loss: 0.76961 | Running loss: 0.80219 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1995 | Classification loss: 0.28548 | Regression loss: 0.17051 | Running loss: 0.80120 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1996 | Classification loss: 0.14150 | Regression loss: 0.27511 | Running loss: 0.79868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1997 | Classification loss: 0.68221 | Regression loss: 0.44593 | Running loss: 0.80039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1998 | Classification loss: 0.17364 | Regression loss: 0.26584 | Running loss: 0.80069 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 1999 | Classification loss: 0.34202 | Regression loss: 0.50301 | Running loss: 0.80208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2000 | Classification loss: 0.74086 | Regression loss: 0.66138 | Running loss: 0.80319 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2001 | Classification loss: 0.14413 | Regression loss: 0.20654 | Running loss: 0.80073 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2002 | Classification loss: 0.33339 | Regression loss: 0.10703 | Running loss: 0.79753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2003 | Classification loss: 0.00853 | Regression loss: 0.17081 | Running loss: 0.79770 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2004 | Classification loss: 0.00846 | Regression loss: 0.12029 | Running loss: 0.79783 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2005 | Classification loss: 0.28311 | Regression loss: 0.36883 | Running loss: 0.79896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2006 | Classification loss: 0.27439 | Regression loss: 0.33279 | Running loss: 0.79938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2007 | Classification loss: 0.36718 | Regression loss: 0.32012 | Running loss: 0.80015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2008 | Classification loss: 0.48721 | Regression loss: 0.41998 | Running loss: 0.80001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2009 | Classification loss: 0.39523 | Regression loss: 0.59760 | Running loss: 0.80128 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2010 | Classification loss: 0.52902 | Regression loss: 0.28989 | Running loss: 0.80101 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2011 | Classification loss: 0.36382 | Regression loss: 0.33248 | Running loss: 0.80121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2012 | Classification loss: 0.89049 | Regression loss: 0.30810 | Running loss: 0.80298 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2013 | Classification loss: 0.07044 | Regression loss: 0.30408 | Running loss: 0.80266 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2014 | Classification loss: 0.10258 | Regression loss: 0.35924 | Running loss: 0.80004 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2015 | Classification loss: 1.25333 | Regression loss: 0.11882 | Running loss: 0.80240 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2016 | Classification loss: 0.42305 | Regression loss: 0.65653 | Running loss: 0.80256 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2017 | Classification loss: 0.31499 | Regression loss: 0.38921 | Running loss: 0.80241 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2018 | Classification loss: 0.45589 | Regression loss: 0.70420 | Running loss: 0.80368 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2019 | Classification loss: 0.43255 | Regression loss: 0.68602 | Running loss: 0.80410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2020 | Classification loss: 0.35285 | Regression loss: 0.50381 | Running loss: 0.80509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2021 | Classification loss: 1.38369 | Regression loss: 0.62588 | Running loss: 0.80825 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2022 | Classification loss: 0.16791 | Regression loss: 0.43409 | Running loss: 0.80834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2023 | Classification loss: 0.30043 | Regression loss: 0.31236 | Running loss: 0.80850 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2024 | Classification loss: 0.00443 | Regression loss: 0.16665 | Running loss: 0.80588 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2025 | Classification loss: 0.00534 | Regression loss: 0.18515 | Running loss: 0.80572 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2026 | Classification loss: 0.06833 | Regression loss: 0.24617 | Running loss: 0.80126 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2027 | Classification loss: 0.19654 | Regression loss: 0.21472 | Running loss: 0.80169 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2028 | Classification loss: 0.29814 | Regression loss: 0.33107 | Running loss: 0.80192 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2029 | Classification loss: 1.01646 | Regression loss: 0.57997 | Running loss: 0.80445 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2030 | Classification loss: 0.18504 | Regression loss: 0.50550 | Running loss: 0.80446 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2031 | Classification loss: 0.33352 | Regression loss: 0.34540 | Running loss: 0.80523 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2032 | Classification loss: 0.03123 | Regression loss: 0.20080 | Running loss: 0.80399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2033 | Classification loss: 0.37335 | Regression loss: 0.47258 | Running loss: 0.80280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2034 | Classification loss: 0.78376 | Regression loss: 0.43878 | Running loss: 0.80236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2035 | Classification loss: 0.74008 | Regression loss: 0.75043 | Running loss: 0.80177 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2036 | Classification loss: 1.87690 | Regression loss: 0.23100 | Running loss: 0.80578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2037 | Classification loss: 0.26930 | Regression loss: 0.29547 | Running loss: 0.80502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2038 | Classification loss: 1.01780 | Regression loss: 0.20934 | Running loss: 0.80551 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2039 | Classification loss: 1.27520 | Regression loss: 0.52016 | Running loss: 0.80667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2040 | Classification loss: 0.16237 | Regression loss: 0.09394 | Running loss: 0.80628 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2041 | Classification loss: 0.25108 | Regression loss: 0.16190 | Running loss: 0.80688 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2042 | Classification loss: 0.15815 | Regression loss: 0.42231 | Running loss: 0.80761 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2043 | Classification loss: 0.18561 | Regression loss: 0.39205 | Running loss: 0.80764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2044 | Classification loss: 0.87803 | Regression loss: 0.60290 | Running loss: 0.80999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2045 | Classification loss: 0.00673 | Regression loss: 0.17350 | Running loss: 0.81024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2046 | Classification loss: 1.13481 | Regression loss: 0.27224 | Running loss: 0.81262 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2047 | Classification loss: 0.22140 | Regression loss: 0.34649 | Running loss: 0.81238 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2048 | Classification loss: 0.55279 | Regression loss: 0.47002 | Running loss: 0.81357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2049 | Classification loss: 0.00351 | Regression loss: 0.01513 | Running loss: 0.81334 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2050 | Classification loss: 0.85659 | Regression loss: 0.31139 | Running loss: 0.81479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2051 | Classification loss: 0.37970 | Regression loss: 0.52440 | Running loss: 0.81060 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2052 | Classification loss: 0.00102 | Regression loss: 0.26441 | Running loss: 0.81030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2053 | Classification loss: 0.04377 | Regression loss: 0.30129 | Running loss: 0.80991 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2054 | Classification loss: 0.00283 | Regression loss: 0.02549 | Running loss: 0.80948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2055 | Classification loss: 0.55440 | Regression loss: 0.39338 | Running loss: 0.80956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2056 | Classification loss: 1.19063 | Regression loss: 0.48758 | Running loss: 0.81053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2057 | Classification loss: 0.02396 | Regression loss: 0.28713 | Running loss: 0.81089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2058 | Classification loss: 0.00282 | Regression loss: 0.09584 | Running loss: 0.81044 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2059 | Classification loss: 0.03272 | Regression loss: 0.19767 | Running loss: 0.80769 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2060 | Classification loss: 0.14844 | Regression loss: 0.49074 | Running loss: 0.80849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2061 | Classification loss: 0.12309 | Regression loss: 0.38414 | Running loss: 0.80672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2062 | Classification loss: 0.04084 | Regression loss: 0.21052 | Running loss: 0.80441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2063 | Classification loss: 0.00007 | Regression loss: 0.16262 | Running loss: 0.80456 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2064 | Classification loss: 0.95371 | Regression loss: 0.59910 | Running loss: 0.80336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2065 | Classification loss: 1.95711 | Regression loss: 0.92755 | Running loss: 0.80799 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2066 | Classification loss: 0.19172 | Regression loss: 0.18613 | Running loss: 0.80841 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2067 | Classification loss: 0.10064 | Regression loss: 0.37835 | Running loss: 0.80759 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2068 | Classification loss: 0.65930 | Regression loss: 0.49456 | Running loss: 0.80900 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2069 | Classification loss: 0.02024 | Regression loss: 0.16718 | Running loss: 0.80899 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2070 | Classification loss: 0.10619 | Regression loss: 0.32596 | Running loss: 0.80963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2071 | Classification loss: 0.52492 | Regression loss: 0.32697 | Running loss: 0.80842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2072 | Classification loss: 0.19919 | Regression loss: 0.31257 | Running loss: 0.80824 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2073 | Classification loss: 1.16331 | Regression loss: 0.55366 | Running loss: 0.81125 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2074 | Classification loss: 0.19836 | Regression loss: 0.29469 | Running loss: 0.81082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2075 | Classification loss: 0.84977 | Regression loss: 0.06154 | Running loss: 0.80975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2076 | Classification loss: 0.35962 | Regression loss: 0.22031 | Running loss: 0.80834 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2077 | Classification loss: 0.00266 | Regression loss: 0.08387 | Running loss: 0.80621 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2078 | Classification loss: 0.02129 | Regression loss: 0.04102 | Running loss: 0.80617 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2079 | Classification loss: 0.21565 | Regression loss: 0.48593 | Running loss: 0.80534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2080 | Classification loss: 0.08393 | Regression loss: 0.27870 | Running loss: 0.80531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2081 | Classification loss: 0.27238 | Regression loss: 0.35491 | Running loss: 0.80503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2082 | Classification loss: 0.72532 | Regression loss: 0.67881 | Running loss: 0.80618 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2083 | Classification loss: 0.09253 | Regression loss: 0.21176 | Running loss: 0.80628 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2084 | Classification loss: 0.17616 | Regression loss: 0.25082 | Running loss: 0.80701 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2085 | Classification loss: 0.23324 | Regression loss: 0.04161 | Running loss: 0.80644 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2086 | Classification loss: 0.01030 | Regression loss: 0.25057 | Running loss: 0.80375 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2087 | Classification loss: 0.27750 | Regression loss: 0.17975 | Running loss: 0.80164 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2088 | Classification loss: 0.09858 | Regression loss: 0.24686 | Running loss: 0.80182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2089 | Classification loss: 0.16648 | Regression loss: 0.34187 | Running loss: 0.79992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2090 | Classification loss: 0.18637 | Regression loss: 0.20949 | Running loss: 0.79960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2091 | Classification loss: 0.02948 | Regression loss: 0.20795 | Running loss: 0.79797 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2092 | Classification loss: 0.01389 | Regression loss: 0.14601 | Running loss: 0.79630 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2093 | Classification loss: 0.02141 | Regression loss: 0.17214 | Running loss: 0.79507 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2094 | Classification loss: 2.02762 | Regression loss: 0.46653 | Running loss: 0.79958 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2095 | Classification loss: 0.12318 | Regression loss: 0.17195 | Running loss: 0.79913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2096 | Classification loss: 1.09936 | Regression loss: 0.37050 | Running loss: 0.79849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2097 | Classification loss: 0.34555 | Regression loss: 0.35351 | Running loss: 0.79965 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2098 | Classification loss: 0.02193 | Regression loss: 0.09001 | Running loss: 0.79955 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2099 | Classification loss: 0.18911 | Regression loss: 0.26672 | Running loss: 0.80034 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2100 | Classification loss: 0.20605 | Regression loss: 0.33088 | Running loss: 0.79885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2101 | Classification loss: 0.00056 | Regression loss: 0.10577 | Running loss: 0.79838 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2102 | Classification loss: 0.43771 | Regression loss: 0.68600 | Running loss: 0.79994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2103 | Classification loss: 0.54443 | Regression loss: 0.32344 | Running loss: 0.80106 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2104 | Classification loss: 0.01358 | Regression loss: 0.07195 | Running loss: 0.79947 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2105 | Classification loss: 0.02821 | Regression loss: 0.20427 | Running loss: 0.79514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2106 | Classification loss: 1.28169 | Regression loss: 0.30532 | Running loss: 0.79810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2107 | Classification loss: 0.06554 | Regression loss: 0.04044 | Running loss: 0.79515 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2108 | Classification loss: 0.08354 | Regression loss: 0.31592 | Running loss: 0.79401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2109 | Classification loss: 0.34434 | Regression loss: 0.49774 | Running loss: 0.79481 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2110 | Classification loss: 0.00063 | Regression loss: 0.02280 | Running loss: 0.79365 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2111 | Classification loss: 0.72225 | Regression loss: 0.56516 | Running loss: 0.79527 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2112 | Classification loss: 0.40816 | Regression loss: 0.33747 | Running loss: 0.79313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2113 | Classification loss: 0.01221 | Regression loss: 0.24787 | Running loss: 0.79280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2114 | Classification loss: 0.29326 | Regression loss: 0.36690 | Running loss: 0.79187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2115 | Classification loss: 0.56628 | Regression loss: 0.36699 | Running loss: 0.79301 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2116 | Classification loss: 0.25987 | Regression loss: 0.19069 | Running loss: 0.79098 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2117 | Classification loss: 1.29535 | Regression loss: 0.57139 | Running loss: 0.79427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2118 | Classification loss: 0.24784 | Regression loss: 0.31106 | Running loss: 0.79465 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2119 | Classification loss: 2.20089 | Regression loss: 0.71594 | Running loss: 0.79640 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2120 | Classification loss: 0.79472 | Regression loss: 0.35546 | Running loss: 0.79818 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2121 | Classification loss: 0.02799 | Regression loss: 0.28700 | Running loss: 0.79818 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2122 | Classification loss: 0.03160 | Regression loss: 0.09068 | Running loss: 0.79790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2123 | Classification loss: 0.73815 | Regression loss: 0.45449 | Running loss: 0.79834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2124 | Classification loss: 0.78611 | Regression loss: 0.72863 | Running loss: 0.80082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2125 | Classification loss: 0.57695 | Regression loss: 0.06152 | Running loss: 0.79831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2126 | Classification loss: 0.27552 | Regression loss: 0.10664 | Running loss: 0.79740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2127 | Classification loss: 0.86368 | Regression loss: 0.54250 | Running loss: 0.79879 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2128 | Classification loss: 0.51533 | Regression loss: 0.49088 | Running loss: 0.80052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2129 | Classification loss: 0.28064 | Regression loss: 0.28455 | Running loss: 0.79747 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2130 | Classification loss: 0.76920 | Regression loss: 0.53816 | Running loss: 0.79756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2131 | Classification loss: 0.98313 | Regression loss: 0.57576 | Running loss: 0.79714 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2132 | Classification loss: 0.41986 | Regression loss: 0.54907 | Running loss: 0.79679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2133 | Classification loss: 0.00801 | Regression loss: 0.16628 | Running loss: 0.79615 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2134 | Classification loss: 0.01107 | Regression loss: 0.14000 | Running loss: 0.79508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2135 | Classification loss: 0.53750 | Regression loss: 0.35609 | Running loss: 0.79446 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2136 | Classification loss: 0.02087 | Regression loss: 0.07008 | Running loss: 0.79112 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2137 | Classification loss: 0.41071 | Regression loss: 0.49712 | Running loss: 0.78983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2138 | Classification loss: 1.41986 | Regression loss: 0.50260 | Running loss: 0.79352 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2139 | Classification loss: 1.61299 | Regression loss: 0.51231 | Running loss: 0.79629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2140 | Classification loss: 0.20802 | Regression loss: 0.21991 | Running loss: 0.79577 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2141 | Classification loss: 0.26672 | Regression loss: 0.17937 | Running loss: 0.79647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2142 | Classification loss: 0.90603 | Regression loss: 0.51630 | Running loss: 0.79604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2143 | Classification loss: 0.65113 | Regression loss: 0.37399 | Running loss: 0.79597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2144 | Classification loss: 0.59769 | Regression loss: 0.37754 | Running loss: 0.79523 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2145 | Classification loss: 0.78491 | Regression loss: 0.60101 | Running loss: 0.79774 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2146 | Classification loss: 0.60511 | Regression loss: 0.48528 | Running loss: 0.79487 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2147 | Classification loss: 0.18160 | Regression loss: 0.22261 | Running loss: 0.79534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2148 | Classification loss: 1.12551 | Regression loss: 0.26835 | Running loss: 0.79639 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2149 | Classification loss: 0.81533 | Regression loss: 0.61093 | Running loss: 0.79885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2150 | Classification loss: 0.33862 | Regression loss: 0.46579 | Running loss: 0.79965 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2151 | Classification loss: 0.09837 | Regression loss: 0.30486 | Running loss: 0.79616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2152 | Classification loss: 0.91585 | Regression loss: 0.42048 | Running loss: 0.79713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2153 | Classification loss: 0.00946 | Regression loss: 0.10654 | Running loss: 0.79547 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2154 | Classification loss: 0.33670 | Regression loss: 0.37222 | Running loss: 0.79215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2155 | Classification loss: 1.81129 | Regression loss: 0.75266 | Running loss: 0.79393 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2156 | Classification loss: 0.01973 | Regression loss: 0.09576 | Running loss: 0.79105 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2157 | Classification loss: 2.15624 | Regression loss: 0.56042 | Running loss: 0.79307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2158 | Classification loss: 0.34066 | Regression loss: 0.46402 | Running loss: 0.79374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2159 | Classification loss: 0.88277 | Regression loss: 0.56233 | Running loss: 0.79569 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2160 | Classification loss: 0.05261 | Regression loss: 0.23709 | Running loss: 0.79392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2161 | Classification loss: 0.00842 | Regression loss: 0.13304 | Running loss: 0.79234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2162 | Classification loss: 0.06710 | Regression loss: 0.10390 | Running loss: 0.79064 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2163 | Classification loss: 0.21599 | Regression loss: 0.31441 | Running loss: 0.79113 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2164 | Classification loss: 0.08860 | Regression loss: 0.28204 | Running loss: 0.78819 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2165 | Classification loss: 0.00935 | Regression loss: 0.10529 | Running loss: 0.78567 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2166 | Classification loss: 0.34836 | Regression loss: 0.28196 | Running loss: 0.78669 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2167 | Classification loss: 0.50689 | Regression loss: 0.59141 | Running loss: 0.78866 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2168 | Classification loss: 1.52169 | Regression loss: 0.41806 | Running loss: 0.78887 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2169 | Classification loss: 0.13461 | Regression loss: 0.25230 | Running loss: 0.78852 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2170 | Classification loss: 1.46385 | Regression loss: 0.65606 | Running loss: 0.79246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2171 | Classification loss: 0.18004 | Regression loss: 0.00000 | Running loss: 0.79223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2172 | Classification loss: 0.01738 | Regression loss: 0.12279 | Running loss: 0.78900 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2173 | Classification loss: 0.74021 | Regression loss: 0.41784 | Running loss: 0.79053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2174 | Classification loss: 0.43817 | Regression loss: 0.42829 | Running loss: 0.79125 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2175 | Classification loss: 0.05845 | Regression loss: 0.27048 | Running loss: 0.79065 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2176 | Classification loss: 1.54251 | Regression loss: 0.87649 | Running loss: 0.79399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2177 | Classification loss: 1.42313 | Regression loss: 0.48792 | Running loss: 0.79735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2178 | Classification loss: 0.88158 | Regression loss: 0.69964 | Running loss: 0.80029 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2179 | Classification loss: 0.00023 | Regression loss: 0.02072 | Running loss: 0.80012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2180 | Classification loss: 0.03042 | Regression loss: 0.13771 | Running loss: 0.79906 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2181 | Classification loss: 0.00020 | Regression loss: 0.07924 | Running loss: 0.79566 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2182 | Classification loss: 1.18831 | Regression loss: 0.60548 | Running loss: 0.79762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2183 | Classification loss: 0.59041 | Regression loss: 0.41651 | Running loss: 0.79771 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2184 | Classification loss: 0.01040 | Regression loss: 0.03724 | Running loss: 0.79641 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2185 | Classification loss: 0.63615 | Regression loss: 0.59934 | Running loss: 0.79748 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2186 | Classification loss: 0.79860 | Regression loss: 0.45748 | Running loss: 0.79686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2187 | Classification loss: 0.10744 | Regression loss: 0.13886 | Running loss: 0.79516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2188 | Classification loss: 0.37981 | Regression loss: 0.60306 | Running loss: 0.79548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2189 | Classification loss: 0.10215 | Regression loss: 0.40775 | Running loss: 0.79516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2190 | Classification loss: 0.00072 | Regression loss: 0.08661 | Running loss: 0.79490 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2191 | Classification loss: 0.25884 | Regression loss: 0.64006 | Running loss: 0.79430 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2192 | Classification loss: 0.00104 | Regression loss: 0.12766 | Running loss: 0.79387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2193 | Classification loss: 0.13727 | Regression loss: 0.23866 | Running loss: 0.79402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2194 | Classification loss: 0.15169 | Regression loss: 0.25724 | Running loss: 0.79265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2195 | Classification loss: 0.27426 | Regression loss: 0.21981 | Running loss: 0.79269 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2196 | Classification loss: 0.09010 | Regression loss: 0.17950 | Running loss: 0.79229 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2197 | Classification loss: 0.00159 | Regression loss: 0.11777 | Running loss: 0.78896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2198 | Classification loss: 0.46855 | Regression loss: 0.38901 | Running loss: 0.78729 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2199 | Classification loss: 0.01121 | Regression loss: 0.25434 | Running loss: 0.78731 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2200 | Classification loss: 0.83216 | Regression loss: 0.38499 | Running loss: 0.78826 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2201 | Classification loss: 0.63222 | Regression loss: 0.41976 | Running loss: 0.78890 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2202 | Classification loss: 0.83427 | Regression loss: 0.50501 | Running loss: 0.79067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2203 | Classification loss: 0.54099 | Regression loss: 0.42133 | Running loss: 0.79224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2204 | Classification loss: 1.47667 | Regression loss: 0.60255 | Running loss: 0.79622 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2205 | Classification loss: 0.08057 | Regression loss: 0.43473 | Running loss: 0.79606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2206 | Classification loss: 1.77653 | Regression loss: 0.53684 | Running loss: 0.79950 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2207 | Classification loss: 1.06037 | Regression loss: 0.22704 | Running loss: 0.79939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2208 | Classification loss: 0.34358 | Regression loss: 0.13702 | Running loss: 0.79866 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2209 | Classification loss: 0.15070 | Regression loss: 0.31254 | Running loss: 0.79904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2210 | Classification loss: 0.13625 | Regression loss: 0.15492 | Running loss: 0.79940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2211 | Classification loss: 0.03611 | Regression loss: 0.20533 | Running loss: 0.79860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2212 | Classification loss: 0.28471 | Regression loss: 0.29771 | Running loss: 0.79886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2213 | Classification loss: 0.04416 | Regression loss: 0.15380 | Running loss: 0.79673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2214 | Classification loss: 0.81496 | Regression loss: 0.38002 | Running loss: 0.79803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2215 | Classification loss: 0.73882 | Regression loss: 0.72440 | Running loss: 0.79986 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2216 | Classification loss: 0.78855 | Regression loss: 0.42197 | Running loss: 0.80201 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2217 | Classification loss: 0.30181 | Regression loss: 0.38894 | Running loss: 0.80098 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2218 | Classification loss: 0.45669 | Regression loss: 0.36052 | Running loss: 0.79406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2219 | Classification loss: 0.13025 | Regression loss: 0.09207 | Running loss: 0.79439 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2220 | Classification loss: 0.71189 | Regression loss: 0.72492 | Running loss: 0.79563 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2221 | Classification loss: 0.02421 | Regression loss: 0.08679 | Running loss: 0.79392 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2222 | Classification loss: 1.00440 | Regression loss: 0.45359 | Running loss: 0.79357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2223 | Classification loss: 0.66159 | Regression loss: 0.47942 | Running loss: 0.79432 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2224 | Classification loss: 0.60003 | Regression loss: 0.08132 | Running loss: 0.79340 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2225 | Classification loss: 0.77695 | Regression loss: 0.30806 | Running loss: 0.79421 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2226 | Classification loss: 0.15329 | Regression loss: 0.44439 | Running loss: 0.79495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2227 | Classification loss: 0.23362 | Regression loss: 0.31751 | Running loss: 0.79552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2228 | Classification loss: 1.20106 | Regression loss: 0.74531 | Running loss: 0.79690 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2229 | Classification loss: 1.80515 | Regression loss: 0.85789 | Running loss: 0.79987 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2230 | Classification loss: 0.75033 | Regression loss: 0.50511 | Running loss: 0.80124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2231 | Classification loss: 0.29584 | Regression loss: 0.29151 | Running loss: 0.80189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2232 | Classification loss: 0.23736 | Regression loss: 0.23673 | Running loss: 0.80151 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2233 | Classification loss: 0.46497 | Regression loss: 0.41435 | Running loss: 0.80299 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2234 | Classification loss: 0.24317 | Regression loss: 0.41750 | Running loss: 0.80280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2235 | Classification loss: 0.00058 | Regression loss: 0.08963 | Running loss: 0.80103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2236 | Classification loss: 0.84292 | Regression loss: 0.58832 | Running loss: 0.80340 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2237 | Classification loss: 0.00248 | Regression loss: 0.12184 | Running loss: 0.80306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2238 | Classification loss: 1.32441 | Regression loss: 0.40827 | Running loss: 0.80328 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2239 | Classification loss: 0.12943 | Regression loss: 0.25668 | Running loss: 0.79998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2240 | Classification loss: 0.49234 | Regression loss: 0.19278 | Running loss: 0.79826 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2241 | Classification loss: 0.60765 | Regression loss: 0.78218 | Running loss: 0.80009 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2242 | Classification loss: 0.00037 | Regression loss: 0.14473 | Running loss: 0.80019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2243 | Classification loss: 1.41061 | Regression loss: 0.66495 | Running loss: 0.80251 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2244 | Classification loss: 0.00590 | Regression loss: 0.11615 | Running loss: 0.80020 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2245 | Classification loss: 0.46382 | Regression loss: 0.35533 | Running loss: 0.80090 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2246 | Classification loss: 0.10532 | Regression loss: 0.25839 | Running loss: 0.79610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2247 | Classification loss: 0.14261 | Regression loss: 0.30738 | Running loss: 0.79054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2248 | Classification loss: 0.03836 | Regression loss: 0.12361 | Running loss: 0.78926 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2249 | Classification loss: 0.83515 | Regression loss: 0.45242 | Running loss: 0.79071 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2250 | Classification loss: 1.59150 | Regression loss: 0.69880 | Running loss: 0.79271 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2251 | Classification loss: 0.03361 | Regression loss: 0.08785 | Running loss: 0.79057 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2252 | Classification loss: 0.08713 | Regression loss: 0.33949 | Running loss: 0.79114 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2253 | Classification loss: 0.86276 | Regression loss: 0.36218 | Running loss: 0.78888 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2254 | Classification loss: 0.01523 | Regression loss: 0.25229 | Running loss: 0.78813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2255 | Classification loss: 0.00085 | Regression loss: 0.07254 | Running loss: 0.78325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2256 | Classification loss: 0.52939 | Regression loss: 0.37679 | Running loss: 0.78395 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2257 | Classification loss: 0.00014 | Regression loss: 0.09372 | Running loss: 0.78314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2258 | Classification loss: 1.00233 | Regression loss: 0.20548 | Running loss: 0.78403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2259 | Classification loss: 0.00324 | Regression loss: 0.07542 | Running loss: 0.78288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2260 | Classification loss: 1.03919 | Regression loss: 0.36474 | Running loss: 0.78501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2261 | Classification loss: 0.38006 | Regression loss: 0.53052 | Running loss: 0.78492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2262 | Classification loss: 0.00116 | Regression loss: 0.06333 | Running loss: 0.78386 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2263 | Classification loss: 0.00168 | Regression loss: 0.14074 | Running loss: 0.78181 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2264 | Classification loss: 0.02095 | Regression loss: 0.13661 | Running loss: 0.78037 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2265 | Classification loss: 0.58689 | Regression loss: 0.66726 | Running loss: 0.78112 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2266 | Classification loss: 0.10027 | Regression loss: 0.12373 | Running loss: 0.78053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2267 | Classification loss: 0.54874 | Regression loss: 0.46071 | Running loss: 0.78052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2268 | Classification loss: 0.01694 | Regression loss: 0.19708 | Running loss: 0.78075 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2269 | Classification loss: 1.29764 | Regression loss: 0.51398 | Running loss: 0.78298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2270 | Classification loss: 1.00533 | Regression loss: 0.45627 | Running loss: 0.78558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2271 | Classification loss: 0.01596 | Regression loss: 0.09694 | Running loss: 0.78373 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2272 | Classification loss: 1.17042 | Regression loss: 0.61251 | Running loss: 0.78612 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2273 | Classification loss: 4.11992 | Regression loss: 0.55594 | Running loss: 0.79396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2274 | Classification loss: 0.07073 | Regression loss: 0.38227 | Running loss: 0.79429 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2275 | Classification loss: 0.00125 | Regression loss: 0.08684 | Running loss: 0.78867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2276 | Classification loss: 0.45528 | Regression loss: 0.62172 | Running loss: 0.79068 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2277 | Classification loss: 0.11047 | Regression loss: 0.30180 | Running loss: 0.79103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2278 | Classification loss: 0.00075 | Regression loss: 0.12283 | Running loss: 0.79093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2279 | Classification loss: 1.43127 | Regression loss: 0.45921 | Running loss: 0.79428 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2280 | Classification loss: 0.00042 | Regression loss: 0.04027 | Running loss: 0.79346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2281 | Classification loss: 0.01950 | Regression loss: 0.07792 | Running loss: 0.79184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2282 | Classification loss: 0.59565 | Regression loss: 0.54032 | Running loss: 0.79182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2283 | Classification loss: 0.02198 | Regression loss: 0.03351 | Running loss: 0.78780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2284 | Classification loss: 0.12083 | Regression loss: 0.25811 | Running loss: 0.78604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2285 | Classification loss: 0.76630 | Regression loss: 0.60186 | Running loss: 0.78868 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2286 | Classification loss: 0.10707 | Regression loss: 0.19233 | Running loss: 0.78500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2287 | Classification loss: 0.04975 | Regression loss: 0.21121 | Running loss: 0.78510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2288 | Classification loss: 0.00045 | Regression loss: 0.17322 | Running loss: 0.78494 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2289 | Classification loss: 0.63729 | Regression loss: 0.53858 | Running loss: 0.78503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2290 | Classification loss: 0.12777 | Regression loss: 0.09333 | Running loss: 0.78430 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2291 | Classification loss: 2.62630 | Regression loss: 0.69348 | Running loss: 0.78780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2292 | Classification loss: 0.00132 | Regression loss: 0.03459 | Running loss: 0.78762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2293 | Classification loss: 0.05845 | Regression loss: 0.04727 | Running loss: 0.78771 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2294 | Classification loss: 0.04933 | Regression loss: 0.21760 | Running loss: 0.78674 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2295 | Classification loss: 0.38578 | Regression loss: 0.43760 | Running loss: 0.78812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2296 | Classification loss: 0.34240 | Regression loss: 0.28136 | Running loss: 0.78802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2297 | Classification loss: 0.01314 | Regression loss: 0.35953 | Running loss: 0.78651 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2298 | Classification loss: 0.87754 | Regression loss: 0.54583 | Running loss: 0.78760 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2299 | Classification loss: 0.23480 | Regression loss: 0.44675 | Running loss: 0.78582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2300 | Classification loss: 0.07015 | Regression loss: 0.30745 | Running loss: 0.78416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2301 | Classification loss: 2.06724 | Regression loss: 0.77890 | Running loss: 0.78964 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2302 | Classification loss: 0.29217 | Regression loss: 0.24697 | Running loss: 0.78846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2303 | Classification loss: 1.05349 | Regression loss: 0.80612 | Running loss: 0.79027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2304 | Classification loss: 0.61697 | Regression loss: 0.53636 | Running loss: 0.79202 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2305 | Classification loss: 0.56489 | Regression loss: 0.28500 | Running loss: 0.79333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2306 | Classification loss: 1.33481 | Regression loss: 0.50588 | Running loss: 0.79652 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2307 | Classification loss: 0.52888 | Regression loss: 0.52852 | Running loss: 0.79834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2308 | Classification loss: 0.03746 | Regression loss: 0.13205 | Running loss: 0.79460 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2309 | Classification loss: 1.90055 | Regression loss: 0.68065 | Running loss: 0.79734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2310 | Classification loss: 0.41973 | Regression loss: 0.42748 | Running loss: 0.79676 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2311 | Classification loss: 0.00263 | Regression loss: 0.07875 | Running loss: 0.79684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2312 | Classification loss: 1.64567 | Regression loss: 0.51889 | Running loss: 0.79970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2313 | Classification loss: 0.12548 | Regression loss: 0.49594 | Running loss: 0.80025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2314 | Classification loss: 0.11071 | Regression loss: 0.03515 | Running loss: 0.79930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2315 | Classification loss: 0.39588 | Regression loss: 0.51466 | Running loss: 0.80039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2316 | Classification loss: 0.46452 | Regression loss: 0.36152 | Running loss: 0.79917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2317 | Classification loss: 0.45736 | Regression loss: 0.37319 | Running loss: 0.79798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2318 | Classification loss: 0.00635 | Regression loss: 0.18533 | Running loss: 0.79654 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2319 | Classification loss: 0.04288 | Regression loss: 0.19584 | Running loss: 0.79557 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2320 | Classification loss: 0.23703 | Regression loss: 0.53565 | Running loss: 0.79689 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2321 | Classification loss: 0.31771 | Regression loss: 0.36519 | Running loss: 0.79781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2322 | Classification loss: 1.09788 | Regression loss: 0.51307 | Running loss: 0.79784 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2323 | Classification loss: 0.00012 | Regression loss: 0.05884 | Running loss: 0.79561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2324 | Classification loss: 0.46687 | Regression loss: 0.39385 | Running loss: 0.79598 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2325 | Classification loss: 0.01521 | Regression loss: 0.13308 | Running loss: 0.79504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2326 | Classification loss: 0.11171 | Regression loss: 0.15592 | Running loss: 0.79397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2327 | Classification loss: 0.13037 | Regression loss: 0.30431 | Running loss: 0.79454 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2328 | Classification loss: 0.41083 | Regression loss: 0.56300 | Running loss: 0.79615 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2329 | Classification loss: 0.01489 | Regression loss: 0.14777 | Running loss: 0.79627 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2330 | Classification loss: 0.66314 | Regression loss: 0.47797 | Running loss: 0.79656 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2331 | Classification loss: 0.81022 | Regression loss: 0.64969 | Running loss: 0.79750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2332 | Classification loss: 0.70200 | Regression loss: 0.53659 | Running loss: 0.79868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2333 | Classification loss: 0.31951 | Regression loss: 0.45345 | Running loss: 0.79833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2334 | Classification loss: 1.40815 | Regression loss: 0.54092 | Running loss: 0.80197 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2335 | Classification loss: 0.20293 | Regression loss: 0.42072 | Running loss: 0.80165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2336 | Classification loss: 0.20797 | Regression loss: 0.25288 | Running loss: 0.80193 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2337 | Classification loss: 0.06811 | Regression loss: 0.13617 | Running loss: 0.79831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2338 | Classification loss: 0.00657 | Regression loss: 0.11155 | Running loss: 0.79840 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2339 | Classification loss: 1.79293 | Regression loss: 0.37701 | Running loss: 0.80082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2340 | Classification loss: 0.42295 | Regression loss: 0.26811 | Running loss: 0.80017 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2341 | Classification loss: 0.00567 | Regression loss: 0.04785 | Running loss: 0.79903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2342 | Classification loss: 0.00355 | Regression loss: 0.15283 | Running loss: 0.79524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2343 | Classification loss: 1.44747 | Regression loss: 0.49331 | Running loss: 0.79687 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2344 | Classification loss: 0.05129 | Regression loss: 0.18481 | Running loss: 0.79531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2345 | Classification loss: 0.17708 | Regression loss: 0.28906 | Running loss: 0.79379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2346 | Classification loss: 0.01374 | Regression loss: 0.09766 | Running loss: 0.79335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2347 | Classification loss: 1.19590 | Regression loss: 0.49156 | Running loss: 0.79481 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2348 | Classification loss: 0.06013 | Regression loss: 0.27251 | Running loss: 0.79433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2349 | Classification loss: 1.23696 | Regression loss: 0.61614 | Running loss: 0.79627 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2350 | Classification loss: 0.45516 | Regression loss: 0.24824 | Running loss: 0.79683 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2351 | Classification loss: 0.03405 | Regression loss: 0.26462 | Running loss: 0.79678 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2352 | Classification loss: 0.21930 | Regression loss: 0.30090 | Running loss: 0.79718 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2353 | Classification loss: 0.03903 | Regression loss: 0.17824 | Running loss: 0.79658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2354 | Classification loss: 0.00071 | Regression loss: 0.12051 | Running loss: 0.79555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2355 | Classification loss: 1.54819 | Regression loss: 0.76568 | Running loss: 0.79972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2356 | Classification loss: 0.19138 | Regression loss: 0.24240 | Running loss: 0.80022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2357 | Classification loss: 2.06326 | Regression loss: 0.82555 | Running loss: 0.80553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2358 | Classification loss: 0.82948 | Regression loss: 0.66136 | Running loss: 0.80828 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2359 | Classification loss: 0.10387 | Regression loss: 0.27801 | Running loss: 0.80803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2360 | Classification loss: 0.05238 | Regression loss: 0.17950 | Running loss: 0.80605 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2361 | Classification loss: 0.81003 | Regression loss: 0.23932 | Running loss: 0.80559 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2362 | Classification loss: 0.07226 | Regression loss: 0.18923 | Running loss: 0.80544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2363 | Classification loss: 2.16634 | Regression loss: 0.50864 | Running loss: 0.81047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2364 | Classification loss: 0.05782 | Regression loss: 0.19072 | Running loss: 0.81029 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2365 | Classification loss: 0.05044 | Regression loss: 0.13268 | Running loss: 0.81015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2366 | Classification loss: 0.24328 | Regression loss: 0.33788 | Running loss: 0.80658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2367 | Classification loss: 0.11032 | Regression loss: 0.27748 | Running loss: 0.80711 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2368 | Classification loss: 0.02208 | Regression loss: 0.18493 | Running loss: 0.80701 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2369 | Classification loss: 0.11250 | Regression loss: 0.16852 | Running loss: 0.80718 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2370 | Classification loss: 0.05078 | Regression loss: 0.10562 | Running loss: 0.80683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2371 | Classification loss: 0.87979 | Regression loss: 0.48304 | Running loss: 0.80885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2372 | Classification loss: 0.42438 | Regression loss: 0.22764 | Running loss: 0.80800 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2373 | Classification loss: 0.14547 | Regression loss: 0.19432 | Running loss: 0.80682 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2374 | Classification loss: 0.66967 | Regression loss: 0.52740 | Running loss: 0.80883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2375 | Classification loss: 0.09193 | Regression loss: 0.07405 | Running loss: 0.80620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2376 | Classification loss: 0.36845 | Regression loss: 0.49200 | Running loss: 0.80614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2377 | Classification loss: 0.04600 | Regression loss: 0.30282 | Running loss: 0.80578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2378 | Classification loss: 0.22411 | Regression loss: 0.44123 | Running loss: 0.80389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2379 | Classification loss: 0.02727 | Regression loss: 0.15438 | Running loss: 0.80071 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2380 | Classification loss: 0.44571 | Regression loss: 0.28836 | Running loss: 0.80011 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2381 | Classification loss: 0.00763 | Regression loss: 0.30812 | Running loss: 0.79983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2382 | Classification loss: 0.01278 | Regression loss: 0.22689 | Running loss: 0.79855 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2383 | Classification loss: 0.00871 | Regression loss: 0.17556 | Running loss: 0.79295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2384 | Classification loss: 0.63416 | Regression loss: 0.59428 | Running loss: 0.79074 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2385 | Classification loss: 0.02557 | Regression loss: 0.21908 | Running loss: 0.79117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2386 | Classification loss: 0.64426 | Regression loss: 0.33139 | Running loss: 0.79223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2387 | Classification loss: 0.10349 | Regression loss: 0.20848 | Running loss: 0.79192 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2388 | Classification loss: 0.15990 | Regression loss: 0.56815 | Running loss: 0.79041 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2389 | Classification loss: 0.10470 | Regression loss: 0.24447 | Running loss: 0.78908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2390 | Classification loss: 0.19425 | Regression loss: 0.39828 | Running loss: 0.79012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2391 | Classification loss: 0.74130 | Regression loss: 0.46964 | Running loss: 0.79143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2392 | Classification loss: 1.36859 | Regression loss: 0.69541 | Running loss: 0.79314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2393 | Classification loss: 0.22775 | Regression loss: 0.29942 | Running loss: 0.79286 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2394 | Classification loss: 0.60770 | Regression loss: 0.40023 | Running loss: 0.79261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2395 | Classification loss: 0.45349 | Regression loss: 0.50028 | Running loss: 0.79439 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2396 | Classification loss: 0.73459 | Regression loss: 0.41488 | Running loss: 0.79447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2397 | Classification loss: 0.02179 | Regression loss: 0.13003 | Running loss: 0.79428 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2398 | Classification loss: 1.62182 | Regression loss: 0.64663 | Running loss: 0.79817 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2399 | Classification loss: 0.38380 | Regression loss: 0.39356 | Running loss: 0.79345 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2400 | Classification loss: 0.32757 | Regression loss: 0.14134 | Running loss: 0.79372 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2401 | Classification loss: 0.35213 | Regression loss: 0.49723 | Running loss: 0.79270 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2402 | Classification loss: 0.09623 | Regression loss: 0.08193 | Running loss: 0.79194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2403 | Classification loss: 0.55644 | Regression loss: 0.34983 | Running loss: 0.79268 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2404 | Classification loss: 0.02904 | Regression loss: 0.21509 | Running loss: 0.79132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2405 | Classification loss: 0.93297 | Regression loss: 0.60322 | Running loss: 0.79132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2406 | Classification loss: 0.09754 | Regression loss: 0.24221 | Running loss: 0.79087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2407 | Classification loss: 0.08195 | Regression loss: 0.39650 | Running loss: 0.79162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2408 | Classification loss: 0.71447 | Regression loss: 0.42547 | Running loss: 0.79346 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2409 | Classification loss: 0.64724 | Regression loss: 0.71526 | Running loss: 0.79388 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2410 | Classification loss: 0.07639 | Regression loss: 0.16072 | Running loss: 0.79341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2411 | Classification loss: 1.55263 | Regression loss: 0.39532 | Running loss: 0.79426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2412 | Classification loss: 0.20685 | Regression loss: 0.67591 | Running loss: 0.79422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2413 | Classification loss: 0.84720 | Regression loss: 0.30680 | Running loss: 0.79430 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2414 | Classification loss: 0.45034 | Regression loss: 0.26468 | Running loss: 0.79520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2415 | Classification loss: 0.33961 | Regression loss: 0.38068 | Running loss: 0.79639 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2416 | Classification loss: 0.00507 | Regression loss: 0.02987 | Running loss: 0.79600 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2417 | Classification loss: 0.60652 | Regression loss: 0.42841 | Running loss: 0.79723 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2418 | Classification loss: 0.41851 | Regression loss: 0.47334 | Running loss: 0.79667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2419 | Classification loss: 0.42396 | Regression loss: 0.32807 | Running loss: 0.79655 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2420 | Classification loss: 0.11080 | Regression loss: 0.35606 | Running loss: 0.79677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2421 | Classification loss: 0.17643 | Regression loss: 0.35635 | Running loss: 0.79445 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2422 | Classification loss: 0.24972 | Regression loss: 0.34795 | Running loss: 0.79505 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2423 | Classification loss: 0.65319 | Regression loss: 0.58308 | Running loss: 0.79685 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2424 | Classification loss: 0.03808 | Regression loss: 0.10450 | Running loss: 0.79570 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2425 | Classification loss: 0.06593 | Regression loss: 0.12894 | Running loss: 0.79381 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2426 | Classification loss: 0.23363 | Regression loss: 0.57425 | Running loss: 0.79524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2427 | Classification loss: 0.01058 | Regression loss: 0.10491 | Running loss: 0.79408 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2428 | Classification loss: 0.05687 | Regression loss: 0.04524 | Running loss: 0.79383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2429 | Classification loss: 1.12730 | Regression loss: 0.62545 | Running loss: 0.79654 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2430 | Classification loss: 0.72815 | Regression loss: 0.57239 | Running loss: 0.79752 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2431 | Classification loss: 1.91178 | Regression loss: 0.51765 | Running loss: 0.80176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2432 | Classification loss: 0.82477 | Regression loss: 0.50489 | Running loss: 0.79483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2433 | Classification loss: 0.89674 | Regression loss: 0.72894 | Running loss: 0.79709 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2434 | Classification loss: 0.98573 | Regression loss: 0.58844 | Running loss: 0.80005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2435 | Classification loss: 0.00094 | Regression loss: 0.15137 | Running loss: 0.79961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2436 | Classification loss: 0.71029 | Regression loss: 0.54215 | Running loss: 0.80176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2437 | Classification loss: 0.03083 | Regression loss: 0.09847 | Running loss: 0.79973 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2438 | Classification loss: 0.11316 | Regression loss: 0.41111 | Running loss: 0.80043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2439 | Classification loss: 0.46813 | Regression loss: 0.58165 | Running loss: 0.80121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2440 | Classification loss: 0.62118 | Regression loss: 0.55318 | Running loss: 0.80234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2441 | Classification loss: 0.30032 | Regression loss: 0.17867 | Running loss: 0.80103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2442 | Classification loss: 0.42168 | Regression loss: 0.52513 | Running loss: 0.80248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2443 | Classification loss: 1.31786 | Regression loss: 0.53664 | Running loss: 0.80589 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2444 | Classification loss: 0.74607 | Regression loss: 0.52492 | Running loss: 0.80748 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2445 | Classification loss: 0.24638 | Regression loss: 0.16081 | Running loss: 0.80758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2446 | Classification loss: 1.08768 | Regression loss: 0.36899 | Running loss: 0.80912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2447 | Classification loss: 0.16931 | Regression loss: 0.46907 | Running loss: 0.80984 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2448 | Classification loss: 0.06613 | Regression loss: 0.15849 | Running loss: 0.80912 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2449 | Classification loss: 0.04859 | Regression loss: 0.06293 | Running loss: 0.80292 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2450 | Classification loss: 1.95807 | Regression loss: 0.26446 | Running loss: 0.80624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2451 | Classification loss: 0.00857 | Regression loss: 0.17235 | Running loss: 0.80384 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2452 | Classification loss: 0.30286 | Regression loss: 0.55602 | Running loss: 0.80377 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2453 | Classification loss: 0.00362 | Regression loss: 0.11288 | Running loss: 0.80384 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2454 | Classification loss: 0.62199 | Regression loss: 0.44859 | Running loss: 0.80575 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2455 | Classification loss: 1.17215 | Regression loss: 0.41622 | Running loss: 0.80820 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2456 | Classification loss: 1.06614 | Regression loss: 0.73691 | Running loss: 0.80878 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2457 | Classification loss: 0.65227 | Regression loss: 0.49074 | Running loss: 0.80980 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2458 | Classification loss: 0.29928 | Regression loss: 0.46680 | Running loss: 0.81047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2459 | Classification loss: 0.13570 | Regression loss: 0.18046 | Running loss: 0.80915 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2460 | Classification loss: 0.04136 | Regression loss: 0.16469 | Running loss: 0.80909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2461 | Classification loss: 0.09862 | Regression loss: 0.22049 | Running loss: 0.80710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2462 | Classification loss: 0.55062 | Regression loss: 0.40249 | Running loss: 0.80513 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2463 | Classification loss: 0.00124 | Regression loss: 0.06018 | Running loss: 0.80501 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2464 | Classification loss: 0.00545 | Regression loss: 0.09729 | Running loss: 0.80118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2465 | Classification loss: 0.00661 | Regression loss: 0.22944 | Running loss: 0.79824 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2466 | Classification loss: 0.12155 | Regression loss: 0.25572 | Running loss: 0.79715 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2467 | Classification loss: 0.96315 | Regression loss: 0.45834 | Running loss: 0.79917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2468 | Classification loss: 0.08628 | Regression loss: 0.21155 | Running loss: 0.79926 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2469 | Classification loss: 0.01683 | Regression loss: 0.19570 | Running loss: 0.79898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2470 | Classification loss: 0.36421 | Regression loss: 0.45312 | Running loss: 0.80013 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2471 | Classification loss: 1.88594 | Regression loss: 0.78301 | Running loss: 0.80424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2472 | Classification loss: 0.02419 | Regression loss: 0.20238 | Running loss: 0.80317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2473 | Classification loss: 0.15241 | Regression loss: 0.22788 | Running loss: 0.80260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2474 | Classification loss: 0.48370 | Regression loss: 0.31482 | Running loss: 0.80341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2475 | Classification loss: 0.01092 | Regression loss: 0.12447 | Running loss: 0.80353 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2476 | Classification loss: 0.08807 | Regression loss: 0.23626 | Running loss: 0.80227 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2477 | Classification loss: 0.63238 | Regression loss: 0.48765 | Running loss: 0.80369 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2478 | Classification loss: 0.03825 | Regression loss: 0.21319 | Running loss: 0.80406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2479 | Classification loss: 1.60844 | Regression loss: 0.65430 | Running loss: 0.80763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2480 | Classification loss: 0.24779 | Regression loss: 0.63435 | Running loss: 0.80832 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2481 | Classification loss: 0.00644 | Regression loss: 0.13509 | Running loss: 0.80794 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2482 | Classification loss: 0.12012 | Regression loss: 0.21201 | Running loss: 0.80815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2483 | Classification loss: 0.39966 | Regression loss: 0.18156 | Running loss: 0.80894 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2484 | Classification loss: 1.13418 | Regression loss: 0.72198 | Running loss: 0.81262 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2485 | Classification loss: 0.18700 | Regression loss: 0.11305 | Running loss: 0.81276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2486 | Classification loss: 0.46705 | Regression loss: 0.48083 | Running loss: 0.81385 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2487 | Classification loss: 0.22664 | Regression loss: 0.53465 | Running loss: 0.81211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2488 | Classification loss: 0.62964 | Regression loss: 0.30876 | Running loss: 0.80616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2489 | Classification loss: 0.04498 | Regression loss: 0.21421 | Running loss: 0.80442 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2490 | Classification loss: 0.15612 | Regression loss: 0.29003 | Running loss: 0.80236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2491 | Classification loss: 1.24682 | Regression loss: 0.27785 | Running loss: 0.80375 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2492 | Classification loss: 0.67955 | Regression loss: 0.53246 | Running loss: 0.80263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2493 | Classification loss: 0.74804 | Regression loss: 0.32626 | Running loss: 0.80396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2494 | Classification loss: 0.41873 | Regression loss: 0.46811 | Running loss: 0.80115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2495 | Classification loss: 1.23744 | Regression loss: 0.47435 | Running loss: 0.80366 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2496 | Classification loss: 0.10120 | Regression loss: 0.14810 | Running loss: 0.80333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2497 | Classification loss: 0.27934 | Regression loss: 0.27993 | Running loss: 0.80219 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2498 | Classification loss: 0.06560 | Regression loss: 0.15955 | Running loss: 0.80176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2499 | Classification loss: 1.82783 | Regression loss: 0.69595 | Running loss: 0.80512 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2500 | Classification loss: 0.34972 | Regression loss: 0.14497 | Running loss: 0.80330 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2501 | Classification loss: 0.66230 | Regression loss: 0.27277 | Running loss: 0.80447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2502 | Classification loss: 0.18515 | Regression loss: 0.37667 | Running loss: 0.80471 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2503 | Classification loss: 0.17136 | Regression loss: 0.14212 | Running loss: 0.80498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2504 | Classification loss: 0.13245 | Regression loss: 0.32326 | Running loss: 0.80564 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2505 | Classification loss: 0.07389 | Regression loss: 0.16948 | Running loss: 0.80482 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2506 | Classification loss: 0.45630 | Regression loss: 0.26581 | Running loss: 0.80505 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2507 | Classification loss: 0.03973 | Regression loss: 0.26859 | Running loss: 0.80429 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2508 | Classification loss: 0.05314 | Regression loss: 0.13555 | Running loss: 0.80285 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2509 | Classification loss: 0.54807 | Regression loss: 0.30240 | Running loss: 0.80257 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2510 | Classification loss: 0.83136 | Regression loss: 0.36404 | Running loss: 0.80332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2511 | Classification loss: 0.02226 | Regression loss: 0.17718 | Running loss: 0.80233 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2512 | Classification loss: 0.02199 | Regression loss: 0.13186 | Running loss: 0.80024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2513 | Classification loss: 0.13857 | Regression loss: 0.25135 | Running loss: 0.80027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2514 | Classification loss: 1.20599 | Regression loss: 0.65385 | Running loss: 0.80307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2515 | Classification loss: 1.35423 | Regression loss: 0.56744 | Running loss: 0.80416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2516 | Classification loss: 0.03426 | Regression loss: 0.27274 | Running loss: 0.80262 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2517 | Classification loss: 0.77963 | Regression loss: 0.55141 | Running loss: 0.80387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2518 | Classification loss: 0.83245 | Regression loss: 0.54403 | Running loss: 0.80431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2519 | Classification loss: 0.18932 | Regression loss: 0.69970 | Running loss: 0.80385 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2520 | Classification loss: 1.29122 | Regression loss: 0.64633 | Running loss: 0.80601 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2521 | Classification loss: 0.34825 | Regression loss: 0.12595 | Running loss: 0.80294 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2522 | Classification loss: 0.17049 | Regression loss: 0.23960 | Running loss: 0.80255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2523 | Classification loss: 0.00004 | Regression loss: 0.11731 | Running loss: 0.80156 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2524 | Classification loss: 0.01898 | Regression loss: 0.19899 | Running loss: 0.80166 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2525 | Classification loss: 0.16206 | Regression loss: 0.23441 | Running loss: 0.80207 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2526 | Classification loss: 0.70855 | Regression loss: 0.51757 | Running loss: 0.80389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2527 | Classification loss: 0.04107 | Regression loss: 0.09435 | Running loss: 0.80334 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2528 | Classification loss: 0.11555 | Regression loss: 0.26134 | Running loss: 0.80284 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2529 | Classification loss: 0.04352 | Regression loss: 0.17862 | Running loss: 0.80009 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2530 | Classification loss: 0.41523 | Regression loss: 0.42825 | Running loss: 0.80039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2531 | Classification loss: 1.36531 | Regression loss: 0.80421 | Running loss: 0.80337 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2532 | Classification loss: 0.48275 | Regression loss: 0.57112 | Running loss: 0.80502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2533 | Classification loss: 0.01071 | Regression loss: 0.11942 | Running loss: 0.80359 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2534 | Classification loss: 0.62866 | Regression loss: 0.71662 | Running loss: 0.80383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2535 | Classification loss: 0.02264 | Regression loss: 0.07139 | Running loss: 0.80104 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2536 | Classification loss: 2.20533 | Regression loss: 0.79686 | Running loss: 0.80283 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2537 | Classification loss: 0.59110 | Regression loss: 0.54644 | Running loss: 0.80397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2538 | Classification loss: 0.01041 | Regression loss: 0.07853 | Running loss: 0.80170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2539 | Classification loss: 1.59903 | Regression loss: 0.62599 | Running loss: 0.80256 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2540 | Classification loss: 0.00599 | Regression loss: 0.08972 | Running loss: 0.80223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2541 | Classification loss: 2.37877 | Regression loss: 0.75060 | Running loss: 0.80767 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 2542 | Classification loss: 1.49306 | Regression loss: 0.41764 | Running loss: 0.81033 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2543 | Classification loss: 0.04387 | Regression loss: 0.31896 | Running loss: 0.80990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2544 | Classification loss: 0.88650 | Regression loss: 0.48454 | Running loss: 0.80968 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2545 | Classification loss: 0.11402 | Regression loss: 0.30251 | Running loss: 0.81015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2546 | Classification loss: 0.00029 | Regression loss: 0.14223 | Running loss: 0.80762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2547 | Classification loss: 0.09358 | Regression loss: 0.05944 | Running loss: 0.80679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2548 | Classification loss: 0.01269 | Regression loss: 0.11373 | Running loss: 0.80500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2549 | Classification loss: 0.28319 | Regression loss: 0.42210 | Running loss: 0.80637 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2550 | Classification loss: 0.05384 | Regression loss: 0.13475 | Running loss: 0.80441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2551 | Classification loss: 0.00859 | Regression loss: 0.04631 | Running loss: 0.80272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2552 | Classification loss: 0.67999 | Regression loss: 0.39384 | Running loss: 0.80433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2553 | Classification loss: 0.19312 | Regression loss: 0.15298 | Running loss: 0.80433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2554 | Classification loss: 0.11543 | Regression loss: 0.20522 | Running loss: 0.80492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2555 | Classification loss: 0.00122 | Regression loss: 0.06960 | Running loss: 0.80317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2556 | Classification loss: 1.01054 | Regression loss: 0.69005 | Running loss: 0.80321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2557 | Classification loss: 0.00300 | Regression loss: 0.11109 | Running loss: 0.80282 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2558 | Classification loss: 0.03588 | Regression loss: 0.15268 | Running loss: 0.80300 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2559 | Classification loss: 0.21465 | Regression loss: 0.39713 | Running loss: 0.80376 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2560 | Classification loss: 0.16064 | Regression loss: 0.31334 | Running loss: 0.80343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2561 | Classification loss: 1.43725 | Regression loss: 0.11386 | Running loss: 0.80552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2562 | Classification loss: 0.31079 | Regression loss: 0.34163 | Running loss: 0.80632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2563 | Classification loss: 0.57277 | Regression loss: 0.42986 | Running loss: 0.80800 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2564 | Classification loss: 0.58957 | Regression loss: 0.59672 | Running loss: 0.80727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2565 | Classification loss: 0.70142 | Regression loss: 0.51326 | Running loss: 0.80393 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2566 | Classification loss: 0.18492 | Regression loss: 0.47031 | Running loss: 0.80448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2567 | Classification loss: 1.40561 | Regression loss: 0.42775 | Running loss: 0.80719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2568 | Classification loss: 0.91895 | Regression loss: 0.49302 | Running loss: 0.80771 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2569 | Classification loss: 0.50355 | Regression loss: 0.29624 | Running loss: 0.80893 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2570 | Classification loss: 0.00586 | Regression loss: 0.13787 | Running loss: 0.80835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2571 | Classification loss: 0.05326 | Regression loss: 0.34281 | Running loss: 0.80744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2572 | Classification loss: 0.71762 | Regression loss: 0.34968 | Running loss: 0.80855 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2573 | Classification loss: 0.36432 | Regression loss: 0.25297 | Running loss: 0.80635 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2574 | Classification loss: 0.05521 | Regression loss: 0.31261 | Running loss: 0.80610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2575 | Classification loss: 0.33484 | Regression loss: 0.31265 | Running loss: 0.80557 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2576 | Classification loss: 0.25268 | Regression loss: 0.20745 | Running loss: 0.80534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2577 | Classification loss: 0.83467 | Regression loss: 0.26250 | Running loss: 0.80736 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2578 | Classification loss: 1.27324 | Regression loss: 0.46199 | Running loss: 0.81070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2579 | Classification loss: 0.87147 | Regression loss: 0.39544 | Running loss: 0.81183 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2580 | Classification loss: 0.13483 | Regression loss: 0.21176 | Running loss: 0.81180 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2581 | Classification loss: 0.46515 | Regression loss: 0.20948 | Running loss: 0.81190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2582 | Classification loss: 0.77504 | Regression loss: 0.60309 | Running loss: 0.81184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2583 | Classification loss: 0.00108 | Regression loss: 0.08167 | Running loss: 0.81140 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2584 | Classification loss: 0.13247 | Regression loss: 0.28079 | Running loss: 0.81137 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2585 | Classification loss: 0.17968 | Regression loss: 0.37968 | Running loss: 0.81194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2586 | Classification loss: 1.72980 | Regression loss: 0.63435 | Running loss: 0.81615 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2587 | Classification loss: 0.61526 | Regression loss: 0.58906 | Running loss: 0.81764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2588 | Classification loss: 0.66316 | Regression loss: 0.52434 | Running loss: 0.81933 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2589 | Classification loss: 0.20728 | Regression loss: 0.29633 | Running loss: 0.81932 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2590 | Classification loss: 1.03338 | Regression loss: 0.35465 | Running loss: 0.82130 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2591 | Classification loss: 0.67233 | Regression loss: 0.56519 | Running loss: 0.82330 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2592 | Classification loss: 0.17373 | Regression loss: 0.37591 | Running loss: 0.82408 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2593 | Classification loss: 0.32188 | Regression loss: 0.59447 | Running loss: 0.82553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2594 | Classification loss: 0.16907 | Regression loss: 0.25348 | Running loss: 0.82138 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2595 | Classification loss: 0.00037 | Regression loss: 0.04566 | Running loss: 0.82089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2596 | Classification loss: 0.66425 | Regression loss: 0.39583 | Running loss: 0.82007 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2597 | Classification loss: 0.01949 | Regression loss: 0.15901 | Running loss: 0.81903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2598 | Classification loss: 0.02416 | Regression loss: 0.16199 | Running loss: 0.81917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2599 | Classification loss: 0.00109 | Regression loss: 0.11258 | Running loss: 0.81849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2600 | Classification loss: 0.00300 | Regression loss: 0.06990 | Running loss: 0.81756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2601 | Classification loss: 0.18691 | Regression loss: 0.21095 | Running loss: 0.81814 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2602 | Classification loss: 1.75194 | Regression loss: 0.77359 | Running loss: 0.82095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2603 | Classification loss: 0.29408 | Regression loss: 0.33419 | Running loss: 0.82047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2604 | Classification loss: 0.12785 | Regression loss: 0.24386 | Running loss: 0.82104 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2605 | Classification loss: 0.16694 | Regression loss: 0.34924 | Running loss: 0.82161 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2606 | Classification loss: 0.38288 | Regression loss: 0.18136 | Running loss: 0.81956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2607 | Classification loss: 1.13511 | Regression loss: 0.51129 | Running loss: 0.82264 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2608 | Classification loss: 2.13981 | Regression loss: 0.75393 | Running loss: 0.82763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2609 | Classification loss: 1.60196 | Regression loss: 0.70205 | Running loss: 0.83056 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2610 | Classification loss: 0.10742 | Regression loss: 0.28448 | Running loss: 0.83129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2611 | Classification loss: 0.64026 | Regression loss: 0.26583 | Running loss: 0.83053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2612 | Classification loss: 0.15520 | Regression loss: 0.21270 | Running loss: 0.82977 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2613 | Classification loss: 0.07854 | Regression loss: 0.29653 | Running loss: 0.83000 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2614 | Classification loss: 0.10884 | Regression loss: 0.18862 | Running loss: 0.82928 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2615 | Classification loss: 0.21191 | Regression loss: 0.36312 | Running loss: 0.82856 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2616 | Classification loss: 0.09454 | Regression loss: 0.26908 | Running loss: 0.82839 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2617 | Classification loss: 0.03815 | Regression loss: 0.15452 | Running loss: 0.82504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2618 | Classification loss: 0.01795 | Regression loss: 0.08820 | Running loss: 0.82414 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2619 | Classification loss: 0.68258 | Regression loss: 0.38749 | Running loss: 0.82044 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2620 | Classification loss: 1.17933 | Regression loss: 0.40252 | Running loss: 0.82131 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2621 | Classification loss: 0.11778 | Regression loss: 0.30661 | Running loss: 0.82152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2622 | Classification loss: 0.72216 | Regression loss: 0.49524 | Running loss: 0.82371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2623 | Classification loss: 0.03788 | Regression loss: 0.21811 | Running loss: 0.82184 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2624 | Classification loss: 0.37636 | Regression loss: 0.50276 | Running loss: 0.82057 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2625 | Classification loss: 0.16904 | Regression loss: 0.30293 | Running loss: 0.82024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2626 | Classification loss: 0.09766 | Regression loss: 0.04472 | Running loss: 0.81976 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2627 | Classification loss: 0.47572 | Regression loss: 0.53615 | Running loss: 0.81897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2628 | Classification loss: 0.32757 | Regression loss: 0.56807 | Running loss: 0.81875 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2629 | Classification loss: 0.05970 | Regression loss: 0.26552 | Running loss: 0.81827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2630 | Classification loss: 0.24386 | Regression loss: 0.21954 | Running loss: 0.81658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2631 | Classification loss: 0.35436 | Regression loss: 0.23580 | Running loss: 0.81464 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2632 | Classification loss: 0.13313 | Regression loss: 0.26232 | Running loss: 0.81350 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2633 | Classification loss: 1.01648 | Regression loss: 0.53915 | Running loss: 0.81626 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2634 | Classification loss: 0.17791 | Regression loss: 0.35353 | Running loss: 0.81702 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2635 | Classification loss: 0.32975 | Regression loss: 0.44341 | Running loss: 0.81678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2636 | Classification loss: 0.39364 | Regression loss: 0.09949 | Running loss: 0.81758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2637 | Classification loss: 0.35823 | Regression loss: 0.54444 | Running loss: 0.81757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2638 | Classification loss: 0.18087 | Regression loss: 0.33404 | Running loss: 0.81476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2639 | Classification loss: 0.08858 | Regression loss: 0.08498 | Running loss: 0.81085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2640 | Classification loss: 0.40858 | Regression loss: 0.31056 | Running loss: 0.81144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2641 | Classification loss: 0.02388 | Regression loss: 0.13129 | Running loss: 0.81085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2642 | Classification loss: 0.59370 | Regression loss: 0.61627 | Running loss: 0.81043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2643 | Classification loss: 0.09036 | Regression loss: 0.21260 | Running loss: 0.80898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2644 | Classification loss: 0.81599 | Regression loss: 0.51526 | Running loss: 0.80970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2645 | Classification loss: 1.16614 | Regression loss: 0.27606 | Running loss: 0.80981 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2646 | Classification loss: 0.24627 | Regression loss: 0.28586 | Running loss: 0.80869 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2647 | Classification loss: 1.30065 | Regression loss: 0.33237 | Running loss: 0.81115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2648 | Classification loss: 0.13700 | Regression loss: 0.30707 | Running loss: 0.80925 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2649 | Classification loss: 0.50347 | Regression loss: 0.18444 | Running loss: 0.80777 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2650 | Classification loss: 0.93686 | Regression loss: 0.65934 | Running loss: 0.80936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2651 | Classification loss: 0.61363 | Regression loss: 0.36916 | Running loss: 0.81052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2652 | Classification loss: 0.00655 | Regression loss: 0.07133 | Running loss: 0.80800 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2653 | Classification loss: 0.03361 | Regression loss: 0.13995 | Running loss: 0.80811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2654 | Classification loss: 0.46791 | Regression loss: 0.50982 | Running loss: 0.80865 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2655 | Classification loss: 0.06061 | Regression loss: 0.20360 | Running loss: 0.80405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2656 | Classification loss: 0.84379 | Regression loss: 0.69126 | Running loss: 0.80689 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2657 | Classification loss: 0.86436 | Regression loss: 0.34468 | Running loss: 0.80388 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2658 | Classification loss: 0.82015 | Regression loss: 0.46156 | Running loss: 0.80483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2659 | Classification loss: 0.60936 | Regression loss: 0.20274 | Running loss: 0.80357 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2660 | Classification loss: 0.01872 | Regression loss: 0.11277 | Running loss: 0.80325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2661 | Classification loss: 1.51372 | Regression loss: 0.67862 | Running loss: 0.80735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2662 | Classification loss: 0.24538 | Regression loss: 0.33270 | Running loss: 0.80816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2663 | Classification loss: 0.22540 | Regression loss: 0.25414 | Running loss: 0.80806 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2664 | Classification loss: 0.20009 | Regression loss: 0.42899 | Running loss: 0.80858 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2665 | Classification loss: 0.69651 | Regression loss: 0.33884 | Running loss: 0.81042 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2666 | Classification loss: 0.63906 | Regression loss: 0.64494 | Running loss: 0.81173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2667 | Classification loss: 0.83854 | Regression loss: 0.72385 | Running loss: 0.81266 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2668 | Classification loss: 0.09793 | Regression loss: 0.21174 | Running loss: 0.80940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2669 | Classification loss: 0.03642 | Regression loss: 0.20863 | Running loss: 0.80911 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2670 | Classification loss: 0.47239 | Regression loss: 0.48793 | Running loss: 0.80679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2671 | Classification loss: 0.09769 | Regression loss: 0.27821 | Running loss: 0.80719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2672 | Classification loss: 1.34348 | Regression loss: 0.59006 | Running loss: 0.81077 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2673 | Classification loss: 0.09157 | Regression loss: 0.42539 | Running loss: 0.80949 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2674 | Classification loss: 0.87213 | Regression loss: 0.42718 | Running loss: 0.81036 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2675 | Classification loss: 0.39628 | Regression loss: 0.46658 | Running loss: 0.81142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2676 | Classification loss: 0.12130 | Regression loss: 0.14677 | Running loss: 0.80712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2677 | Classification loss: 0.09610 | Regression loss: 0.25118 | Running loss: 0.80399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2678 | Classification loss: 0.00144 | Regression loss: 0.09703 | Running loss: 0.80103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2679 | Classification loss: 0.18119 | Regression loss: 0.43128 | Running loss: 0.80221 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2680 | Classification loss: 0.06174 | Regression loss: 0.16072 | Running loss: 0.80232 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2681 | Classification loss: 0.13093 | Regression loss: 0.17656 | Running loss: 0.80278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2682 | Classification loss: 1.32066 | Regression loss: 0.64458 | Running loss: 0.80312 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2683 | Classification loss: 0.60204 | Regression loss: 0.64664 | Running loss: 0.80360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2684 | Classification loss: 0.71740 | Regression loss: 0.43984 | Running loss: 0.80582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2685 | Classification loss: 1.56126 | Regression loss: 0.71379 | Running loss: 0.80790 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2686 | Classification loss: 0.00975 | Regression loss: 0.24705 | Running loss: 0.80590 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2687 | Classification loss: 0.75350 | Regression loss: 0.75749 | Running loss: 0.80843 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2688 | Classification loss: 0.07278 | Regression loss: 0.11159 | Running loss: 0.80683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2689 | Classification loss: 0.00536 | Regression loss: 0.20079 | Running loss: 0.80623 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2690 | Classification loss: 0.12010 | Regression loss: 0.26742 | Running loss: 0.80683 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2691 | Classification loss: 0.03426 | Regression loss: 0.13224 | Running loss: 0.80536 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2692 | Classification loss: 0.26893 | Regression loss: 0.12848 | Running loss: 0.80590 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2693 | Classification loss: 0.02037 | Regression loss: 0.11883 | Running loss: 0.80543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2694 | Classification loss: 0.12894 | Regression loss: 0.51357 | Running loss: 0.80589 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2695 | Classification loss: 0.39181 | Regression loss: 0.33303 | Running loss: 0.80636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2696 | Classification loss: 1.60242 | Regression loss: 0.65252 | Running loss: 0.81033 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2697 | Classification loss: 0.00971 | Regression loss: 0.05814 | Running loss: 0.81022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2698 | Classification loss: 0.10838 | Regression loss: 0.12175 | Running loss: 0.80897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2699 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.80844 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2700 | Classification loss: 0.76718 | Regression loss: 0.49622 | Running loss: 0.80853 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2701 | Classification loss: 2.43439 | Regression loss: 0.11310 | Running loss: 0.81152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2702 | Classification loss: 0.15840 | Regression loss: 0.34720 | Running loss: 0.80985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2703 | Classification loss: 0.32810 | Regression loss: 0.55137 | Running loss: 0.80969 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2704 | Classification loss: 0.33340 | Regression loss: 0.58518 | Running loss: 0.80737 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2705 | Classification loss: 0.00948 | Regression loss: 0.33504 | Running loss: 0.80703 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2706 | Classification loss: 0.21855 | Regression loss: 0.57742 | Running loss: 0.80399 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2707 | Classification loss: 0.00571 | Regression loss: 0.08061 | Running loss: 0.80159 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2708 | Classification loss: 0.00054 | Regression loss: 0.07627 | Running loss: 0.80078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2709 | Classification loss: 0.56838 | Regression loss: 0.22073 | Running loss: 0.80143 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2710 | Classification loss: 1.15178 | Regression loss: 0.26828 | Running loss: 0.80369 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2711 | Classification loss: 0.45247 | Regression loss: 0.24418 | Running loss: 0.80460 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2712 | Classification loss: 0.00019 | Regression loss: 0.13530 | Running loss: 0.80371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2713 | Classification loss: 0.33374 | Regression loss: 0.40520 | Running loss: 0.80479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2714 | Classification loss: 0.25686 | Regression loss: 0.42126 | Running loss: 0.80375 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2715 | Classification loss: 0.18771 | Regression loss: 0.22083 | Running loss: 0.80165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2716 | Classification loss: 0.48257 | Regression loss: 0.49708 | Running loss: 0.80118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2717 | Classification loss: 0.02632 | Regression loss: 0.19809 | Running loss: 0.80025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2718 | Classification loss: 0.43552 | Regression loss: 0.48264 | Running loss: 0.80045 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2719 | Classification loss: 0.03356 | Regression loss: 0.11169 | Running loss: 0.80030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2720 | Classification loss: 0.20754 | Regression loss: 0.29840 | Running loss: 0.79844 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2721 | Classification loss: 0.75480 | Regression loss: 0.45093 | Running loss: 0.80063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2722 | Classification loss: 1.74876 | Regression loss: 0.46554 | Running loss: 0.80214 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2723 | Classification loss: 0.06214 | Regression loss: 0.25972 | Running loss: 0.80050 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2724 | Classification loss: 0.58502 | Regression loss: 0.53018 | Running loss: 0.80137 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2725 | Classification loss: 0.55346 | Regression loss: 0.24494 | Running loss: 0.80080 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2726 | Classification loss: 0.26931 | Regression loss: 0.18555 | Running loss: 0.80051 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2727 | Classification loss: 1.26493 | Regression loss: 0.44141 | Running loss: 0.80282 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2728 | Classification loss: 0.28146 | Regression loss: 0.39528 | Running loss: 0.80028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2729 | Classification loss: 0.00352 | Regression loss: 0.08711 | Running loss: 0.79514 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2730 | Classification loss: 0.40211 | Regression loss: 0.33684 | Running loss: 0.79410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2731 | Classification loss: 0.36505 | Regression loss: 0.35846 | Running loss: 0.79438 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2732 | Classification loss: 0.66367 | Regression loss: 0.50222 | Running loss: 0.79576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2733 | Classification loss: 1.23102 | Regression loss: 0.59601 | Running loss: 0.79765 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2734 | Classification loss: 0.00013 | Regression loss: 0.04542 | Running loss: 0.79642 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2735 | Classification loss: 0.18622 | Regression loss: 0.27166 | Running loss: 0.79716 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2736 | Classification loss: 1.03001 | Regression loss: 0.44304 | Running loss: 0.79724 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2737 | Classification loss: 0.11131 | Regression loss: 0.15752 | Running loss: 0.79753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2738 | Classification loss: 1.15324 | Regression loss: 0.75840 | Running loss: 0.79789 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2739 | Classification loss: 1.80548 | Regression loss: 0.51411 | Running loss: 0.80176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2740 | Classification loss: 0.50358 | Regression loss: 0.24145 | Running loss: 0.80188 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2741 | Classification loss: 1.20119 | Regression loss: 0.24094 | Running loss: 0.80198 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2742 | Classification loss: 0.00021 | Regression loss: 0.08581 | Running loss: 0.80186 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2743 | Classification loss: 0.48584 | Regression loss: 0.57441 | Running loss: 0.79983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2744 | Classification loss: 0.22669 | Regression loss: 0.32537 | Running loss: 0.80069 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2745 | Classification loss: 1.20392 | Regression loss: 0.61885 | Running loss: 0.80270 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2746 | Classification loss: 1.95618 | Regression loss: 0.72995 | Running loss: 0.80734 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2747 | Classification loss: 0.11903 | Regression loss: 0.24432 | Running loss: 0.80717 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2748 | Classification loss: 0.00083 | Regression loss: 0.14155 | Running loss: 0.80713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2749 | Classification loss: 0.04160 | Regression loss: 0.29610 | Running loss: 0.80523 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2750 | Classification loss: 0.90866 | Regression loss: 0.52144 | Running loss: 0.80351 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2751 | Classification loss: 0.62480 | Regression loss: 0.30191 | Running loss: 0.80512 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2752 | Classification loss: 0.15473 | Regression loss: 0.29506 | Running loss: 0.80517 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2753 | Classification loss: 2.38938 | Regression loss: 0.75261 | Running loss: 0.80900 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2754 | Classification loss: 1.73660 | Regression loss: 0.92339 | Running loss: 0.81379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2755 | Classification loss: 0.07945 | Regression loss: 0.12004 | Running loss: 0.81404 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2756 | Classification loss: 0.54755 | Regression loss: 0.11452 | Running loss: 0.81355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2757 | Classification loss: 0.15327 | Regression loss: 0.32985 | Running loss: 0.81433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2758 | Classification loss: 0.17317 | Regression loss: 0.21376 | Running loss: 0.81269 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2759 | Classification loss: 1.32421 | Regression loss: 0.52569 | Running loss: 0.81623 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2760 | Classification loss: 0.00010 | Regression loss: 0.02055 | Running loss: 0.81346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2761 | Classification loss: 0.14250 | Regression loss: 0.28821 | Running loss: 0.81250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2762 | Classification loss: 0.03568 | Regression loss: 0.12285 | Running loss: 0.81269 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2763 | Classification loss: 0.01078 | Regression loss: 0.20554 | Running loss: 0.81284 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2764 | Classification loss: 0.00143 | Regression loss: 0.13874 | Running loss: 0.81281 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2765 | Classification loss: 0.40064 | Regression loss: 0.40044 | Running loss: 0.81190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2766 | Classification loss: 0.04317 | Regression loss: 0.49512 | Running loss: 0.81253 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2767 | Classification loss: 0.53183 | Regression loss: 0.39786 | Running loss: 0.81237 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2768 | Classification loss: 0.15174 | Regression loss: 0.44639 | Running loss: 0.81314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2769 | Classification loss: 0.00239 | Regression loss: 0.02733 | Running loss: 0.80957 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2770 | Classification loss: 0.13405 | Regression loss: 0.31362 | Running loss: 0.80755 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2771 | Classification loss: 0.06398 | Regression loss: 0.14131 | Running loss: 0.80773 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2772 | Classification loss: 0.32231 | Regression loss: 0.44728 | Running loss: 0.80570 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2773 | Classification loss: 0.56028 | Regression loss: 0.54261 | Running loss: 0.79856 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2774 | Classification loss: 1.50335 | Regression loss: 0.37473 | Running loss: 0.80141 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2775 | Classification loss: 0.38487 | Regression loss: 0.46719 | Running loss: 0.80294 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2776 | Classification loss: 0.00412 | Regression loss: 0.11595 | Running loss: 0.80102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2777 | Classification loss: 0.53696 | Regression loss: 0.49441 | Running loss: 0.80226 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2778 | Classification loss: 1.06734 | Regression loss: 0.74171 | Running loss: 0.80563 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2779 | Classification loss: 0.47380 | Regression loss: 0.32113 | Running loss: 0.80344 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2780 | Classification loss: 0.42438 | Regression loss: 0.36083 | Running loss: 0.80493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2781 | Classification loss: 0.60391 | Regression loss: 0.50523 | Running loss: 0.80695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2782 | Classification loss: 0.52597 | Regression loss: 0.37963 | Running loss: 0.80649 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2783 | Classification loss: 0.89830 | Regression loss: 0.47864 | Running loss: 0.80913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2784 | Classification loss: 0.06392 | Regression loss: 0.25108 | Running loss: 0.80901 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2785 | Classification loss: 0.35985 | Regression loss: 0.18429 | Running loss: 0.80736 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2786 | Classification loss: 0.04654 | Regression loss: 0.13355 | Running loss: 0.80712 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2787 | Classification loss: 0.30041 | Regression loss: 0.28429 | Running loss: 0.80777 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2788 | Classification loss: 0.69319 | Regression loss: 0.57415 | Running loss: 0.80995 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2789 | Classification loss: 0.79519 | Regression loss: 0.58929 | Running loss: 0.81037 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2790 | Classification loss: 1.09500 | Regression loss: 0.57728 | Running loss: 0.81327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2791 | Classification loss: 0.09804 | Regression loss: 0.16792 | Running loss: 0.80717 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2792 | Classification loss: 0.56414 | Regression loss: 0.15522 | Running loss: 0.80853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2793 | Classification loss: 1.40072 | Regression loss: 0.75777 | Running loss: 0.81264 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2794 | Classification loss: 0.45028 | Regression loss: 0.48543 | Running loss: 0.81398 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2795 | Classification loss: 0.81368 | Regression loss: 0.47738 | Running loss: 0.81491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2796 | Classification loss: 0.22775 | Regression loss: 0.40480 | Running loss: 0.81493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2797 | Classification loss: 0.01449 | Regression loss: 0.05613 | Running loss: 0.81433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2798 | Classification loss: 0.41957 | Regression loss: 0.25713 | Running loss: 0.81283 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2799 | Classification loss: 0.03786 | Regression loss: 0.15209 | Running loss: 0.81185 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2800 | Classification loss: 0.27371 | Regression loss: 0.51279 | Running loss: 0.81267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2801 | Classification loss: 0.31858 | Regression loss: 0.49600 | Running loss: 0.80860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2802 | Classification loss: 0.00978 | Regression loss: 0.09368 | Running loss: 0.80773 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2803 | Classification loss: 0.83214 | Regression loss: 0.49409 | Running loss: 0.80667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2804 | Classification loss: 0.60401 | Regression loss: 0.46646 | Running loss: 0.80650 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2805 | Classification loss: 0.01084 | Regression loss: 0.10063 | Running loss: 0.80502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2806 | Classification loss: 0.08149 | Regression loss: 0.31353 | Running loss: 0.80213 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2807 | Classification loss: 0.73793 | Regression loss: 0.46587 | Running loss: 0.80242 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2808 | Classification loss: 0.63418 | Regression loss: 0.65373 | Running loss: 0.80466 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2809 | Classification loss: 0.19486 | Regression loss: 0.34119 | Running loss: 0.80057 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2810 | Classification loss: 0.03866 | Regression loss: 0.19530 | Running loss: 0.79934 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2811 | Classification loss: 0.09453 | Regression loss: 0.11765 | Running loss: 0.79961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2812 | Classification loss: 0.08959 | Regression loss: 0.30702 | Running loss: 0.79607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2813 | Classification loss: 1.66188 | Regression loss: 0.72839 | Running loss: 0.79961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2814 | Classification loss: 0.00426 | Regression loss: 0.13557 | Running loss: 0.79960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2815 | Classification loss: 0.04076 | Regression loss: 0.12134 | Running loss: 0.79810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2816 | Classification loss: 0.00029 | Regression loss: 0.03660 | Running loss: 0.79652 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2817 | Classification loss: 1.29983 | Regression loss: 0.81077 | Running loss: 0.79908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2818 | Classification loss: 1.58092 | Regression loss: 0.65081 | Running loss: 0.80316 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2819 | Classification loss: 0.39852 | Regression loss: 0.27691 | Running loss: 0.80403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2820 | Classification loss: 0.05474 | Regression loss: 0.31679 | Running loss: 0.80323 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2821 | Classification loss: 0.32788 | Regression loss: 0.23695 | Running loss: 0.80300 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2822 | Classification loss: 1.00028 | Regression loss: 0.57291 | Running loss: 0.80292 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2823 | Classification loss: 0.71396 | Regression loss: 0.47949 | Running loss: 0.80519 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2824 | Classification loss: 0.61598 | Regression loss: 0.50726 | Running loss: 0.80571 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2825 | Classification loss: 0.46824 | Regression loss: 0.36485 | Running loss: 0.80708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2826 | Classification loss: 0.28742 | Regression loss: 0.34322 | Running loss: 0.80781 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2827 | Classification loss: 0.02336 | Regression loss: 0.18258 | Running loss: 0.80735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2828 | Classification loss: 0.09046 | Regression loss: 0.11992 | Running loss: 0.80583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2829 | Classification loss: 0.52119 | Regression loss: 0.40154 | Running loss: 0.80735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2830 | Classification loss: 0.86090 | Regression loss: 0.42484 | Running loss: 0.80764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2831 | Classification loss: 0.06721 | Regression loss: 0.26987 | Running loss: 0.80539 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2832 | Classification loss: 0.29401 | Regression loss: 0.44458 | Running loss: 0.80439 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2833 | Classification loss: 0.01850 | Regression loss: 0.12339 | Running loss: 0.80313 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2834 | Classification loss: 0.10683 | Regression loss: 0.18853 | Running loss: 0.79982 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2835 | Classification loss: 0.47074 | Regression loss: 0.28292 | Running loss: 0.80008 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2836 | Classification loss: 0.91971 | Regression loss: 0.77675 | Running loss: 0.80255 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2837 | Classification loss: 0.20478 | Regression loss: 0.28589 | Running loss: 0.80312 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2838 | Classification loss: 0.59627 | Regression loss: 0.53173 | Running loss: 0.80514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2839 | Classification loss: 0.00150 | Regression loss: 0.08308 | Running loss: 0.80097 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2840 | Classification loss: 0.08525 | Regression loss: 0.17764 | Running loss: 0.80012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2841 | Classification loss: 0.39205 | Regression loss: 0.70680 | Running loss: 0.80221 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2842 | Classification loss: 0.07892 | Regression loss: 0.20473 | Running loss: 0.80246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2843 | Classification loss: 0.10023 | Regression loss: 0.20322 | Running loss: 0.79919 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2844 | Classification loss: 1.56420 | Regression loss: 0.76029 | Running loss: 0.80336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2845 | Classification loss: 0.16439 | Regression loss: 0.28593 | Running loss: 0.80333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2846 | Classification loss: 0.00058 | Regression loss: 0.02937 | Running loss: 0.80317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2847 | Classification loss: 0.14923 | Regression loss: 0.36760 | Running loss: 0.80083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2848 | Classification loss: 1.43054 | Regression loss: 0.71003 | Running loss: 0.80444 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2849 | Classification loss: 0.04750 | Regression loss: 0.12204 | Running loss: 0.80108 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2850 | Classification loss: 0.01396 | Regression loss: 0.17949 | Running loss: 0.80006 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2851 | Classification loss: 0.07334 | Regression loss: 0.12340 | Running loss: 0.79985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2852 | Classification loss: 0.15490 | Regression loss: 0.23105 | Running loss: 0.79958 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2853 | Classification loss: 0.04573 | Regression loss: 0.08342 | Running loss: 0.79941 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2854 | Classification loss: 0.08370 | Regression loss: 0.28235 | Running loss: 0.79990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2855 | Classification loss: 0.02892 | Regression loss: 0.16611 | Running loss: 0.79566 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2856 | Classification loss: 0.13081 | Regression loss: 0.18047 | Running loss: 0.79542 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2857 | Classification loss: 0.71716 | Regression loss: 0.50702 | Running loss: 0.79209 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2858 | Classification loss: 0.02728 | Regression loss: 0.17241 | Running loss: 0.78950 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2859 | Classification loss: 0.05992 | Regression loss: 0.10331 | Running loss: 0.78907 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2860 | Classification loss: 0.69520 | Regression loss: 0.37263 | Running loss: 0.79074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2861 | Classification loss: 0.18638 | Regression loss: 0.15591 | Running loss: 0.78932 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2862 | Classification loss: 1.08260 | Regression loss: 0.61681 | Running loss: 0.79220 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2863 | Classification loss: 0.47975 | Regression loss: 0.43930 | Running loss: 0.78869 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2864 | Classification loss: 1.69448 | Regression loss: 0.56619 | Running loss: 0.79271 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2865 | Classification loss: 0.13169 | Regression loss: 0.26587 | Running loss: 0.79314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2866 | Classification loss: 0.54980 | Regression loss: 0.46970 | Running loss: 0.79402 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2867 | Classification loss: 2.75166 | Regression loss: 0.87630 | Running loss: 0.80050 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2868 | Classification loss: 0.25486 | Regression loss: 0.33360 | Running loss: 0.80126 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2869 | Classification loss: 0.22444 | Regression loss: 0.49883 | Running loss: 0.80215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2870 | Classification loss: 1.05162 | Regression loss: 0.52892 | Running loss: 0.80499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2871 | Classification loss: 0.23218 | Regression loss: 0.20842 | Running loss: 0.80315 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2872 | Classification loss: 0.47184 | Regression loss: 0.55499 | Running loss: 0.80390 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2873 | Classification loss: 0.33375 | Regression loss: 0.34454 | Running loss: 0.80458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2874 | Classification loss: 0.02663 | Regression loss: 0.12331 | Running loss: 0.80248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2875 | Classification loss: 0.10821 | Regression loss: 0.31220 | Running loss: 0.80299 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2876 | Classification loss: 0.28021 | Regression loss: 0.26545 | Running loss: 0.80236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2877 | Classification loss: 0.01650 | Regression loss: 0.14871 | Running loss: 0.80199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2878 | Classification loss: 0.57149 | Regression loss: 0.63964 | Running loss: 0.80309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2879 | Classification loss: 0.00242 | Regression loss: 0.10655 | Running loss: 0.80294 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2880 | Classification loss: 0.00003 | Regression loss: 0.04597 | Running loss: 0.80156 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2881 | Classification loss: 0.12502 | Regression loss: 0.22394 | Running loss: 0.80163 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2882 | Classification loss: 0.56619 | Regression loss: 0.52388 | Running loss: 0.80333 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2883 | Classification loss: 0.02720 | Regression loss: 0.06016 | Running loss: 0.80314 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2884 | Classification loss: 0.17822 | Regression loss: 0.20324 | Running loss: 0.80144 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2885 | Classification loss: 0.67945 | Regression loss: 0.39885 | Running loss: 0.80311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2886 | Classification loss: 0.35038 | Regression loss: 0.40784 | Running loss: 0.80268 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2887 | Classification loss: 0.39005 | Regression loss: 0.34943 | Running loss: 0.80353 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2888 | Classification loss: 0.02336 | Regression loss: 0.18316 | Running loss: 0.80249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2889 | Classification loss: 0.53158 | Regression loss: 0.51704 | Running loss: 0.80389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2890 | Classification loss: 0.04083 | Regression loss: 0.23476 | Running loss: 0.80325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2891 | Classification loss: 0.30241 | Regression loss: 0.33555 | Running loss: 0.80211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2892 | Classification loss: 0.02824 | Regression loss: 0.27384 | Running loss: 0.79858 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2893 | Classification loss: 0.12121 | Regression loss: 0.30981 | Running loss: 0.79839 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2894 | Classification loss: 0.30992 | Regression loss: 0.32852 | Running loss: 0.79765 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2895 | Classification loss: 0.99381 | Regression loss: 0.66010 | Running loss: 0.79905 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2896 | Classification loss: 0.63670 | Regression loss: 0.37531 | Running loss: 0.79878 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2897 | Classification loss: 0.33284 | Regression loss: 0.19624 | Running loss: 0.79953 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2898 | Classification loss: 0.70900 | Regression loss: 0.54474 | Running loss: 0.79750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2899 | Classification loss: 0.03681 | Regression loss: 0.24865 | Running loss: 0.79652 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2900 | Classification loss: 0.09285 | Regression loss: 0.22620 | Running loss: 0.79622 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2901 | Classification loss: 0.27608 | Regression loss: 0.20997 | Running loss: 0.79549 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2902 | Classification loss: 0.02609 | Regression loss: 0.13058 | Running loss: 0.79545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2903 | Classification loss: 0.36861 | Regression loss: 0.67222 | Running loss: 0.79572 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2904 | Classification loss: 0.08052 | Regression loss: 0.23956 | Running loss: 0.79587 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2905 | Classification loss: 1.64998 | Regression loss: 0.30493 | Running loss: 0.79671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2906 | Classification loss: 1.18865 | Regression loss: 0.64657 | Running loss: 0.79970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2907 | Classification loss: 0.04111 | Regression loss: 0.11167 | Running loss: 0.79905 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2908 | Classification loss: 0.02003 | Regression loss: 0.40728 | Running loss: 0.79762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2909 | Classification loss: 1.45298 | Regression loss: 0.88543 | Running loss: 0.79957 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2910 | Classification loss: 0.16728 | Regression loss: 0.17649 | Running loss: 0.79979 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2911 | Classification loss: 0.72465 | Regression loss: 0.22971 | Running loss: 0.79780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2912 | Classification loss: 0.82129 | Regression loss: 0.45560 | Running loss: 0.79859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2913 | Classification loss: 0.00545 | Regression loss: 0.14276 | Running loss: 0.79658 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2914 | Classification loss: 0.09751 | Regression loss: 0.17251 | Running loss: 0.79569 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2915 | Classification loss: 0.14311 | Regression loss: 0.21653 | Running loss: 0.79497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2916 | Classification loss: 0.06793 | Regression loss: 0.12398 | Running loss: 0.79528 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2917 | Classification loss: 1.07100 | Regression loss: 0.73495 | Running loss: 0.79682 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2918 | Classification loss: 0.00002 | Regression loss: 0.01341 | Running loss: 0.79506 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2919 | Classification loss: 0.00954 | Regression loss: 0.11444 | Running loss: 0.79381 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2920 | Classification loss: 0.33489 | Regression loss: 0.45311 | Running loss: 0.79445 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2921 | Classification loss: 1.37444 | Regression loss: 0.45054 | Running loss: 0.79703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2922 | Classification loss: 0.12991 | Regression loss: 0.14089 | Running loss: 0.79638 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2923 | Classification loss: 0.00031 | Regression loss: 0.02100 | Running loss: 0.79395 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2924 | Classification loss: 0.76874 | Regression loss: 0.34487 | Running loss: 0.79589 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2925 | Classification loss: 0.59966 | Regression loss: 0.77463 | Running loss: 0.79825 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2926 | Classification loss: 0.23766 | Regression loss: 0.44008 | Running loss: 0.79799 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2927 | Classification loss: 0.00708 | Regression loss: 0.19748 | Running loss: 0.79817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2928 | Classification loss: 0.25877 | Regression loss: 0.27076 | Running loss: 0.79902 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2929 | Classification loss: 1.33321 | Regression loss: 0.73821 | Running loss: 0.79966 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2930 | Classification loss: 0.18889 | Regression loss: 0.54920 | Running loss: 0.79854 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2931 | Classification loss: 1.25824 | Regression loss: 0.59337 | Running loss: 0.79738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2932 | Classification loss: 0.21058 | Regression loss: 0.16922 | Running loss: 0.79548 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2933 | Classification loss: 0.58257 | Regression loss: 0.46489 | Running loss: 0.79433 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2934 | Classification loss: 0.06571 | Regression loss: 0.19821 | Running loss: 0.79170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2935 | Classification loss: 0.06038 | Regression loss: 0.04819 | Running loss: 0.79162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2936 | Classification loss: 0.75965 | Regression loss: 0.55543 | Running loss: 0.79174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2937 | Classification loss: 0.11883 | Regression loss: 0.23051 | Running loss: 0.79218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2938 | Classification loss: 2.55915 | Regression loss: 0.62016 | Running loss: 0.79749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2939 | Classification loss: 0.03495 | Regression loss: 0.15664 | Running loss: 0.79578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2940 | Classification loss: 0.11655 | Regression loss: 0.32644 | Running loss: 0.79431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2941 | Classification loss: 0.79429 | Regression loss: 0.48912 | Running loss: 0.79592 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2942 | Classification loss: 0.04755 | Regression loss: 0.33889 | Running loss: 0.79480 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2943 | Classification loss: 0.50966 | Regression loss: 0.43261 | Running loss: 0.79298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2944 | Classification loss: 0.27659 | Regression loss: 0.39666 | Running loss: 0.79178 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2945 | Classification loss: 0.02152 | Regression loss: 0.19540 | Running loss: 0.79140 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2946 | Classification loss: 0.01695 | Regression loss: 0.12189 | Running loss: 0.78877 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2947 | Classification loss: 0.78592 | Regression loss: 0.23004 | Running loss: 0.78952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2948 | Classification loss: 0.10815 | Regression loss: 0.16787 | Running loss: 0.78962 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2949 | Classification loss: 0.36885 | Regression loss: 0.50444 | Running loss: 0.79115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2950 | Classification loss: 0.09688 | Regression loss: 0.37623 | Running loss: 0.78765 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2951 | Classification loss: 0.53007 | Regression loss: 0.34723 | Running loss: 0.78904 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2952 | Classification loss: 0.12327 | Regression loss: 0.27253 | Running loss: 0.78812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2953 | Classification loss: 0.08763 | Regression loss: 0.21157 | Running loss: 0.78848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2954 | Classification loss: 0.01846 | Regression loss: 0.10989 | Running loss: 0.78660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2955 | Classification loss: 0.31336 | Regression loss: 0.46247 | Running loss: 0.78497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2956 | Classification loss: 0.75929 | Regression loss: 0.77508 | Running loss: 0.78443 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2957 | Classification loss: 0.39987 | Regression loss: 0.40253 | Running loss: 0.78375 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2958 | Classification loss: 0.30701 | Regression loss: 0.55304 | Running loss: 0.78394 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2959 | Classification loss: 0.37430 | Regression loss: 0.46570 | Running loss: 0.78499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2960 | Classification loss: 0.57629 | Regression loss: 0.40813 | Running loss: 0.78654 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2961 | Classification loss: 0.27762 | Regression loss: 0.28197 | Running loss: 0.78703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2962 | Classification loss: 0.22361 | Regression loss: 0.25109 | Running loss: 0.78607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2963 | Classification loss: 0.13286 | Regression loss: 0.23265 | Running loss: 0.78668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2964 | Classification loss: 0.15189 | Regression loss: 0.59064 | Running loss: 0.78796 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2965 | Classification loss: 0.16497 | Regression loss: 0.33536 | Running loss: 0.78849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2966 | Classification loss: 1.05397 | Regression loss: 0.54402 | Running loss: 0.79093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2967 | Classification loss: 0.14779 | Regression loss: 0.10657 | Running loss: 0.78859 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2968 | Classification loss: 0.93377 | Regression loss: 0.21905 | Running loss: 0.79030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2969 | Classification loss: 0.01083 | Regression loss: 0.12358 | Running loss: 0.79015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2970 | Classification loss: 0.47582 | Regression loss: 0.65505 | Running loss: 0.79077 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2971 | Classification loss: 0.17820 | Regression loss: 0.43321 | Running loss: 0.78666 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2972 | Classification loss: 0.06509 | Regression loss: 0.07241 | Running loss: 0.78648 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2973 | Classification loss: 0.00970 | Regression loss: 0.16718 | Running loss: 0.78607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2974 | Classification loss: 0.78459 | Regression loss: 0.45140 | Running loss: 0.78695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2975 | Classification loss: 0.13011 | Regression loss: 0.25248 | Running loss: 0.78744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2976 | Classification loss: 0.00104 | Regression loss: 0.04007 | Running loss: 0.78688 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2977 | Classification loss: 0.01667 | Regression loss: 0.17334 | Running loss: 0.78502 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2978 | Classification loss: 0.11695 | Regression loss: 0.25569 | Running loss: 0.78526 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2979 | Classification loss: 0.56489 | Regression loss: 0.45211 | Running loss: 0.78277 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2980 | Classification loss: 0.58056 | Regression loss: 0.59450 | Running loss: 0.78335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2981 | Classification loss: 0.69459 | Regression loss: 0.52883 | Running loss: 0.78552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2982 | Classification loss: 0.28856 | Regression loss: 0.41729 | Running loss: 0.78626 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2983 | Classification loss: 0.43044 | Regression loss: 0.22752 | Running loss: 0.78642 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2984 | Classification loss: 0.18527 | Regression loss: 0.42115 | Running loss: 0.78392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2985 | Classification loss: 0.00255 | Regression loss: 0.00000 | Running loss: 0.78332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2986 | Classification loss: 0.65788 | Regression loss: 0.67628 | Running loss: 0.78410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2987 | Classification loss: 0.09903 | Regression loss: 0.17701 | Running loss: 0.78313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2988 | Classification loss: 1.28866 | Regression loss: 0.21943 | Running loss: 0.78426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2989 | Classification loss: 1.17301 | Regression loss: 0.22937 | Running loss: 0.78655 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2990 | Classification loss: 0.61967 | Regression loss: 0.36156 | Running loss: 0.78762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2991 | Classification loss: 0.11725 | Regression loss: 0.26987 | Running loss: 0.78535 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2992 | Classification loss: 0.03471 | Regression loss: 0.05511 | Running loss: 0.78310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2993 | Classification loss: 1.14074 | Regression loss: 0.69365 | Running loss: 0.78462 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2994 | Classification loss: 0.29583 | Regression loss: 0.40537 | Running loss: 0.78425 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2995 | Classification loss: 0.65535 | Regression loss: 0.53184 | Running loss: 0.78320 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 2996 | Classification loss: 1.12354 | Regression loss: 0.67654 | Running loss: 0.78630 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 2997 | Classification loss: 0.67955 | Regression loss: 0.48306 | Running loss: 0.78751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2998 | Classification loss: 0.14172 | Regression loss: 0.55921 | Running loss: 0.78846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 2999 | Classification loss: 0.05377 | Regression loss: 0.17295 | Running loss: 0.78387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3000 | Classification loss: 0.40805 | Regression loss: 0.44185 | Running loss: 0.78458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3001 | Classification loss: 0.44593 | Regression loss: 0.18541 | Running loss: 0.78397 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3002 | Classification loss: 0.50048 | Regression loss: 0.37537 | Running loss: 0.78460 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3003 | Classification loss: 0.41223 | Regression loss: 0.41172 | Running loss: 0.78562 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3004 | Classification loss: 0.14727 | Regression loss: 0.23012 | Running loss: 0.78546 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3005 | Classification loss: 0.04646 | Regression loss: 0.38821 | Running loss: 0.78584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3006 | Classification loss: 0.08383 | Regression loss: 0.22415 | Running loss: 0.78502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3007 | Classification loss: 0.47774 | Regression loss: 0.32288 | Running loss: 0.78600 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3008 | Classification loss: 0.94104 | Regression loss: 0.59684 | Running loss: 0.78870 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3009 | Classification loss: 1.42251 | Regression loss: 0.64420 | Running loss: 0.79113 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3010 | Classification loss: 0.25785 | Regression loss: 0.57589 | Running loss: 0.79041 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3011 | Classification loss: 1.22647 | Regression loss: 0.23613 | Running loss: 0.79293 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3012 | Classification loss: 0.47652 | Regression loss: 0.48294 | Running loss: 0.79455 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3013 | Classification loss: 0.41595 | Regression loss: 0.15063 | Running loss: 0.79490 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3014 | Classification loss: 0.12233 | Regression loss: 0.02604 | Running loss: 0.79148 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3015 | Classification loss: 1.16687 | Regression loss: 0.59347 | Running loss: 0.79115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3016 | Classification loss: 0.48402 | Regression loss: 0.54922 | Running loss: 0.79261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3017 | Classification loss: 0.38989 | Regression loss: 0.14168 | Running loss: 0.79101 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3018 | Classification loss: 0.00368 | Regression loss: 0.11517 | Running loss: 0.78849 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3019 | Classification loss: 0.18214 | Regression loss: 0.43369 | Running loss: 0.78795 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3020 | Classification loss: 0.16192 | Regression loss: 0.15250 | Running loss: 0.78470 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3021 | Classification loss: 0.34837 | Regression loss: 0.25745 | Running loss: 0.78496 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3022 | Classification loss: 0.01005 | Regression loss: 0.15759 | Running loss: 0.78448 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3023 | Classification loss: 0.01354 | Regression loss: 0.36219 | Running loss: 0.78499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3024 | Classification loss: 0.46460 | Regression loss: 0.64196 | Running loss: 0.78677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3025 | Classification loss: 0.23076 | Regression loss: 0.33028 | Running loss: 0.78710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3026 | Classification loss: 0.44989 | Regression loss: 0.34734 | Running loss: 0.78624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3027 | Classification loss: 0.50190 | Regression loss: 0.53910 | Running loss: 0.78805 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3028 | Classification loss: 0.41321 | Regression loss: 0.40736 | Running loss: 0.78894 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3029 | Classification loss: 1.27983 | Regression loss: 0.77801 | Running loss: 0.79261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3030 | Classification loss: 0.26195 | Regression loss: 0.45348 | Running loss: 0.79236 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3031 | Classification loss: 0.93219 | Regression loss: 0.71344 | Running loss: 0.79131 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3032 | Classification loss: 0.51891 | Regression loss: 0.29706 | Running loss: 0.79083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3033 | Classification loss: 0.07952 | Regression loss: 0.32416 | Running loss: 0.79138 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3034 | Classification loss: 0.46658 | Regression loss: 0.45338 | Running loss: 0.79053 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3035 | Classification loss: 0.02876 | Regression loss: 0.12696 | Running loss: 0.79065 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3036 | Classification loss: 1.28405 | Regression loss: 0.46578 | Running loss: 0.78815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3037 | Classification loss: 0.15298 | Regression loss: 0.41205 | Running loss: 0.78700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3038 | Classification loss: 0.49979 | Regression loss: 0.28721 | Running loss: 0.78840 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3039 | Classification loss: 0.01687 | Regression loss: 0.06527 | Running loss: 0.78411 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3040 | Classification loss: 0.62739 | Regression loss: 0.18789 | Running loss: 0.78555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3041 | Classification loss: 0.00085 | Regression loss: 0.14073 | Running loss: 0.77958 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3042 | Classification loss: 0.08571 | Regression loss: 0.09285 | Running loss: 0.77611 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3043 | Classification loss: 0.14291 | Regression loss: 0.37677 | Running loss: 0.77643 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3044 | Classification loss: 0.42660 | Regression loss: 0.60201 | Running loss: 0.77574 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3045 | Classification loss: 1.12536 | Regression loss: 0.40124 | Running loss: 0.77796 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3046 | Classification loss: 1.48391 | Regression loss: 0.54598 | Running loss: 0.78174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3047 | Classification loss: 0.06602 | Regression loss: 0.23322 | Running loss: 0.78203 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3048 | Classification loss: 0.10580 | Regression loss: 0.20288 | Running loss: 0.78239 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3049 | Classification loss: 0.27011 | Regression loss: 0.34119 | Running loss: 0.78221 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3050 | Classification loss: 1.46665 | Regression loss: 0.65704 | Running loss: 0.78608 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3051 | Classification loss: 0.03408 | Regression loss: 0.17274 | Running loss: 0.78638 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3052 | Classification loss: 0.06623 | Regression loss: 0.37426 | Running loss: 0.78511 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3053 | Classification loss: 0.03026 | Regression loss: 0.11353 | Running loss: 0.78471 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3054 | Classification loss: 0.10386 | Regression loss: 0.22971 | Running loss: 0.78473 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3055 | Classification loss: 0.02169 | Regression loss: 0.18079 | Running loss: 0.78500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3056 | Classification loss: 0.00439 | Regression loss: 0.17743 | Running loss: 0.78196 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3057 | Classification loss: 0.39112 | Regression loss: 0.39766 | Running loss: 0.78331 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3058 | Classification loss: 0.00890 | Regression loss: 0.05103 | Running loss: 0.78305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3059 | Classification loss: 0.04281 | Regression loss: 0.20481 | Running loss: 0.78232 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3060 | Classification loss: 0.53354 | Regression loss: 0.51100 | Running loss: 0.78347 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3061 | Classification loss: 0.08347 | Regression loss: 0.31015 | Running loss: 0.78115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3062 | Classification loss: 0.65100 | Regression loss: 0.65910 | Running loss: 0.78247 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3063 | Classification loss: 0.71239 | Regression loss: 0.79646 | Running loss: 0.78348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3064 | Classification loss: 0.10219 | Regression loss: 0.26970 | Running loss: 0.78185 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3065 | Classification loss: 0.64759 | Regression loss: 0.67559 | Running loss: 0.78207 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3066 | Classification loss: 0.70730 | Regression loss: 0.31475 | Running loss: 0.78280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3067 | Classification loss: 1.69739 | Regression loss: 0.43936 | Running loss: 0.78341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3068 | Classification loss: 0.12310 | Regression loss: 0.13324 | Running loss: 0.78110 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3069 | Classification loss: 0.89486 | Regression loss: 0.07301 | Running loss: 0.78143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3070 | Classification loss: 0.00119 | Regression loss: 0.00000 | Running loss: 0.78115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3071 | Classification loss: 0.60672 | Regression loss: 0.12586 | Running loss: 0.78182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3072 | Classification loss: 0.26721 | Regression loss: 0.40625 | Running loss: 0.78103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3073 | Classification loss: 0.14480 | Regression loss: 0.34031 | Running loss: 0.78077 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3074 | Classification loss: 0.17146 | Regression loss: 0.42188 | Running loss: 0.78122 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3075 | Classification loss: 0.01576 | Regression loss: 0.04914 | Running loss: 0.78005 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3076 | Classification loss: 0.52602 | Regression loss: 0.41375 | Running loss: 0.78101 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3077 | Classification loss: 1.10521 | Regression loss: 0.68687 | Running loss: 0.78240 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3078 | Classification loss: 0.35268 | Regression loss: 0.24663 | Running loss: 0.78013 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3079 | Classification loss: 0.26025 | Regression loss: 0.63233 | Running loss: 0.77938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3080 | Classification loss: 0.45663 | Regression loss: 0.44918 | Running loss: 0.78050 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3081 | Classification loss: 0.70219 | Regression loss: 0.44582 | Running loss: 0.78145 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3082 | Classification loss: 0.63055 | Regression loss: 0.15609 | Running loss: 0.78026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3083 | Classification loss: 0.40722 | Regression loss: 0.61880 | Running loss: 0.78215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3084 | Classification loss: 0.10382 | Regression loss: 0.32191 | Running loss: 0.78218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3085 | Classification loss: 1.51612 | Regression loss: 0.49496 | Running loss: 0.78508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3086 | Classification loss: 0.02285 | Regression loss: 0.39871 | Running loss: 0.78119 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3087 | Classification loss: 0.11507 | Regression loss: 0.32078 | Running loss: 0.77966 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3088 | Classification loss: 0.66951 | Regression loss: 0.32888 | Running loss: 0.77928 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3089 | Classification loss: 0.11785 | Regression loss: 0.22941 | Running loss: 0.77897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3090 | Classification loss: 0.73541 | Regression loss: 0.56536 | Running loss: 0.77879 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3091 | Classification loss: 0.42396 | Regression loss: 0.55424 | Running loss: 0.77827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3092 | Classification loss: 2.18793 | Regression loss: 0.68102 | Running loss: 0.78291 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3093 | Classification loss: 0.00104 | Regression loss: 0.05398 | Running loss: 0.78119 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3094 | Classification loss: 0.53802 | Regression loss: 0.62453 | Running loss: 0.78267 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3095 | Classification loss: 0.00345 | Regression loss: 0.19495 | Running loss: 0.78297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3096 | Classification loss: 0.57765 | Regression loss: 0.52217 | Running loss: 0.78305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3097 | Classification loss: 0.42689 | Regression loss: 0.25058 | Running loss: 0.78405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3098 | Classification loss: 1.24992 | Regression loss: 0.73055 | Running loss: 0.78764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3099 | Classification loss: 0.01237 | Regression loss: 0.21343 | Running loss: 0.78786 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3100 | Classification loss: 1.28120 | Regression loss: 0.29529 | Running loss: 0.79087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3101 | Classification loss: 1.09672 | Regression loss: 0.59284 | Running loss: 0.79345 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3102 | Classification loss: 0.42519 | Regression loss: 0.40750 | Running loss: 0.79007 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3103 | Classification loss: 1.05176 | Regression loss: 0.47943 | Running loss: 0.79187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3104 | Classification loss: 0.41608 | Regression loss: 0.49123 | Running loss: 0.79295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3105 | Classification loss: 1.67628 | Regression loss: 0.57576 | Running loss: 0.79642 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3106 | Classification loss: 1.30418 | Regression loss: 0.00000 | Running loss: 0.79790 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3107 | Classification loss: 0.08680 | Regression loss: 0.14486 | Running loss: 0.79507 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3108 | Classification loss: 0.08434 | Regression loss: 0.33426 | Running loss: 0.79012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3109 | Classification loss: 0.20834 | Regression loss: 0.46678 | Running loss: 0.78686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3110 | Classification loss: 0.59687 | Regression loss: 0.30018 | Running loss: 0.78787 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3111 | Classification loss: 0.00004 | Regression loss: 0.02176 | Running loss: 0.78610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3112 | Classification loss: 0.02610 | Regression loss: 0.20941 | Running loss: 0.78584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3113 | Classification loss: 0.06378 | Regression loss: 0.16548 | Running loss: 0.78555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3114 | Classification loss: 0.03039 | Regression loss: 0.09417 | Running loss: 0.78520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3115 | Classification loss: 0.00032 | Regression loss: 0.08774 | Running loss: 0.78423 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3116 | Classification loss: 0.02613 | Regression loss: 0.21766 | Running loss: 0.78399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3117 | Classification loss: 0.79150 | Regression loss: 0.33411 | Running loss: 0.78585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3118 | Classification loss: 0.10966 | Regression loss: 0.13216 | Running loss: 0.78612 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3119 | Classification loss: 0.41207 | Regression loss: 0.34967 | Running loss: 0.78551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3120 | Classification loss: 0.39967 | Regression loss: 0.16214 | Running loss: 0.78347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3121 | Classification loss: 0.03945 | Regression loss: 0.10608 | Running loss: 0.78291 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3122 | Classification loss: 0.04626 | Regression loss: 0.11330 | Running loss: 0.78079 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3123 | Classification loss: 0.46108 | Regression loss: 0.42778 | Running loss: 0.78206 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3124 | Classification loss: 1.03875 | Regression loss: 0.56968 | Running loss: 0.78352 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3125 | Classification loss: 0.46459 | Regression loss: 0.62206 | Running loss: 0.78475 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3126 | Classification loss: 0.01222 | Regression loss: 0.08771 | Running loss: 0.78466 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3127 | Classification loss: 0.01248 | Regression loss: 0.31278 | Running loss: 0.78329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3128 | Classification loss: 0.10207 | Regression loss: 0.09550 | Running loss: 0.78189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3129 | Classification loss: 0.16113 | Regression loss: 0.20422 | Running loss: 0.78197 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3130 | Classification loss: 0.29816 | Regression loss: 0.33093 | Running loss: 0.78230 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3131 | Classification loss: 0.69463 | Regression loss: 0.63643 | Running loss: 0.78379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3132 | Classification loss: 0.19645 | Regression loss: 0.23618 | Running loss: 0.78386 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3133 | Classification loss: 0.00324 | Regression loss: 0.06962 | Running loss: 0.78089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3134 | Classification loss: 0.74588 | Regression loss: 0.28931 | Running loss: 0.78190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3135 | Classification loss: 0.12111 | Regression loss: 0.17393 | Running loss: 0.78095 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3136 | Classification loss: 0.59954 | Regression loss: 0.33596 | Running loss: 0.78183 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3137 | Classification loss: 0.09283 | Regression loss: 0.20837 | Running loss: 0.78063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3138 | Classification loss: 0.02718 | Regression loss: 0.14051 | Running loss: 0.77993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3139 | Classification loss: 0.33170 | Regression loss: 0.51305 | Running loss: 0.78128 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3140 | Classification loss: 0.20969 | Regression loss: 0.43333 | Running loss: 0.78112 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3141 | Classification loss: 0.11354 | Regression loss: 0.35414 | Running loss: 0.78175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3142 | Classification loss: 0.87142 | Regression loss: 0.38915 | Running loss: 0.78185 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3143 | Classification loss: 0.07508 | Regression loss: 0.44621 | Running loss: 0.78229 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3144 | Classification loss: 0.02473 | Regression loss: 0.25872 | Running loss: 0.78019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3145 | Classification loss: 0.01632 | Regression loss: 0.20379 | Running loss: 0.77775 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3146 | Classification loss: 0.06787 | Regression loss: 0.11152 | Running loss: 0.77704 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3147 | Classification loss: 0.00759 | Regression loss: 0.11363 | Running loss: 0.77402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3148 | Classification loss: 0.00019 | Regression loss: 0.03986 | Running loss: 0.77321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3149 | Classification loss: 0.05326 | Regression loss: 0.20097 | Running loss: 0.77234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3150 | Classification loss: 0.61048 | Regression loss: 0.17016 | Running loss: 0.77071 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3151 | Classification loss: 0.03603 | Regression loss: 0.28293 | Running loss: 0.76938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3152 | Classification loss: 0.03178 | Regression loss: 0.05953 | Running loss: 0.76941 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3153 | Classification loss: 0.86449 | Regression loss: 0.51412 | Running loss: 0.77182 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3154 | Classification loss: 0.04694 | Regression loss: 0.12305 | Running loss: 0.77020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3155 | Classification loss: 0.15681 | Regression loss: 0.10115 | Running loss: 0.77019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3156 | Classification loss: 0.40520 | Regression loss: 0.55436 | Running loss: 0.76904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3157 | Classification loss: 0.00209 | Regression loss: 0.10234 | Running loss: 0.76683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3158 | Classification loss: 0.08936 | Regression loss: 0.21684 | Running loss: 0.76488 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3159 | Classification loss: 0.17171 | Regression loss: 0.13078 | Running loss: 0.76386 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3160 | Classification loss: 0.12214 | Regression loss: 0.33664 | Running loss: 0.76452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3161 | Classification loss: 0.84033 | Regression loss: 0.53366 | Running loss: 0.76288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3162 | Classification loss: 0.02599 | Regression loss: 0.19491 | Running loss: 0.76217 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3163 | Classification loss: 0.48792 | Regression loss: 0.49637 | Running loss: 0.76317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3164 | Classification loss: 0.01252 | Regression loss: 0.12082 | Running loss: 0.76218 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3165 | Classification loss: 0.02357 | Regression loss: 0.22635 | Running loss: 0.76061 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3166 | Classification loss: 1.16768 | Regression loss: 0.69602 | Running loss: 0.76177 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3167 | Classification loss: 0.00923 | Regression loss: 0.15022 | Running loss: 0.75897 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3168 | Classification loss: 0.15981 | Regression loss: 0.45346 | Running loss: 0.75957 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3169 | Classification loss: 0.14919 | Regression loss: 0.28362 | Running loss: 0.75995 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3170 | Classification loss: 0.05334 | Regression loss: 0.18755 | Running loss: 0.75851 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3171 | Classification loss: 0.04896 | Regression loss: 0.24524 | Running loss: 0.75835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3172 | Classification loss: 0.11830 | Regression loss: 0.18029 | Running loss: 0.75508 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3173 | Classification loss: 0.60140 | Regression loss: 0.41995 | Running loss: 0.75609 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3174 | Classification loss: 0.34574 | Regression loss: 0.33134 | Running loss: 0.75484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3175 | Classification loss: 0.05717 | Regression loss: 0.23913 | Running loss: 0.75371 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3176 | Classification loss: 0.10086 | Regression loss: 0.28569 | Running loss: 0.75394 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3177 | Classification loss: 0.71252 | Regression loss: 0.63477 | Running loss: 0.75594 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3178 | Classification loss: 0.48429 | Regression loss: 0.31685 | Running loss: 0.75735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3179 | Classification loss: 0.00791 | Regression loss: 0.08586 | Running loss: 0.75631 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3180 | Classification loss: 0.73531 | Regression loss: 0.48760 | Running loss: 0.75831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3181 | Classification loss: 1.20252 | Regression loss: 0.45797 | Running loss: 0.76102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3182 | Classification loss: 0.34281 | Regression loss: 0.24102 | Running loss: 0.75826 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3183 | Classification loss: 0.28998 | Regression loss: 0.18488 | Running loss: 0.75671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3184 | Classification loss: 0.02677 | Regression loss: 0.15898 | Running loss: 0.75477 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3185 | Classification loss: 0.15529 | Regression loss: 0.18173 | Running loss: 0.75089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3186 | Classification loss: 0.00532 | Regression loss: 0.12762 | Running loss: 0.75064 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3187 | Classification loss: 0.04601 | Regression loss: 0.15670 | Running loss: 0.74803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3188 | Classification loss: 0.18976 | Regression loss: 0.39925 | Running loss: 0.74884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3189 | Classification loss: 0.37171 | Regression loss: 0.36755 | Running loss: 0.74990 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3190 | Classification loss: 0.15883 | Regression loss: 0.20894 | Running loss: 0.74986 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3191 | Classification loss: 0.18373 | Regression loss: 0.36483 | Running loss: 0.75063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3192 | Classification loss: 0.50197 | Regression loss: 0.51932 | Running loss: 0.75187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3193 | Classification loss: 0.01905 | Regression loss: 0.22749 | Running loss: 0.75209 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3194 | Classification loss: 0.00056 | Regression loss: 0.03755 | Running loss: 0.75088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3195 | Classification loss: 0.07153 | Regression loss: 0.14109 | Running loss: 0.74986 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3196 | Classification loss: 0.03887 | Regression loss: 0.20724 | Running loss: 0.74584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3197 | Classification loss: 2.05198 | Regression loss: 0.70866 | Running loss: 0.75122 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3198 | Classification loss: 0.72520 | Regression loss: 0.50886 | Running loss: 0.75323 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3199 | Classification loss: 0.06120 | Regression loss: 0.23066 | Running loss: 0.75381 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3200 | Classification loss: 0.06801 | Regression loss: 0.31176 | Running loss: 0.75205 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3201 | Classification loss: 0.55985 | Regression loss: 0.50611 | Running loss: 0.74908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3202 | Classification loss: 0.53040 | Regression loss: 0.42117 | Running loss: 0.74998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3203 | Classification loss: 0.10958 | Regression loss: 0.18449 | Running loss: 0.74881 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3204 | Classification loss: 0.02277 | Regression loss: 0.12308 | Running loss: 0.74726 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3205 | Classification loss: 0.61234 | Regression loss: 0.64825 | Running loss: 0.74909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3206 | Classification loss: 0.07973 | Regression loss: 0.26216 | Running loss: 0.74818 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3207 | Classification loss: 0.98000 | Regression loss: 0.56174 | Running loss: 0.75109 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3208 | Classification loss: 0.02342 | Regression loss: 0.04067 | Running loss: 0.75107 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3209 | Classification loss: 0.19357 | Regression loss: 0.29055 | Running loss: 0.75046 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3210 | Classification loss: 0.45883 | Regression loss: 0.51950 | Running loss: 0.74958 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3211 | Classification loss: 0.14835 | Regression loss: 0.10135 | Running loss: 0.74868 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3212 | Classification loss: 0.48697 | Regression loss: 0.46568 | Running loss: 0.75032 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3213 | Classification loss: 0.00530 | Regression loss: 0.14076 | Running loss: 0.74913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3214 | Classification loss: 0.21326 | Regression loss: 0.37913 | Running loss: 0.74896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3215 | Classification loss: 0.71441 | Regression loss: 0.54268 | Running loss: 0.75066 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3216 | Classification loss: 0.03228 | Regression loss: 0.08345 | Running loss: 0.74893 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3217 | Classification loss: 0.17545 | Regression loss: 0.31866 | Running loss: 0.74947 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3218 | Classification loss: 1.53374 | Regression loss: 0.53582 | Running loss: 0.75177 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3219 | Classification loss: 0.00902 | Regression loss: 0.08386 | Running loss: 0.75167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3220 | Classification loss: 0.04480 | Regression loss: 0.26605 | Running loss: 0.75128 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3221 | Classification loss: 0.42004 | Regression loss: 0.38786 | Running loss: 0.75048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3222 | Classification loss: 0.99070 | Regression loss: 0.43515 | Running loss: 0.74890 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3223 | Classification loss: 1.72163 | Regression loss: 0.56258 | Running loss: 0.75283 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3224 | Classification loss: 0.60351 | Regression loss: 0.29553 | Running loss: 0.75240 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3225 | Classification loss: 0.52300 | Regression loss: 0.46832 | Running loss: 0.75278 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3226 | Classification loss: 0.05530 | Regression loss: 0.30909 | Running loss: 0.75260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3227 | Classification loss: 1.21702 | Regression loss: 0.68363 | Running loss: 0.75299 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3228 | Classification loss: 0.26678 | Regression loss: 0.22135 | Running loss: 0.75261 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3229 | Classification loss: 0.02118 | Regression loss: 0.13078 | Running loss: 0.75273 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3230 | Classification loss: 0.06271 | Regression loss: 0.23810 | Running loss: 0.75186 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3231 | Classification loss: 0.37049 | Regression loss: 0.38896 | Running loss: 0.75193 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3232 | Classification loss: 0.96954 | Regression loss: 0.44138 | Running loss: 0.75242 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3233 | Classification loss: 0.85968 | Regression loss: 0.34325 | Running loss: 0.75117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3234 | Classification loss: 1.74171 | Regression loss: 0.50607 | Running loss: 0.75558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3235 | Classification loss: 0.01294 | Regression loss: 0.11391 | Running loss: 0.75491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3236 | Classification loss: 0.07905 | Regression loss: 0.17746 | Running loss: 0.75248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3237 | Classification loss: 0.02638 | Regression loss: 0.13868 | Running loss: 0.75227 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3238 | Classification loss: 0.16968 | Regression loss: 0.38918 | Running loss: 0.74957 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3239 | Classification loss: 0.92483 | Regression loss: 0.62519 | Running loss: 0.74803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3240 | Classification loss: 0.02292 | Regression loss: 0.10216 | Running loss: 0.74679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3241 | Classification loss: 0.26325 | Regression loss: 0.39294 | Running loss: 0.74522 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3242 | Classification loss: 0.01651 | Regression loss: 0.33371 | Running loss: 0.74575 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3243 | Classification loss: 0.41061 | Regression loss: 0.33127 | Running loss: 0.74511 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3244 | Classification loss: 1.07744 | Regression loss: 0.50123 | Running loss: 0.74716 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3245 | Classification loss: 0.00204 | Regression loss: 0.07910 | Running loss: 0.74368 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3246 | Classification loss: 0.16122 | Regression loss: 0.29358 | Running loss: 0.73922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3247 | Classification loss: 0.14571 | Regression loss: 0.33484 | Running loss: 0.73945 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3248 | Classification loss: 0.63075 | Regression loss: 0.64700 | Running loss: 0.74172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3249 | Classification loss: 0.09523 | Regression loss: 0.18741 | Running loss: 0.74161 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3250 | Classification loss: 0.04463 | Regression loss: 0.30695 | Running loss: 0.73945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3251 | Classification loss: 0.00762 | Regression loss: 0.20198 | Running loss: 0.73802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3252 | Classification loss: 0.03091 | Regression loss: 0.15183 | Running loss: 0.73749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3253 | Classification loss: 1.31910 | Regression loss: 0.61658 | Running loss: 0.73507 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3254 | Classification loss: 0.08734 | Regression loss: 0.13119 | Running loss: 0.73019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3255 | Classification loss: 0.01981 | Regression loss: 0.28584 | Running loss: 0.73040 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3256 | Classification loss: 0.65174 | Regression loss: 0.59915 | Running loss: 0.73158 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3257 | Classification loss: 0.62751 | Regression loss: 0.39172 | Running loss: 0.73265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3258 | Classification loss: 0.17090 | Regression loss: 0.35722 | Running loss: 0.73293 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3259 | Classification loss: 0.12964 | Regression loss: 0.12596 | Running loss: 0.72975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3260 | Classification loss: 0.14210 | Regression loss: 0.34532 | Running loss: 0.73068 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3261 | Classification loss: 0.04003 | Regression loss: 0.08593 | Running loss: 0.73007 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3262 | Classification loss: 0.08060 | Regression loss: 0.31810 | Running loss: 0.73055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3263 | Classification loss: 0.07540 | Regression loss: 0.23432 | Running loss: 0.73074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3264 | Classification loss: 0.48365 | Regression loss: 0.58197 | Running loss: 0.73259 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3265 | Classification loss: 0.25048 | Regression loss: 0.38263 | Running loss: 0.73225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3266 | Classification loss: 0.01634 | Regression loss: 0.23838 | Running loss: 0.73169 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3267 | Classification loss: 0.14046 | Regression loss: 0.37368 | Running loss: 0.73085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3268 | Classification loss: 1.26794 | Regression loss: 0.52649 | Running loss: 0.73325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3269 | Classification loss: 1.10899 | Regression loss: 0.57027 | Running loss: 0.73655 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3270 | Classification loss: 0.81320 | Regression loss: 0.40046 | Running loss: 0.73808 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3271 | Classification loss: 0.02116 | Regression loss: 0.12063 | Running loss: 0.73795 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3272 | Classification loss: 0.13691 | Regression loss: 0.41315 | Running loss: 0.73751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3273 | Classification loss: 0.00355 | Regression loss: 0.10759 | Running loss: 0.73553 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3274 | Classification loss: 0.86274 | Regression loss: 0.66861 | Running loss: 0.73483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3275 | Classification loss: 1.64076 | Regression loss: 0.32597 | Running loss: 0.73706 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3276 | Classification loss: 0.16470 | Regression loss: 0.16609 | Running loss: 0.73749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3277 | Classification loss: 1.14839 | Regression loss: 0.59391 | Running loss: 0.73891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3278 | Classification loss: 0.36669 | Regression loss: 0.42469 | Running loss: 0.73687 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3279 | Classification loss: 0.07237 | Regression loss: 0.14237 | Running loss: 0.73571 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3280 | Classification loss: 0.51131 | Regression loss: 0.51151 | Running loss: 0.73619 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3281 | Classification loss: 0.10807 | Regression loss: 0.08128 | Running loss: 0.73435 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3282 | Classification loss: 1.40936 | Regression loss: 0.29715 | Running loss: 0.73595 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3283 | Classification loss: 1.11720 | Regression loss: 0.60598 | Running loss: 0.73664 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3284 | Classification loss: 0.18646 | Regression loss: 0.29925 | Running loss: 0.73698 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3285 | Classification loss: 0.01947 | Regression loss: 0.11640 | Running loss: 0.73617 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3286 | Classification loss: 2.31523 | Regression loss: 0.55435 | Running loss: 0.74155 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3287 | Classification loss: 0.00151 | Regression loss: 0.14772 | Running loss: 0.74067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3288 | Classification loss: 0.01583 | Regression loss: 0.12398 | Running loss: 0.73842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3289 | Classification loss: 0.01545 | Regression loss: 0.22191 | Running loss: 0.73613 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3290 | Classification loss: 0.24154 | Regression loss: 0.48440 | Running loss: 0.73423 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3291 | Classification loss: 0.11357 | Regression loss: 0.26763 | Running loss: 0.73446 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3292 | Classification loss: 0.02552 | Regression loss: 0.16031 | Running loss: 0.73340 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3293 | Classification loss: 0.96013 | Regression loss: 0.53888 | Running loss: 0.73208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3294 | Classification loss: 0.75974 | Regression loss: 0.60962 | Running loss: 0.73294 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3295 | Classification loss: 0.05183 | Regression loss: 0.04443 | Running loss: 0.73055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3296 | Classification loss: 0.04787 | Regression loss: 0.23200 | Running loss: 0.72985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3297 | Classification loss: 0.48387 | Regression loss: 0.49548 | Running loss: 0.73167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3298 | Classification loss: 0.19595 | Regression loss: 0.20871 | Running loss: 0.73112 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3299 | Classification loss: 0.45557 | Regression loss: 0.48660 | Running loss: 0.73263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3300 | Classification loss: 0.01398 | Regression loss: 0.17270 | Running loss: 0.73143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3301 | Classification loss: 0.17968 | Regression loss: 0.16300 | Running loss: 0.73048 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3302 | Classification loss: 0.35659 | Regression loss: 0.49910 | Running loss: 0.73199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3303 | Classification loss: 0.00104 | Regression loss: 0.11138 | Running loss: 0.72956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3304 | Classification loss: 0.46723 | Regression loss: 0.40440 | Running loss: 0.72916 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3305 | Classification loss: 0.33165 | Regression loss: 0.15408 | Running loss: 0.72991 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3306 | Classification loss: 0.12355 | Regression loss: 0.31778 | Running loss: 0.73000 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3307 | Classification loss: 1.40319 | Regression loss: 0.58369 | Running loss: 0.73157 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3308 | Classification loss: 0.03961 | Regression loss: 0.06294 | Running loss: 0.72920 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3309 | Classification loss: 0.00475 | Regression loss: 0.09709 | Running loss: 0.72833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3310 | Classification loss: 0.18471 | Regression loss: 0.24160 | Running loss: 0.72872 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3311 | Classification loss: 0.55089 | Regression loss: 0.65545 | Running loss: 0.73070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3312 | Classification loss: 0.25757 | Regression loss: 0.41250 | Running loss: 0.73125 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3313 | Classification loss: 0.10777 | Regression loss: 0.39283 | Running loss: 0.72747 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3314 | Classification loss: 0.04335 | Regression loss: 0.15356 | Running loss: 0.72759 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3315 | Classification loss: 0.13036 | Regression loss: 0.32301 | Running loss: 0.72817 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3316 | Classification loss: 0.00280 | Regression loss: 0.10279 | Running loss: 0.72831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3317 | Classification loss: 0.50867 | Regression loss: 0.49713 | Running loss: 0.72610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3318 | Classification loss: 0.82875 | Regression loss: 0.52724 | Running loss: 0.72434 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3319 | Classification loss: 0.11653 | Regression loss: 0.16152 | Running loss: 0.72355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3320 | Classification loss: 0.01007 | Regression loss: 0.10175 | Running loss: 0.72303 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3321 | Classification loss: 0.18412 | Regression loss: 0.19362 | Running loss: 0.72266 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3322 | Classification loss: 0.04170 | Regression loss: 0.05779 | Running loss: 0.71971 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3323 | Classification loss: 0.87254 | Regression loss: 0.47018 | Running loss: 0.72001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3324 | Classification loss: 1.07140 | Regression loss: 0.60389 | Running loss: 0.72111 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3325 | Classification loss: 1.03442 | Regression loss: 0.55954 | Running loss: 0.72263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3326 | Classification loss: 0.94528 | Regression loss: 0.61336 | Running loss: 0.72449 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3327 | Classification loss: 0.40884 | Regression loss: 0.23045 | Running loss: 0.72536 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3328 | Classification loss: 0.79013 | Regression loss: 0.39242 | Running loss: 0.72730 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3329 | Classification loss: 0.51057 | Regression loss: 0.48033 | Running loss: 0.72744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3330 | Classification loss: 1.60724 | Regression loss: 0.51521 | Running loss: 0.72911 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3331 | Classification loss: 0.42713 | Regression loss: 0.36710 | Running loss: 0.73002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3332 | Classification loss: 0.18462 | Regression loss: 0.43860 | Running loss: 0.72979 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3333 | Classification loss: 0.00174 | Regression loss: 0.08489 | Running loss: 0.72968 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3334 | Classification loss: 0.29186 | Regression loss: 0.23214 | Running loss: 0.73014 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3335 | Classification loss: 0.31367 | Regression loss: 0.18648 | Running loss: 0.72963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3336 | Classification loss: 1.95429 | Regression loss: 0.41773 | Running loss: 0.73098 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3337 | Classification loss: 0.03664 | Regression loss: 0.24346 | Running loss: 0.73056 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3338 | Classification loss: 0.05433 | Regression loss: 0.20219 | Running loss: 0.72882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3339 | Classification loss: 0.00289 | Regression loss: 0.12528 | Running loss: 0.72891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3340 | Classification loss: 1.28982 | Regression loss: 0.58558 | Running loss: 0.73213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3341 | Classification loss: 0.01670 | Regression loss: 0.08556 | Running loss: 0.73014 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3342 | Classification loss: 0.93780 | Regression loss: 0.22767 | Running loss: 0.73190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3343 | Classification loss: 0.75798 | Regression loss: 0.33394 | Running loss: 0.73348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3344 | Classification loss: 0.51382 | Regression loss: 0.40449 | Running loss: 0.73067 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3345 | Classification loss: 0.01140 | Regression loss: 0.16674 | Running loss: 0.73012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3346 | Classification loss: 0.14789 | Regression loss: 0.13523 | Running loss: 0.73063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3347 | Classification loss: 0.15288 | Regression loss: 0.30258 | Running loss: 0.73051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3348 | Classification loss: 0.02252 | Regression loss: 0.23709 | Running loss: 0.72675 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3349 | Classification loss: 0.14781 | Regression loss: 0.19067 | Running loss: 0.72708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3350 | Classification loss: 0.09149 | Regression loss: 0.19674 | Running loss: 0.72727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3351 | Classification loss: 1.32222 | Regression loss: 0.26442 | Running loss: 0.73005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3352 | Classification loss: 0.25859 | Regression loss: 0.48058 | Running loss: 0.73076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3353 | Classification loss: 0.00853 | Regression loss: 0.15597 | Running loss: 0.73083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3354 | Classification loss: 0.13649 | Regression loss: 0.47709 | Running loss: 0.73132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3355 | Classification loss: 2.66847 | Regression loss: 0.71844 | Running loss: 0.73771 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3356 | Classification loss: 0.22191 | Regression loss: 0.34845 | Running loss: 0.73823 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3357 | Classification loss: 0.37420 | Regression loss: 0.45810 | Running loss: 0.73744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3358 | Classification loss: 0.01186 | Regression loss: 0.07361 | Running loss: 0.73721 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3359 | Classification loss: 2.33597 | Regression loss: 0.67360 | Running loss: 0.74291 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3360 | Classification loss: 0.38783 | Regression loss: 0.16084 | Running loss: 0.74187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3361 | Classification loss: 0.39642 | Regression loss: 0.31138 | Running loss: 0.74260 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3362 | Classification loss: 0.97091 | Regression loss: 0.52746 | Running loss: 0.74220 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3363 | Classification loss: 0.00066 | Regression loss: 0.02406 | Running loss: 0.74041 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3364 | Classification loss: 0.22891 | Regression loss: 0.36488 | Running loss: 0.73708 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3365 | Classification loss: 1.13675 | Regression loss: 0.45820 | Running loss: 0.73947 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3366 | Classification loss: 0.37710 | Regression loss: 0.21772 | Running loss: 0.73862 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3367 | Classification loss: 0.44081 | Regression loss: 0.19675 | Running loss: 0.73264 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3368 | Classification loss: 0.02236 | Regression loss: 0.11686 | Running loss: 0.73174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3369 | Classification loss: 0.15175 | Regression loss: 0.30276 | Running loss: 0.73120 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3370 | Classification loss: 0.10493 | Regression loss: 0.28385 | Running loss: 0.72882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3371 | Classification loss: 0.05391 | Regression loss: 0.30295 | Running loss: 0.72865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3372 | Classification loss: 0.03854 | Regression loss: 0.23818 | Running loss: 0.72715 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3373 | Classification loss: 1.03112 | Regression loss: 0.47773 | Running loss: 0.72881 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3374 | Classification loss: 0.52789 | Regression loss: 0.44698 | Running loss: 0.73046 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3375 | Classification loss: 2.56686 | Regression loss: 0.53214 | Running loss: 0.73582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3376 | Classification loss: 0.01545 | Regression loss: 0.07726 | Running loss: 0.73491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3377 | Classification loss: 0.07192 | Regression loss: 0.51641 | Running loss: 0.73576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3378 | Classification loss: 0.88625 | Regression loss: 0.56919 | Running loss: 0.73625 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3379 | Classification loss: 2.70324 | Regression loss: 0.70414 | Running loss: 0.74285 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3380 | Classification loss: 0.05857 | Regression loss: 0.16899 | Running loss: 0.74321 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3381 | Classification loss: 0.04989 | Regression loss: 0.15318 | Running loss: 0.74292 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3382 | Classification loss: 0.08111 | Regression loss: 0.31177 | Running loss: 0.74152 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3383 | Classification loss: 0.04827 | Regression loss: 0.11291 | Running loss: 0.74167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3384 | Classification loss: 0.00577 | Regression loss: 0.16003 | Running loss: 0.74124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3385 | Classification loss: 0.54476 | Regression loss: 0.59142 | Running loss: 0.74136 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3386 | Classification loss: 0.03215 | Regression loss: 0.12511 | Running loss: 0.74015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3387 | Classification loss: 1.13336 | Regression loss: 0.99163 | Running loss: 0.74292 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3388 | Classification loss: 0.75740 | Regression loss: 0.62515 | Running loss: 0.74528 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3389 | Classification loss: 0.71986 | Regression loss: 0.37966 | Running loss: 0.74538 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3390 | Classification loss: 0.73042 | Regression loss: 0.47835 | Running loss: 0.74724 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3391 | Classification loss: 0.11887 | Regression loss: 0.30000 | Running loss: 0.74681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3392 | Classification loss: 0.41352 | Regression loss: 0.33407 | Running loss: 0.74770 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3393 | Classification loss: 0.07090 | Regression loss: 0.15007 | Running loss: 0.74728 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3394 | Classification loss: 0.02198 | Regression loss: 0.13439 | Running loss: 0.74631 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3395 | Classification loss: 0.27851 | Regression loss: 0.28320 | Running loss: 0.74413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3396 | Classification loss: 0.00027 | Regression loss: 0.07256 | Running loss: 0.74225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3397 | Classification loss: 0.41083 | Regression loss: 0.23455 | Running loss: 0.74248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3398 | Classification loss: 0.21852 | Regression loss: 0.38596 | Running loss: 0.74118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3399 | Classification loss: 0.34152 | Regression loss: 0.21779 | Running loss: 0.74173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3400 | Classification loss: 0.28644 | Regression loss: 0.27271 | Running loss: 0.74221 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3401 | Classification loss: 0.17288 | Regression loss: 0.30586 | Running loss: 0.74220 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3402 | Classification loss: 0.51050 | Regression loss: 0.64064 | Running loss: 0.74419 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3403 | Classification loss: 0.02373 | Regression loss: 0.14396 | Running loss: 0.74244 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3404 | Classification loss: 0.18685 | Regression loss: 0.39199 | Running loss: 0.74296 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3405 | Classification loss: 0.55739 | Regression loss: 0.31236 | Running loss: 0.74079 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3406 | Classification loss: 0.91234 | Regression loss: 0.52119 | Running loss: 0.73998 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3407 | Classification loss: 0.06686 | Regression loss: 0.24748 | Running loss: 0.74031 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3408 | Classification loss: 0.30232 | Regression loss: 0.08167 | Running loss: 0.74022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3409 | Classification loss: 0.73415 | Regression loss: 0.51750 | Running loss: 0.73805 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3410 | Classification loss: 1.20302 | Regression loss: 0.48240 | Running loss: 0.74073 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3411 | Classification loss: 0.09166 | Regression loss: 0.18871 | Running loss: 0.73938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3412 | Classification loss: 0.59479 | Regression loss: 0.51556 | Running loss: 0.73905 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3413 | Classification loss: 0.03525 | Regression loss: 0.34961 | Running loss: 0.73952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3414 | Classification loss: 0.59943 | Regression loss: 0.38553 | Running loss: 0.74095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3415 | Classification loss: 0.58917 | Regression loss: 0.32689 | Running loss: 0.74207 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3416 | Classification loss: 0.00702 | Regression loss: 0.17194 | Running loss: 0.74204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3417 | Classification loss: 0.48465 | Regression loss: 0.32775 | Running loss: 0.74005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3418 | Classification loss: 0.18983 | Regression loss: 0.17556 | Running loss: 0.74076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3419 | Classification loss: 0.61014 | Regression loss: 0.38071 | Running loss: 0.74249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3420 | Classification loss: 0.28191 | Regression loss: 0.39412 | Running loss: 0.74227 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3421 | Classification loss: 0.00059 | Regression loss: 0.13802 | Running loss: 0.73889 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3422 | Classification loss: 0.00620 | Regression loss: 0.10166 | Running loss: 0.73857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3423 | Classification loss: 1.03571 | Regression loss: 0.43567 | Running loss: 0.74147 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3424 | Classification loss: 0.70982 | Regression loss: 0.54748 | Running loss: 0.74176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3425 | Classification loss: 0.08547 | Regression loss: 0.08504 | Running loss: 0.73935 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3426 | Classification loss: 0.22535 | Regression loss: 0.55482 | Running loss: 0.73955 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3427 | Classification loss: 0.00813 | Regression loss: 0.12981 | Running loss: 0.73942 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3428 | Classification loss: 0.52870 | Regression loss: 0.29011 | Running loss: 0.74000 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3429 | Classification loss: 0.60963 | Regression loss: 0.40564 | Running loss: 0.73789 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3430 | Classification loss: 0.00730 | Regression loss: 0.03729 | Running loss: 0.73650 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3431 | Classification loss: 0.00063 | Regression loss: 0.15509 | Running loss: 0.73311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3432 | Classification loss: 0.09390 | Regression loss: 0.20266 | Running loss: 0.73294 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3433 | Classification loss: 0.22998 | Regression loss: 0.13689 | Running loss: 0.73158 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3434 | Classification loss: 0.32699 | Regression loss: 0.43796 | Running loss: 0.73258 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3435 | Classification loss: 0.95921 | Regression loss: 0.55005 | Running loss: 0.73538 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3436 | Classification loss: 1.20979 | Regression loss: 0.74961 | Running loss: 0.73667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3437 | Classification loss: 0.59723 | Regression loss: 0.44308 | Running loss: 0.73805 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3438 | Classification loss: 0.58842 | Regression loss: 0.25457 | Running loss: 0.73338 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3439 | Classification loss: 0.75188 | Regression loss: 0.39669 | Running loss: 0.73529 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3440 | Classification loss: 1.29539 | Regression loss: 0.68198 | Running loss: 0.73836 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3441 | Classification loss: 0.10433 | Regression loss: 0.40710 | Running loss: 0.73682 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3442 | Classification loss: 0.10270 | Regression loss: 0.33209 | Running loss: 0.73692 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3443 | Classification loss: 0.01834 | Regression loss: 0.18029 | Running loss: 0.73543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3444 | Classification loss: 0.00216 | Regression loss: 0.07651 | Running loss: 0.73424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3445 | Classification loss: 0.09075 | Regression loss: 0.44513 | Running loss: 0.73488 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3446 | Classification loss: 0.13029 | Regression loss: 0.08584 | Running loss: 0.73503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3447 | Classification loss: 1.56258 | Regression loss: 0.37744 | Running loss: 0.73688 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3448 | Classification loss: 0.27308 | Regression loss: 0.34051 | Running loss: 0.73756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3449 | Classification loss: 0.15202 | Regression loss: 0.32489 | Running loss: 0.73676 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3450 | Classification loss: 0.16563 | Regression loss: 0.26433 | Running loss: 0.73668 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3451 | Classification loss: 0.00061 | Regression loss: 0.09213 | Running loss: 0.73511 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3452 | Classification loss: 0.25509 | Regression loss: 0.47349 | Running loss: 0.73577 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3453 | Classification loss: 0.78769 | Regression loss: 0.62109 | Running loss: 0.73799 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3454 | Classification loss: 0.41020 | Regression loss: 0.19320 | Running loss: 0.73894 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3455 | Classification loss: 0.04970 | Regression loss: 0.17937 | Running loss: 0.73785 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3456 | Classification loss: 1.22309 | Regression loss: 0.44288 | Running loss: 0.73811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3457 | Classification loss: 0.47395 | Regression loss: 0.25566 | Running loss: 0.73797 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3458 | Classification loss: 0.38190 | Regression loss: 0.27685 | Running loss: 0.73756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3459 | Classification loss: 0.02508 | Regression loss: 0.20342 | Running loss: 0.73634 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3460 | Classification loss: 0.75931 | Regression loss: 0.48292 | Running loss: 0.73686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3461 | Classification loss: 0.79760 | Regression loss: 0.56216 | Running loss: 0.73846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3462 | Classification loss: 1.90379 | Regression loss: 0.43045 | Running loss: 0.74218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3463 | Classification loss: 0.10523 | Regression loss: 0.43529 | Running loss: 0.74253 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3464 | Classification loss: 0.04299 | Regression loss: 0.21030 | Running loss: 0.74155 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3465 | Classification loss: 0.18602 | Regression loss: 0.30078 | Running loss: 0.74152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3466 | Classification loss: 0.00430 | Regression loss: 0.09412 | Running loss: 0.73852 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3467 | Classification loss: 0.04816 | Regression loss: 0.31560 | Running loss: 0.73874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3468 | Classification loss: 0.00020 | Regression loss: 0.08216 | Running loss: 0.73660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3469 | Classification loss: 0.03073 | Regression loss: 0.27741 | Running loss: 0.73695 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3470 | Classification loss: 0.12417 | Regression loss: 0.09370 | Running loss: 0.73512 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3471 | Classification loss: 0.35112 | Regression loss: 0.33743 | Running loss: 0.73527 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3472 | Classification loss: 0.00764 | Regression loss: 0.13943 | Running loss: 0.73529 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3473 | Classification loss: 0.01929 | Regression loss: 0.23761 | Running loss: 0.73545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3474 | Classification loss: 1.12146 | Regression loss: 0.62715 | Running loss: 0.73648 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3475 | Classification loss: 0.00197 | Regression loss: 0.02543 | Running loss: 0.73577 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3476 | Classification loss: 0.00982 | Regression loss: 0.17811 | Running loss: 0.73606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3477 | Classification loss: 0.61459 | Regression loss: 0.44615 | Running loss: 0.73780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3478 | Classification loss: 0.12584 | Regression loss: 0.28570 | Running loss: 0.73788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3479 | Classification loss: 0.65157 | Regression loss: 0.46312 | Running loss: 0.73808 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3480 | Classification loss: 0.39835 | Regression loss: 0.29777 | Running loss: 0.73712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3481 | Classification loss: 0.00370 | Regression loss: 0.07359 | Running loss: 0.73483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3482 | Classification loss: 0.14941 | Regression loss: 0.06269 | Running loss: 0.73384 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3483 | Classification loss: 0.15618 | Regression loss: 0.20134 | Running loss: 0.73324 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3484 | Classification loss: 1.17990 | Regression loss: 0.50214 | Running loss: 0.73539 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3485 | Classification loss: 0.13943 | Regression loss: 0.19436 | Running loss: 0.73605 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3486 | Classification loss: 0.31819 | Regression loss: 0.18610 | Running loss: 0.73439 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3487 | Classification loss: 0.09971 | Regression loss: 0.18363 | Running loss: 0.73441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3488 | Classification loss: 0.30516 | Regression loss: 0.41957 | Running loss: 0.73284 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3489 | Classification loss: 0.10721 | Regression loss: 0.34766 | Running loss: 0.73095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3490 | Classification loss: 0.04833 | Regression loss: 0.19405 | Running loss: 0.72947 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3491 | Classification loss: 1.20703 | Regression loss: 0.74449 | Running loss: 0.73260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3492 | Classification loss: 0.50005 | Regression loss: 0.42555 | Running loss: 0.73427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3493 | Classification loss: 0.11585 | Regression loss: 0.36337 | Running loss: 0.73156 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3494 | Classification loss: 0.00072 | Regression loss: 0.04995 | Running loss: 0.73026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3495 | Classification loss: 0.70356 | Regression loss: 0.13049 | Running loss: 0.72955 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3496 | Classification loss: 0.32265 | Regression loss: 0.38789 | Running loss: 0.72737 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3497 | Classification loss: 0.22698 | Regression loss: 0.20791 | Running loss: 0.72592 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3498 | Classification loss: 0.84113 | Regression loss: 0.48501 | Running loss: 0.72717 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3499 | Classification loss: 0.65761 | Regression loss: 0.85192 | Running loss: 0.72973 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3500 | Classification loss: 0.17366 | Regression loss: 0.43360 | Running loss: 0.72925 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3501 | Classification loss: 0.20817 | Regression loss: 0.24558 | Running loss: 0.72889 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3502 | Classification loss: 0.10333 | Regression loss: 0.35662 | Running loss: 0.72806 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3503 | Classification loss: 0.17523 | Regression loss: 0.07815 | Running loss: 0.72692 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3504 | Classification loss: 1.59811 | Regression loss: 0.56679 | Running loss: 0.73049 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3505 | Classification loss: 0.55147 | Regression loss: 0.51563 | Running loss: 0.73176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3506 | Classification loss: 0.96282 | Regression loss: 0.67892 | Running loss: 0.73443 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3507 | Classification loss: 1.46307 | Regression loss: 0.48031 | Running loss: 0.73671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3508 | Classification loss: 0.66537 | Regression loss: 0.44393 | Running loss: 0.73585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3509 | Classification loss: 0.34687 | Regression loss: 0.45358 | Running loss: 0.73332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3510 | Classification loss: 0.53803 | Regression loss: 0.39944 | Running loss: 0.73353 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3511 | Classification loss: 1.03914 | Regression loss: 0.33959 | Running loss: 0.73336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3512 | Classification loss: 0.00700 | Regression loss: 0.14741 | Running loss: 0.73175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3513 | Classification loss: 0.08788 | Regression loss: 0.24503 | Running loss: 0.73128 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3514 | Classification loss: 0.25563 | Regression loss: 0.58743 | Running loss: 0.73267 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3515 | Classification loss: 0.37779 | Regression loss: 0.16204 | Running loss: 0.73023 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3516 | Classification loss: 0.85362 | Regression loss: 0.29715 | Running loss: 0.73047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3517 | Classification loss: 0.27500 | Regression loss: 0.51899 | Running loss: 0.73099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3518 | Classification loss: 0.12721 | Regression loss: 0.28242 | Running loss: 0.73157 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3519 | Classification loss: 0.10066 | Regression loss: 0.07140 | Running loss: 0.73069 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3520 | Classification loss: 0.00558 | Regression loss: 0.13883 | Running loss: 0.73035 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3521 | Classification loss: 0.04307 | Regression loss: 0.13457 | Running loss: 0.72949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3522 | Classification loss: 0.84519 | Regression loss: 0.49960 | Running loss: 0.73184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3523 | Classification loss: 0.39752 | Regression loss: 0.39932 | Running loss: 0.73269 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3524 | Classification loss: 0.34302 | Regression loss: 0.25136 | Running loss: 0.73166 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3525 | Classification loss: 0.58338 | Regression loss: 0.53920 | Running loss: 0.73279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3526 | Classification loss: 0.11089 | Regression loss: 0.27268 | Running loss: 0.73196 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3527 | Classification loss: 0.46889 | Regression loss: 0.46350 | Running loss: 0.73174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3528 | Classification loss: 0.25155 | Regression loss: 0.40749 | Running loss: 0.73142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3529 | Classification loss: 0.00123 | Regression loss: 0.15735 | Running loss: 0.72762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3530 | Classification loss: 0.02617 | Regression loss: 0.03649 | Running loss: 0.72631 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3531 | Classification loss: 1.24661 | Regression loss: 0.53673 | Running loss: 0.72659 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3532 | Classification loss: 1.26734 | Regression loss: 0.58800 | Running loss: 0.72867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3533 | Classification loss: 0.78773 | Regression loss: 0.53624 | Running loss: 0.73051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3534 | Classification loss: 0.04396 | Regression loss: 0.21941 | Running loss: 0.72919 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3535 | Classification loss: 0.32572 | Regression loss: 0.37023 | Running loss: 0.73028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3536 | Classification loss: 0.93156 | Regression loss: 0.41836 | Running loss: 0.72948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3537 | Classification loss: 0.07575 | Regression loss: 0.16987 | Running loss: 0.72884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3538 | Classification loss: 0.92076 | Regression loss: 0.32271 | Running loss: 0.72975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3539 | Classification loss: 0.23344 | Regression loss: 0.34290 | Running loss: 0.73074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3540 | Classification loss: 0.35286 | Regression loss: 0.33607 | Running loss: 0.73049 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3541 | Classification loss: 0.73738 | Regression loss: 0.61130 | Running loss: 0.73290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3542 | Classification loss: 1.25129 | Regression loss: 0.25384 | Running loss: 0.73555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3543 | Classification loss: 0.51512 | Regression loss: 0.29098 | Running loss: 0.73613 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3544 | Classification loss: 0.62137 | Regression loss: 0.51442 | Running loss: 0.73634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3545 | Classification loss: 0.02338 | Regression loss: 0.34109 | Running loss: 0.73402 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3546 | Classification loss: 0.35154 | Regression loss: 0.27228 | Running loss: 0.73120 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3547 | Classification loss: 1.30861 | Regression loss: 0.56645 | Running loss: 0.73436 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3548 | Classification loss: 0.06380 | Regression loss: 0.27156 | Running loss: 0.73441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3549 | Classification loss: 0.01454 | Regression loss: 0.10609 | Running loss: 0.73343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3550 | Classification loss: 0.00062 | Regression loss: 0.10836 | Running loss: 0.72940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3551 | Classification loss: 2.18551 | Regression loss: 0.92007 | Running loss: 0.73520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3552 | Classification loss: 0.00504 | Regression loss: 0.17956 | Running loss: 0.73468 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3553 | Classification loss: 0.00032 | Regression loss: 0.12360 | Running loss: 0.73464 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3554 | Classification loss: 0.42251 | Regression loss: 0.39282 | Running loss: 0.73561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3555 | Classification loss: 0.02267 | Regression loss: 0.07086 | Running loss: 0.73539 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3556 | Classification loss: 2.13748 | Regression loss: 0.82077 | Running loss: 0.74094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3557 | Classification loss: 0.49885 | Regression loss: 0.36621 | Running loss: 0.74109 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3558 | Classification loss: 1.47517 | Regression loss: 0.33112 | Running loss: 0.74459 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3559 | Classification loss: 0.84797 | Regression loss: 0.51181 | Running loss: 0.74681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3560 | Classification loss: 0.73375 | Regression loss: 0.65760 | Running loss: 0.74751 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3561 | Classification loss: 0.20527 | Regression loss: 0.18773 | Running loss: 0.74750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3562 | Classification loss: 1.27046 | Regression loss: 0.53576 | Running loss: 0.74850 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3563 | Classification loss: 0.02726 | Regression loss: 0.17839 | Running loss: 0.74589 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3564 | Classification loss: 1.36753 | Regression loss: 0.74927 | Running loss: 0.74938 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3565 | Classification loss: 0.33665 | Regression loss: 0.45050 | Running loss: 0.74831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3566 | Classification loss: 0.04902 | Regression loss: 0.16865 | Running loss: 0.74670 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3567 | Classification loss: 0.23518 | Regression loss: 0.23591 | Running loss: 0.74337 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3568 | Classification loss: 0.00945 | Regression loss: 0.09681 | Running loss: 0.74307 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3569 | Classification loss: 0.02695 | Regression loss: 0.21060 | Running loss: 0.74161 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3570 | Classification loss: 0.80816 | Regression loss: 0.51422 | Running loss: 0.74425 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3571 | Classification loss: 0.58259 | Regression loss: 0.39638 | Running loss: 0.74474 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3572 | Classification loss: 0.19738 | Regression loss: 0.28717 | Running loss: 0.74436 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3573 | Classification loss: 1.22625 | Regression loss: 0.45382 | Running loss: 0.74675 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3574 | Classification loss: 0.20268 | Regression loss: 0.18158 | Running loss: 0.74634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3575 | Classification loss: 0.54701 | Regression loss: 0.26447 | Running loss: 0.74783 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3576 | Classification loss: 1.18175 | Regression loss: 0.49421 | Running loss: 0.74930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3577 | Classification loss: 0.41789 | Regression loss: 0.42237 | Running loss: 0.74740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3578 | Classification loss: 0.50771 | Regression loss: 0.13922 | Running loss: 0.74749 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3579 | Classification loss: 0.32598 | Regression loss: 0.33556 | Running loss: 0.74703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3580 | Classification loss: 0.69570 | Regression loss: 0.40568 | Running loss: 0.74742 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3581 | Classification loss: 0.09581 | Regression loss: 0.17581 | Running loss: 0.74567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3582 | Classification loss: 0.06326 | Regression loss: 0.18946 | Running loss: 0.74460 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3583 | Classification loss: 0.06267 | Regression loss: 0.11474 | Running loss: 0.74290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3584 | Classification loss: 0.62681 | Regression loss: 0.27104 | Running loss: 0.74385 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3585 | Classification loss: 0.21315 | Regression loss: 0.29625 | Running loss: 0.74085 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3586 | Classification loss: 0.74578 | Regression loss: 0.58824 | Running loss: 0.74267 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3587 | Classification loss: 0.18990 | Regression loss: 0.23408 | Running loss: 0.74265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3588 | Classification loss: 1.29416 | Regression loss: 0.67033 | Running loss: 0.74458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3589 | Classification loss: 0.95884 | Regression loss: 0.67470 | Running loss: 0.74715 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3590 | Classification loss: 0.04988 | Regression loss: 0.29768 | Running loss: 0.74524 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3591 | Classification loss: 0.10729 | Regression loss: 0.19127 | Running loss: 0.74389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3592 | Classification loss: 0.22069 | Regression loss: 0.33931 | Running loss: 0.73927 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3593 | Classification loss: 0.00085 | Regression loss: 0.00000 | Running loss: 0.73916 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3594 | Classification loss: 0.05355 | Regression loss: 0.09158 | Running loss: 0.73712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3595 | Classification loss: 0.64667 | Regression loss: 0.40258 | Running loss: 0.73883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3596 | Classification loss: 0.16009 | Regression loss: 0.16372 | Running loss: 0.73727 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3597 | Classification loss: 0.36079 | Regression loss: 0.62458 | Running loss: 0.73789 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3598 | Classification loss: 1.92939 | Regression loss: 0.70806 | Running loss: 0.73920 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3599 | Classification loss: 0.85792 | Regression loss: 0.38454 | Running loss: 0.74124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3600 | Classification loss: 0.74272 | Regression loss: 0.53654 | Running loss: 0.74064 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3601 | Classification loss: 0.00549 | Regression loss: 0.13507 | Running loss: 0.73754 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3602 | Classification loss: 0.90140 | Regression loss: 0.43354 | Running loss: 0.73855 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3603 | Classification loss: 0.07969 | Regression loss: 0.16698 | Running loss: 0.73598 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3604 | Classification loss: 0.26402 | Regression loss: 0.08625 | Running loss: 0.73487 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3605 | Classification loss: 0.05431 | Regression loss: 0.07648 | Running loss: 0.73062 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3606 | Classification loss: 0.83681 | Regression loss: 0.51111 | Running loss: 0.73071 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3607 | Classification loss: 0.06001 | Regression loss: 0.43319 | Running loss: 0.73123 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3608 | Classification loss: 0.45818 | Regression loss: 0.68493 | Running loss: 0.73268 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3609 | Classification loss: 0.02114 | Regression loss: 0.18486 | Running loss: 0.73174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3610 | Classification loss: 1.21457 | Regression loss: 0.28737 | Running loss: 0.73295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3611 | Classification loss: 0.33873 | Regression loss: 0.30258 | Running loss: 0.73419 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3612 | Classification loss: 0.04280 | Regression loss: 0.22070 | Running loss: 0.73425 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3613 | Classification loss: 0.61604 | Regression loss: 0.55714 | Running loss: 0.73614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3614 | Classification loss: 0.18488 | Regression loss: 0.31488 | Running loss: 0.73689 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3615 | Classification loss: 0.58689 | Regression loss: 0.44655 | Running loss: 0.73878 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3616 | Classification loss: 0.45815 | Regression loss: 0.46113 | Running loss: 0.74013 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3617 | Classification loss: 0.01016 | Regression loss: 0.28543 | Running loss: 0.73847 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3618 | Classification loss: 0.00951 | Regression loss: 0.28914 | Running loss: 0.73858 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3619 | Classification loss: 0.03584 | Regression loss: 0.21741 | Running loss: 0.73757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3620 | Classification loss: 0.00297 | Regression loss: 0.08846 | Running loss: 0.73663 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3621 | Classification loss: 1.04146 | Regression loss: 0.40924 | Running loss: 0.73924 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3622 | Classification loss: 0.17862 | Regression loss: 0.22300 | Running loss: 0.73972 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3623 | Classification loss: 0.00210 | Regression loss: 0.08344 | Running loss: 0.73811 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3624 | Classification loss: 0.40942 | Regression loss: 0.61061 | Running loss: 0.73694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3625 | Classification loss: 0.17815 | Regression loss: 0.38027 | Running loss: 0.73588 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3626 | Classification loss: 0.46670 | Regression loss: 0.63844 | Running loss: 0.73789 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3627 | Classification loss: 0.12520 | Regression loss: 0.51899 | Running loss: 0.73853 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3628 | Classification loss: 0.00153 | Regression loss: 0.08340 | Running loss: 0.73830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3629 | Classification loss: 0.00633 | Regression loss: 0.14231 | Running loss: 0.73787 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3630 | Classification loss: 0.16956 | Regression loss: 0.27330 | Running loss: 0.73750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3631 | Classification loss: 0.14579 | Regression loss: 0.31099 | Running loss: 0.73575 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3632 | Classification loss: 0.15019 | Regression loss: 0.27645 | Running loss: 0.73574 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3633 | Classification loss: 0.37829 | Regression loss: 0.35974 | Running loss: 0.73707 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3634 | Classification loss: 0.22215 | Regression loss: 0.38889 | Running loss: 0.73622 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3635 | Classification loss: 1.19022 | Regression loss: 0.58157 | Running loss: 0.73917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3636 | Classification loss: 0.95279 | Regression loss: 0.63228 | Running loss: 0.74047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3637 | Classification loss: 0.00901 | Regression loss: 0.15610 | Running loss: 0.74020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3638 | Classification loss: 1.25004 | Regression loss: 0.62454 | Running loss: 0.74361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3639 | Classification loss: 0.02170 | Regression loss: 0.15894 | Running loss: 0.74228 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3640 | Classification loss: 0.84136 | Regression loss: 0.65325 | Running loss: 0.74399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3641 | Classification loss: 0.07087 | Regression loss: 0.22301 | Running loss: 0.74364 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3642 | Classification loss: 0.17682 | Regression loss: 0.46046 | Running loss: 0.74239 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3643 | Classification loss: 0.49088 | Regression loss: 0.64156 | Running loss: 0.74362 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3644 | Classification loss: 0.06056 | Regression loss: 0.04605 | Running loss: 0.74326 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3645 | Classification loss: 0.19285 | Regression loss: 0.35646 | Running loss: 0.74392 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3646 | Classification loss: 1.12598 | Regression loss: 0.59065 | Running loss: 0.74700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3647 | Classification loss: 0.02064 | Regression loss: 0.14523 | Running loss: 0.74708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3648 | Classification loss: 1.08281 | Regression loss: 0.51042 | Running loss: 0.75019 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3649 | Classification loss: 0.27112 | Regression loss: 0.24092 | Running loss: 0.75071 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3650 | Classification loss: 0.40528 | Regression loss: 0.59464 | Running loss: 0.75115 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3651 | Classification loss: 0.10688 | Regression loss: 0.21359 | Running loss: 0.75115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3652 | Classification loss: 0.03997 | Regression loss: 0.15492 | Running loss: 0.75136 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3653 | Classification loss: 0.02483 | Regression loss: 0.13500 | Running loss: 0.74892 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3654 | Classification loss: 0.17446 | Regression loss: 0.27462 | Running loss: 0.74948 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3655 | Classification loss: 0.08802 | Regression loss: 0.30517 | Running loss: 0.74975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3656 | Classification loss: 0.00058 | Regression loss: 0.10431 | Running loss: 0.74804 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3657 | Classification loss: 0.05899 | Regression loss: 0.15965 | Running loss: 0.74827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3658 | Classification loss: 1.49410 | Regression loss: 0.17882 | Running loss: 0.75100 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3659 | Classification loss: 0.68333 | Regression loss: 0.52303 | Running loss: 0.75281 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3660 | Classification loss: 0.25081 | Regression loss: 0.48438 | Running loss: 0.75336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3661 | Classification loss: 0.00665 | Regression loss: 0.10245 | Running loss: 0.75083 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3662 | Classification loss: 0.03810 | Regression loss: 0.20192 | Running loss: 0.75087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3663 | Classification loss: 0.01556 | Regression loss: 0.19867 | Running loss: 0.74933 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3664 | Classification loss: 1.14055 | Regression loss: 0.40318 | Running loss: 0.75215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3665 | Classification loss: 0.58233 | Regression loss: 0.13483 | Running loss: 0.75308 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3666 | Classification loss: 0.35457 | Regression loss: 0.18335 | Running loss: 0.75043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3667 | Classification loss: 0.06487 | Regression loss: 0.07523 | Running loss: 0.75039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3668 | Classification loss: 0.51504 | Regression loss: 0.50533 | Running loss: 0.75121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3669 | Classification loss: 1.03934 | Regression loss: 0.46751 | Running loss: 0.75335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3670 | Classification loss: 0.00024 | Regression loss: 0.01413 | Running loss: 0.75290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3671 | Classification loss: 0.12546 | Regression loss: 0.27771 | Running loss: 0.75312 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3672 | Classification loss: 0.68163 | Regression loss: 0.54263 | Running loss: 0.75497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3673 | Classification loss: 0.67138 | Regression loss: 0.29169 | Running loss: 0.75485 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3674 | Classification loss: 0.00783 | Regression loss: 0.04136 | Running loss: 0.75360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3675 | Classification loss: 0.11319 | Regression loss: 0.31962 | Running loss: 0.75387 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3676 | Classification loss: 0.51258 | Regression loss: 0.46473 | Running loss: 0.75505 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3677 | Classification loss: 0.61093 | Regression loss: 0.41467 | Running loss: 0.75441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3678 | Classification loss: 0.21943 | Regression loss: 0.17604 | Running loss: 0.75360 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3679 | Classification loss: 0.10975 | Regression loss: 0.25362 | Running loss: 0.75414 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3680 | Classification loss: 0.75975 | Regression loss: 0.42286 | Running loss: 0.75406 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3681 | Classification loss: 0.00985 | Regression loss: 0.15359 | Running loss: 0.75106 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3682 | Classification loss: 0.00647 | Regression loss: 0.20121 | Running loss: 0.75031 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3683 | Classification loss: 0.35910 | Regression loss: 0.30932 | Running loss: 0.75070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3684 | Classification loss: 0.43952 | Regression loss: 0.52501 | Running loss: 0.75226 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3685 | Classification loss: 0.63711 | Regression loss: 0.60134 | Running loss: 0.75406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3686 | Classification loss: 0.18217 | Regression loss: 0.27009 | Running loss: 0.75470 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3687 | Classification loss: 0.49107 | Regression loss: 0.62737 | Running loss: 0.75653 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3688 | Classification loss: 0.12328 | Regression loss: 0.41094 | Running loss: 0.75642 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3689 | Classification loss: 0.14495 | Regression loss: 0.27830 | Running loss: 0.75579 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3690 | Classification loss: 0.13943 | Regression loss: 0.25103 | Running loss: 0.75583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3691 | Classification loss: 0.09559 | Regression loss: 0.34988 | Running loss: 0.75563 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3692 | Classification loss: 0.56889 | Regression loss: 0.36052 | Running loss: 0.75544 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3693 | Classification loss: 0.39356 | Regression loss: 0.38688 | Running loss: 0.75651 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3694 | Classification loss: 0.19918 | Regression loss: 0.30287 | Running loss: 0.75744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3695 | Classification loss: 0.03900 | Regression loss: 0.26428 | Running loss: 0.75762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3696 | Classification loss: 1.10697 | Regression loss: 0.62713 | Running loss: 0.76060 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3697 | Classification loss: 0.00003 | Regression loss: 0.00953 | Running loss: 0.75509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3698 | Classification loss: 0.09782 | Regression loss: 0.32693 | Running loss: 0.75347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3699 | Classification loss: 0.40272 | Regression loss: 0.28859 | Running loss: 0.75427 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3700 | Classification loss: 0.44648 | Regression loss: 0.37799 | Running loss: 0.75516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3701 | Classification loss: 1.49232 | Regression loss: 0.43535 | Running loss: 0.75689 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3702 | Classification loss: 0.00949 | Regression loss: 0.08785 | Running loss: 0.75518 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3703 | Classification loss: 0.00748 | Regression loss: 0.04405 | Running loss: 0.75469 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3704 | Classification loss: 0.25476 | Regression loss: 0.57296 | Running loss: 0.75606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3705 | Classification loss: 0.27888 | Regression loss: 0.35694 | Running loss: 0.75481 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3706 | Classification loss: 0.28155 | Regression loss: 0.41599 | Running loss: 0.75552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3707 | Classification loss: 0.39610 | Regression loss: 0.41950 | Running loss: 0.75407 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3708 | Classification loss: 0.83777 | Regression loss: 0.70784 | Running loss: 0.75703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3709 | Classification loss: 0.01380 | Regression loss: 0.12243 | Running loss: 0.75633 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3710 | Classification loss: 0.44971 | Regression loss: 0.52583 | Running loss: 0.75633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3711 | Classification loss: 0.19271 | Regression loss: 0.34041 | Running loss: 0.75689 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3712 | Classification loss: 0.90049 | Regression loss: 0.09415 | Running loss: 0.75698 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3713 | Classification loss: 0.20530 | Regression loss: 0.23726 | Running loss: 0.75757 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3714 | Classification loss: 0.03588 | Regression loss: 0.13993 | Running loss: 0.75674 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3715 | Classification loss: 0.12600 | Regression loss: 0.28363 | Running loss: 0.75504 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3716 | Classification loss: 0.08099 | Regression loss: 0.17121 | Running loss: 0.75532 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3717 | Classification loss: 0.37520 | Regression loss: 0.12195 | Running loss: 0.75532 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3718 | Classification loss: 0.45404 | Regression loss: 0.51283 | Running loss: 0.75312 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3719 | Classification loss: 0.36496 | Regression loss: 0.36175 | Running loss: 0.75438 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3720 | Classification loss: 1.14505 | Regression loss: 0.47351 | Running loss: 0.75700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3721 | Classification loss: 0.38392 | Regression loss: 0.23794 | Running loss: 0.75663 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3722 | Classification loss: 1.04303 | Regression loss: 0.55484 | Running loss: 0.75697 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3723 | Classification loss: 0.00135 | Regression loss: 0.02170 | Running loss: 0.75245 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3724 | Classification loss: 0.15388 | Regression loss: 0.13174 | Running loss: 0.75122 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3725 | Classification loss: 0.50636 | Regression loss: 0.42096 | Running loss: 0.75109 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3726 | Classification loss: 0.15277 | Regression loss: 0.13516 | Running loss: 0.75094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3727 | Classification loss: 0.03699 | Regression loss: 0.11358 | Running loss: 0.74744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3728 | Classification loss: 1.02925 | Regression loss: 0.47736 | Running loss: 0.74948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3729 | Classification loss: 0.30535 | Regression loss: 0.50386 | Running loss: 0.75079 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3730 | Classification loss: 0.87071 | Regression loss: 0.55763 | Running loss: 0.75305 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3731 | Classification loss: 1.28660 | Regression loss: 0.73797 | Running loss: 0.75558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3732 | Classification loss: 0.00799 | Regression loss: 0.12847 | Running loss: 0.75303 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3733 | Classification loss: 1.49809 | Regression loss: 0.33398 | Running loss: 0.75429 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3734 | Classification loss: 0.19137 | Regression loss: 0.42318 | Running loss: 0.75102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3735 | Classification loss: 0.22033 | Regression loss: 0.08034 | Running loss: 0.75137 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3736 | Classification loss: 0.61220 | Regression loss: 0.64760 | Running loss: 0.75338 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3737 | Classification loss: 1.00930 | Regression loss: 0.63878 | Running loss: 0.75634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3738 | Classification loss: 0.06435 | Regression loss: 0.30610 | Running loss: 0.75596 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3739 | Classification loss: 0.21405 | Regression loss: 0.21171 | Running loss: 0.75372 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3740 | Classification loss: 0.45235 | Regression loss: 0.31233 | Running loss: 0.75500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3741 | Classification loss: 2.36820 | Regression loss: 0.56937 | Running loss: 0.75956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3742 | Classification loss: 0.20637 | Regression loss: 0.19235 | Running loss: 0.75966 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3743 | Classification loss: 0.13752 | Regression loss: 0.15449 | Running loss: 0.75876 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3744 | Classification loss: 0.54152 | Regression loss: 0.63800 | Running loss: 0.75796 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3745 | Classification loss: 0.83953 | Regression loss: 0.25874 | Running loss: 0.75999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3746 | Classification loss: 0.76445 | Regression loss: 0.77908 | Running loss: 0.76217 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3747 | Classification loss: 0.37114 | Regression loss: 0.26192 | Running loss: 0.76247 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3748 | Classification loss: 0.08834 | Regression loss: 0.16325 | Running loss: 0.76042 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3749 | Classification loss: 0.02886 | Regression loss: 0.17742 | Running loss: 0.76027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3750 | Classification loss: 0.02289 | Regression loss: 0.16087 | Running loss: 0.75993 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3751 | Classification loss: 0.71055 | Regression loss: 0.47053 | Running loss: 0.76188 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3752 | Classification loss: 0.94442 | Regression loss: 0.65217 | Running loss: 0.76470 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3753 | Classification loss: 1.73671 | Regression loss: 0.44252 | Running loss: 0.76519 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3754 | Classification loss: 2.06707 | Regression loss: 0.53300 | Running loss: 0.76995 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3755 | Classification loss: 0.00525 | Regression loss: 0.07493 | Running loss: 0.76950 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3756 | Classification loss: 0.00105 | Regression loss: 0.04020 | Running loss: 0.76708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3757 | Classification loss: 0.00218 | Regression loss: 0.09862 | Running loss: 0.76525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3758 | Classification loss: 0.94639 | Regression loss: 0.65622 | Running loss: 0.76740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3759 | Classification loss: 0.30526 | Regression loss: 0.24764 | Running loss: 0.76799 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3760 | Classification loss: 0.22575 | Regression loss: 0.44224 | Running loss: 0.76835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3761 | Classification loss: 0.03107 | Regression loss: 0.17241 | Running loss: 0.76851 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3762 | Classification loss: 0.92726 | Regression loss: 0.47590 | Running loss: 0.77052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3763 | Classification loss: 0.73040 | Regression loss: 0.55024 | Running loss: 0.77246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3764 | Classification loss: 0.07640 | Regression loss: 0.18074 | Running loss: 0.77084 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3765 | Classification loss: 0.22569 | Regression loss: 0.17875 | Running loss: 0.77038 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3766 | Classification loss: 0.01637 | Regression loss: 0.24637 | Running loss: 0.77040 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3767 | Classification loss: 0.02291 | Regression loss: 0.06459 | Running loss: 0.76955 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3768 | Classification loss: 0.74095 | Regression loss: 0.59707 | Running loss: 0.76863 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3769 | Classification loss: 0.00461 | Regression loss: 0.11844 | Running loss: 0.76552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3770 | Classification loss: 0.66723 | Regression loss: 0.35147 | Running loss: 0.76513 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3771 | Classification loss: 0.11224 | Regression loss: 0.08522 | Running loss: 0.76524 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3772 | Classification loss: 1.57387 | Regression loss: 0.53589 | Running loss: 0.76836 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3773 | Classification loss: 0.34958 | Regression loss: 0.45313 | Running loss: 0.76974 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3774 | Classification loss: 0.27038 | Regression loss: 0.28760 | Running loss: 0.76780 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3775 | Classification loss: 0.88046 | Regression loss: 0.54459 | Running loss: 0.76671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3776 | Classification loss: 0.06032 | Regression loss: 0.13797 | Running loss: 0.76645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3777 | Classification loss: 1.61144 | Regression loss: 0.35694 | Running loss: 0.76690 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3778 | Classification loss: 0.40364 | Regression loss: 0.30952 | Running loss: 0.76675 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3779 | Classification loss: 0.41439 | Regression loss: 0.34229 | Running loss: 0.76783 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3780 | Classification loss: 0.10436 | Regression loss: 0.37629 | Running loss: 0.76674 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3781 | Classification loss: 0.16045 | Regression loss: 0.15662 | Running loss: 0.76700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3782 | Classification loss: 1.01288 | Regression loss: 0.59690 | Running loss: 0.76681 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3783 | Classification loss: 1.14119 | Regression loss: 0.50692 | Running loss: 0.76666 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3784 | Classification loss: 0.89823 | Regression loss: 0.42062 | Running loss: 0.76832 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3785 | Classification loss: 0.07584 | Regression loss: 0.10593 | Running loss: 0.76841 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3786 | Classification loss: 1.67958 | Regression loss: 0.53456 | Running loss: 0.76710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3787 | Classification loss: 0.26458 | Regression loss: 0.19161 | Running loss: 0.76772 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3788 | Classification loss: 0.23730 | Regression loss: 0.37954 | Running loss: 0.76867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3789 | Classification loss: 1.18171 | Regression loss: 0.36274 | Running loss: 0.77129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3790 | Classification loss: 0.22402 | Regression loss: 0.43623 | Running loss: 0.77115 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3791 | Classification loss: 0.80477 | Regression loss: 0.52140 | Running loss: 0.77304 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3792 | Classification loss: 0.33654 | Regression loss: 0.19186 | Running loss: 0.77373 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3793 | Classification loss: 0.07547 | Regression loss: 0.09350 | Running loss: 0.77107 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3794 | Classification loss: 0.03168 | Regression loss: 0.36912 | Running loss: 0.76913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3795 | Classification loss: 0.38642 | Regression loss: 0.49490 | Running loss: 0.77070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3796 | Classification loss: 0.45972 | Regression loss: 0.59321 | Running loss: 0.77225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3797 | Classification loss: 0.96489 | Regression loss: 0.80429 | Running loss: 0.77383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3798 | Classification loss: 0.40237 | Regression loss: 0.43351 | Running loss: 0.77469 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3799 | Classification loss: 0.35421 | Regression loss: 0.56930 | Running loss: 0.77465 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3800 | Classification loss: 0.35731 | Regression loss: 0.36499 | Running loss: 0.77572 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3801 | Classification loss: 0.88541 | Regression loss: 0.43204 | Running loss: 0.77767 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3802 | Classification loss: 0.34259 | Regression loss: 0.20146 | Running loss: 0.77705 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3803 | Classification loss: 0.09700 | Regression loss: 0.37709 | Running loss: 0.77777 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3804 | Classification loss: 0.00510 | Regression loss: 0.05685 | Running loss: 0.77616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3805 | Classification loss: 0.66634 | Regression loss: 0.53574 | Running loss: 0.77759 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3806 | Classification loss: 0.62429 | Regression loss: 0.56310 | Running loss: 0.77908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3807 | Classification loss: 0.20494 | Regression loss: 0.54167 | Running loss: 0.77660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3808 | Classification loss: 0.19280 | Regression loss: 0.25459 | Running loss: 0.77729 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3809 | Classification loss: 0.23373 | Regression loss: 0.39096 | Running loss: 0.77833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3810 | Classification loss: 0.44702 | Regression loss: 0.28417 | Running loss: 0.77894 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 3811 | Classification loss: 0.54060 | Regression loss: 0.62154 | Running loss: 0.77886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3812 | Classification loss: 0.85719 | Regression loss: 0.63714 | Running loss: 0.78050 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3813 | Classification loss: 0.15027 | Regression loss: 0.54869 | Running loss: 0.78090 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3814 | Classification loss: 0.02188 | Regression loss: 0.12434 | Running loss: 0.78080 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3815 | Classification loss: 1.61730 | Regression loss: 0.55644 | Running loss: 0.78424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3816 | Classification loss: 0.74473 | Regression loss: 0.41408 | Running loss: 0.78635 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3817 | Classification loss: 1.68122 | Regression loss: 0.76710 | Running loss: 0.78923 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3818 | Classification loss: 0.78975 | Regression loss: 0.58479 | Running loss: 0.78927 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3819 | Classification loss: 0.00193 | Regression loss: 0.16112 | Running loss: 0.78904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3820 | Classification loss: 1.00323 | Regression loss: 0.67915 | Running loss: 0.79218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3821 | Classification loss: 1.86878 | Regression loss: 0.73342 | Running loss: 0.79663 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3822 | Classification loss: 0.14698 | Regression loss: 0.17282 | Running loss: 0.79707 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3823 | Classification loss: 0.00575 | Regression loss: 0.18436 | Running loss: 0.79476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3824 | Classification loss: 1.76617 | Regression loss: 0.57569 | Running loss: 0.79610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3825 | Classification loss: 0.00579 | Regression loss: 0.15828 | Running loss: 0.79324 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3826 | Classification loss: 0.11161 | Regression loss: 0.09199 | Running loss: 0.79053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3827 | Classification loss: 0.82200 | Regression loss: 0.56426 | Running loss: 0.79202 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3828 | Classification loss: 0.02530 | Regression loss: 0.10338 | Running loss: 0.78991 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3829 | Classification loss: 0.89892 | Regression loss: 0.48367 | Running loss: 0.79070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3830 | Classification loss: 0.50940 | Regression loss: 0.26578 | Running loss: 0.78800 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3831 | Classification loss: 0.00931 | Regression loss: 0.18306 | Running loss: 0.78680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3832 | Classification loss: 0.03083 | Regression loss: 0.16033 | Running loss: 0.78594 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3833 | Classification loss: 1.25145 | Regression loss: 0.51423 | Running loss: 0.78929 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3834 | Classification loss: 0.09036 | Regression loss: 0.45405 | Running loss: 0.78933 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3835 | Classification loss: 0.92637 | Regression loss: 0.62810 | Running loss: 0.79144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3836 | Classification loss: 0.01441 | Regression loss: 0.09879 | Running loss: 0.78693 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3837 | Classification loss: 0.16580 | Regression loss: 0.46560 | Running loss: 0.78763 | Spend Time:0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3838 | Classification loss: 1.03568 | Regression loss: 0.41537 | Running loss: 0.79002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3839 | Classification loss: 0.11069 | Regression loss: 0.40088 | Running loss: 0.79078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3840 | Classification loss: 0.00135 | Regression loss: 0.29076 | Running loss: 0.78762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3841 | Classification loss: 0.08666 | Regression loss: 0.14171 | Running loss: 0.78787 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3842 | Classification loss: 0.27650 | Regression loss: 0.34061 | Running loss: 0.78677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3843 | Classification loss: 0.58223 | Regression loss: 0.64873 | Running loss: 0.78705 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3844 | Classification loss: 0.01992 | Regression loss: 0.12499 | Running loss: 0.78550 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3845 | Classification loss: 0.36194 | Regression loss: 0.56150 | Running loss: 0.78699 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3846 | Classification loss: 0.05270 | Regression loss: 0.30099 | Running loss: 0.78714 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3847 | Classification loss: 1.78721 | Regression loss: 0.53962 | Running loss: 0.79088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3848 | Classification loss: 0.00296 | Regression loss: 0.15839 | Running loss: 0.79068 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3849 | Classification loss: 0.91917 | Regression loss: 0.24784 | Running loss: 0.79234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3850 | Classification loss: 0.32343 | Regression loss: 0.33997 | Running loss: 0.79309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3851 | Classification loss: 0.48166 | Regression loss: 0.41821 | Running loss: 0.79172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3852 | Classification loss: 0.68046 | Regression loss: 0.53212 | Running loss: 0.79266 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3853 | Classification loss: 0.07806 | Regression loss: 0.24142 | Running loss: 0.79297 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3854 | Classification loss: 0.26825 | Regression loss: 0.43264 | Running loss: 0.79315 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3855 | Classification loss: 0.27755 | Regression loss: 0.26588 | Running loss: 0.78746 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3856 | Classification loss: 1.58603 | Regression loss: 0.26353 | Running loss: 0.79002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3857 | Classification loss: 0.54956 | Regression loss: 0.49032 | Running loss: 0.79043 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3858 | Classification loss: 1.05635 | Regression loss: 0.43512 | Running loss: 0.79325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3859 | Classification loss: 0.87506 | Regression loss: 0.64366 | Running loss: 0.79026 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3860 | Classification loss: 0.19608 | Regression loss: 0.18135 | Running loss: 0.78992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3861 | Classification loss: 1.08668 | Regression loss: 0.63865 | Running loss: 0.79196 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3862 | Classification loss: 0.05448 | Regression loss: 0.11602 | Running loss: 0.78930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3863 | Classification loss: 0.69224 | Regression loss: 0.67864 | Running loss: 0.79199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3864 | Classification loss: 0.22797 | Regression loss: 0.29845 | Running loss: 0.79186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3865 | Classification loss: 0.00403 | Regression loss: 0.15186 | Running loss: 0.78898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3866 | Classification loss: 0.26272 | Regression loss: 0.26187 | Running loss: 0.78884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3867 | Classification loss: 0.01503 | Regression loss: 0.14048 | Running loss: 0.78788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3868 | Classification loss: 0.12161 | Regression loss: 0.31173 | Running loss: 0.78846 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3869 | Classification loss: 2.20250 | Regression loss: 0.72763 | Running loss: 0.79342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3870 | Classification loss: 0.36559 | Regression loss: 0.41375 | Running loss: 0.79420 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3871 | Classification loss: 0.32862 | Regression loss: 0.22588 | Running loss: 0.79459 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3872 | Classification loss: 0.96232 | Regression loss: 0.64357 | Running loss: 0.79725 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3873 | Classification loss: 0.15392 | Regression loss: 0.38329 | Running loss: 0.79531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3874 | Classification loss: 0.84740 | Regression loss: 0.45457 | Running loss: 0.79596 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3875 | Classification loss: 0.28126 | Regression loss: 0.30752 | Running loss: 0.79094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3876 | Classification loss: 0.27441 | Regression loss: 0.55902 | Running loss: 0.79242 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3877 | Classification loss: 0.08535 | Regression loss: 0.07110 | Running loss: 0.79156 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3878 | Classification loss: 0.50965 | Regression loss: 0.40155 | Running loss: 0.79047 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3879 | Classification loss: 0.00189 | Regression loss: 0.11671 | Running loss: 0.78389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3880 | Classification loss: 0.00344 | Regression loss: 0.07677 | Running loss: 0.78360 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3881 | Classification loss: 0.16832 | Regression loss: 0.42470 | Running loss: 0.78438 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3882 | Classification loss: 0.03397 | Regression loss: 0.24171 | Running loss: 0.78414 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3883 | Classification loss: 1.19709 | Regression loss: 0.71034 | Running loss: 0.78764 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3884 | Classification loss: 0.07341 | Regression loss: 0.27903 | Running loss: 0.78801 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3885 | Classification loss: 0.05048 | Regression loss: 0.24870 | Running loss: 0.78633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3886 | Classification loss: 1.68962 | Regression loss: 0.75601 | Running loss: 0.79091 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3887 | Classification loss: 0.13306 | Regression loss: 0.25909 | Running loss: 0.78745 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3888 | Classification loss: 0.00334 | Regression loss: 0.11255 | Running loss: 0.78491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3889 | Classification loss: 0.21158 | Regression loss: 0.17718 | Running loss: 0.78349 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3890 | Classification loss: 0.43628 | Regression loss: 0.40681 | Running loss: 0.78276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3891 | Classification loss: 0.02569 | Regression loss: 0.18754 | Running loss: 0.78235 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3892 | Classification loss: 0.00219 | Regression loss: 0.05220 | Running loss: 0.78096 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3893 | Classification loss: 0.42097 | Regression loss: 0.31378 | Running loss: 0.78199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3894 | Classification loss: 0.85375 | Regression loss: 0.33500 | Running loss: 0.78405 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3895 | Classification loss: 0.04464 | Regression loss: 0.08292 | Running loss: 0.78319 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3896 | Classification loss: 0.03997 | Regression loss: 0.09641 | Running loss: 0.78331 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3897 | Classification loss: 0.72055 | Regression loss: 0.39929 | Running loss: 0.78426 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3898 | Classification loss: 0.82851 | Regression loss: 0.30326 | Running loss: 0.78532 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3899 | Classification loss: 1.95885 | Regression loss: 0.76121 | Running loss: 0.78964 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3900 | Classification loss: 0.16976 | Regression loss: 0.38540 | Running loss: 0.78963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3901 | Classification loss: 1.48297 | Regression loss: 0.76529 | Running loss: 0.79317 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3902 | Classification loss: 1.08089 | Regression loss: 0.57406 | Running loss: 0.79418 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3903 | Classification loss: 0.13399 | Regression loss: 0.23787 | Running loss: 0.79458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3904 | Classification loss: 1.05232 | Regression loss: 0.62555 | Running loss: 0.79678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3905 | Classification loss: 0.65603 | Regression loss: 0.32861 | Running loss: 0.79701 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3906 | Classification loss: 0.98806 | Regression loss: 0.57914 | Running loss: 0.79728 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3907 | Classification loss: 0.69621 | Regression loss: 0.43444 | Running loss: 0.79891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3908 | Classification loss: 0.06576 | Regression loss: 0.21713 | Running loss: 0.79871 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3909 | Classification loss: 0.00476 | Regression loss: 0.04019 | Running loss: 0.79630 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3910 | Classification loss: 0.21970 | Regression loss: 0.33663 | Running loss: 0.79404 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3911 | Classification loss: 0.37299 | Regression loss: 0.57694 | Running loss: 0.79538 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3912 | Classification loss: 0.02050 | Regression loss: 0.13025 | Running loss: 0.79346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3913 | Classification loss: 0.10250 | Regression loss: 0.18425 | Running loss: 0.79326 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3914 | Classification loss: 0.20164 | Regression loss: 0.34791 | Running loss: 0.79239 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3915 | Classification loss: 0.18325 | Regression loss: 0.25117 | Running loss: 0.79143 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3916 | Classification loss: 0.58330 | Regression loss: 0.46019 | Running loss: 0.79316 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3917 | Classification loss: 0.00108 | Regression loss: 0.09764 | Running loss: 0.79173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3918 | Classification loss: 0.38896 | Regression loss: 0.33839 | Running loss: 0.79245 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3919 | Classification loss: 0.24349 | Regression loss: 0.46811 | Running loss: 0.79190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3920 | Classification loss: 0.70817 | Regression loss: 0.73107 | Running loss: 0.79342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3921 | Classification loss: 0.89143 | Regression loss: 0.55636 | Running loss: 0.79604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3922 | Classification loss: 0.29642 | Regression loss: 0.34865 | Running loss: 0.79711 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3923 | Classification loss: 0.46440 | Regression loss: 0.45986 | Running loss: 0.79602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3924 | Classification loss: 0.02123 | Regression loss: 0.31563 | Running loss: 0.79418 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3925 | Classification loss: 0.25206 | Regression loss: 0.27820 | Running loss: 0.79490 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3926 | Classification loss: 0.03695 | Regression loss: 0.24181 | Running loss: 0.79390 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3927 | Classification loss: 0.00325 | Regression loss: 0.09740 | Running loss: 0.79382 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3928 | Classification loss: 0.38123 | Regression loss: 0.30662 | Running loss: 0.79356 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3929 | Classification loss: 0.03876 | Regression loss: 0.12146 | Running loss: 0.79185 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3930 | Classification loss: 1.48990 | Regression loss: 0.55547 | Running loss: 0.79585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3931 | Classification loss: 0.77753 | Regression loss: 0.73743 | Running loss: 0.79857 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3932 | Classification loss: 0.02518 | Regression loss: 0.07791 | Running loss: 0.79818 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3933 | Classification loss: 0.15161 | Regression loss: 0.20488 | Running loss: 0.79816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3934 | Classification loss: 0.78287 | Regression loss: 0.51414 | Running loss: 0.79923 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3935 | Classification loss: 0.06819 | Regression loss: 0.11473 | Running loss: 0.79657 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3936 | Classification loss: 0.22646 | Regression loss: 0.40696 | Running loss: 0.79392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3937 | Classification loss: 0.27325 | Regression loss: 0.17663 | Running loss: 0.79274 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3938 | Classification loss: 0.12371 | Regression loss: 0.24255 | Running loss: 0.79179 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3939 | Classification loss: 0.14628 | Regression loss: 0.37384 | Running loss: 0.79053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3940 | Classification loss: 1.09715 | Regression loss: 0.67664 | Running loss: 0.79012 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3941 | Classification loss: 0.15421 | Regression loss: 0.25724 | Running loss: 0.78992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3942 | Classification loss: 0.06516 | Regression loss: 0.15497 | Running loss: 0.78949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3943 | Classification loss: 0.04953 | Regression loss: 0.16461 | Running loss: 0.78952 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3944 | Classification loss: 1.02890 | Regression loss: 0.57911 | Running loss: 0.79258 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3945 | Classification loss: 0.80682 | Regression loss: 0.55736 | Running loss: 0.79424 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3946 | Classification loss: 0.61147 | Regression loss: 0.40477 | Running loss: 0.79584 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3947 | Classification loss: 0.03834 | Regression loss: 0.23131 | Running loss: 0.79250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3948 | Classification loss: 0.08216 | Regression loss: 0.14576 | Running loss: 0.79173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3949 | Classification loss: 0.21560 | Regression loss: 0.25546 | Running loss: 0.79172 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3950 | Classification loss: 0.13409 | Regression loss: 0.22753 | Running loss: 0.79158 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3951 | Classification loss: 0.38013 | Regression loss: 0.31552 | Running loss: 0.79279 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3952 | Classification loss: 0.80421 | Regression loss: 0.68660 | Running loss: 0.79431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3953 | Classification loss: 1.00955 | Regression loss: 0.61414 | Running loss: 0.79474 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3954 | Classification loss: 0.27220 | Regression loss: 0.12191 | Running loss: 0.79432 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3955 | Classification loss: 0.08625 | Regression loss: 0.17164 | Running loss: 0.79438 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3956 | Classification loss: 0.00844 | Regression loss: 0.12875 | Running loss: 0.79132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3957 | Classification loss: 0.68809 | Regression loss: 0.58749 | Running loss: 0.79241 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3958 | Classification loss: 0.24814 | Regression loss: 0.32256 | Running loss: 0.79224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3959 | Classification loss: 0.80181 | Regression loss: 0.48323 | Running loss: 0.79435 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3960 | Classification loss: 0.10225 | Regression loss: 0.34426 | Running loss: 0.79276 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3961 | Classification loss: 0.75788 | Regression loss: 0.54945 | Running loss: 0.79265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3962 | Classification loss: 0.55943 | Regression loss: 0.43354 | Running loss: 0.78997 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3963 | Classification loss: 1.23671 | Regression loss: 0.49102 | Running loss: 0.79235 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3964 | Classification loss: 1.82326 | Regression loss: 1.05674 | Running loss: 0.79760 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3965 | Classification loss: 0.03307 | Regression loss: 0.21939 | Running loss: 0.79713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3966 | Classification loss: 0.29055 | Regression loss: 0.50196 | Running loss: 0.79852 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 3967 | Classification loss: 0.06136 | Regression loss: 0.12661 | Running loss: 0.79817 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3968 | Classification loss: 0.23052 | Regression loss: 0.27666 | Running loss: 0.79902 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3969 | Classification loss: 0.01007 | Regression loss: 0.09570 | Running loss: 0.79861 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3970 | Classification loss: 0.53693 | Regression loss: 0.43451 | Running loss: 0.80012 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3971 | Classification loss: 0.03161 | Regression loss: 0.27543 | Running loss: 0.79936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3972 | Classification loss: 0.02556 | Regression loss: 0.12571 | Running loss: 0.79936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3973 | Classification loss: 0.12959 | Regression loss: 0.29752 | Running loss: 0.79971 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3974 | Classification loss: 1.03059 | Regression loss: 0.68263 | Running loss: 0.79963 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3975 | Classification loss: 0.01924 | Regression loss: 0.25613 | Running loss: 0.80013 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3976 | Classification loss: 0.12336 | Regression loss: 0.26296 | Running loss: 0.80053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3977 | Classification loss: 0.12210 | Regression loss: 0.12961 | Running loss: 0.79891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3978 | Classification loss: 0.39737 | Regression loss: 0.29626 | Running loss: 0.79947 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3979 | Classification loss: 0.35060 | Regression loss: 0.36496 | Running loss: 0.79867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3980 | Classification loss: 0.63876 | Regression loss: 0.55534 | Running loss: 0.79967 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3981 | Classification loss: 0.35148 | Regression loss: 0.23866 | Running loss: 0.80070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3982 | Classification loss: 1.40982 | Regression loss: 0.62333 | Running loss: 0.80434 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3983 | Classification loss: 1.16399 | Regression loss: 0.55427 | Running loss: 0.80706 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3984 | Classification loss: 0.02156 | Regression loss: 0.24182 | Running loss: 0.80422 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3985 | Classification loss: 0.46998 | Regression loss: 0.85332 | Running loss: 0.80620 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3986 | Classification loss: 0.10565 | Regression loss: 0.29358 | Running loss: 0.80599 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3987 | Classification loss: 1.29091 | Regression loss: 0.72679 | Running loss: 0.80946 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3988 | Classification loss: 0.03404 | Regression loss: 0.12992 | Running loss: 0.80834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3989 | Classification loss: 1.17254 | Regression loss: 0.47743 | Running loss: 0.81073 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3990 | Classification loss: 0.43387 | Regression loss: 0.50793 | Running loss: 0.81213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3991 | Classification loss: 0.08337 | Regression loss: 0.21009 | Running loss: 0.80881 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 3992 | Classification loss: 0.16190 | Regression loss: 0.29538 | Running loss: 0.80788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3993 | Classification loss: 0.04413 | Regression loss: 0.18243 | Running loss: 0.80737 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3994 | Classification loss: 1.44379 | Regression loss: 0.56590 | Running loss: 0.81129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3995 | Classification loss: 0.57218 | Regression loss: 0.28060 | Running loss: 0.81133 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3996 | Classification loss: 1.04890 | Regression loss: 0.44771 | Running loss: 0.81290 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3997 | Classification loss: 0.02898 | Regression loss: 0.25800 | Running loss: 0.81260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3998 | Classification loss: 0.16897 | Regression loss: 0.21970 | Running loss: 0.81073 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 3999 | Classification loss: 0.46854 | Regression loss: 0.44636 | Running loss: 0.80954 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4000 | Classification loss: 0.09292 | Regression loss: 0.22767 | Running loss: 0.80896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4001 | Classification loss: 0.00035 | Regression loss: 0.14218 | Running loss: 0.80834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4002 | Classification loss: 1.06416 | Regression loss: 0.49600 | Running loss: 0.81054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4003 | Classification loss: 0.13152 | Regression loss: 0.43062 | Running loss: 0.81116 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4004 | Classification loss: 1.02597 | Regression loss: 0.60625 | Running loss: 0.81009 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4005 | Classification loss: 0.20221 | Regression loss: 0.38874 | Running loss: 0.80914 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4006 | Classification loss: 0.28998 | Regression loss: 0.34222 | Running loss: 0.80712 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4007 | Classification loss: 0.03611 | Regression loss: 0.17994 | Running loss: 0.80367 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4008 | Classification loss: 0.48376 | Regression loss: 0.39689 | Running loss: 0.80321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4009 | Classification loss: 1.55102 | Regression loss: 0.81279 | Running loss: 0.80634 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4010 | Classification loss: 0.12783 | Regression loss: 0.28897 | Running loss: 0.80530 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4011 | Classification loss: 1.41528 | Regression loss: 0.57960 | Running loss: 0.80653 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4012 | Classification loss: 0.03358 | Regression loss: 0.23179 | Running loss: 0.80675 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4013 | Classification loss: 0.27822 | Regression loss: 0.46537 | Running loss: 0.80757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4014 | Classification loss: 0.17764 | Regression loss: 0.33749 | Running loss: 0.80692 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4015 | Classification loss: 1.19481 | Regression loss: 0.56526 | Running loss: 0.80936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4016 | Classification loss: 0.17766 | Regression loss: 0.33859 | Running loss: 0.80809 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4017 | Classification loss: 0.17003 | Regression loss: 0.36139 | Running loss: 0.80756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4018 | Classification loss: 0.08856 | Regression loss: 0.27628 | Running loss: 0.80747 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4019 | Classification loss: 0.53476 | Regression loss: 0.15347 | Running loss: 0.80851 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4020 | Classification loss: 0.42259 | Regression loss: 0.42859 | Running loss: 0.80992 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4021 | Classification loss: 0.13112 | Regression loss: 0.25883 | Running loss: 0.81034 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4022 | Classification loss: 0.99464 | Regression loss: 0.69826 | Running loss: 0.81104 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4023 | Classification loss: 0.37430 | Regression loss: 0.35039 | Running loss: 0.81090 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4024 | Classification loss: 0.32113 | Regression loss: 0.41870 | Running loss: 0.81119 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4025 | Classification loss: 0.35971 | Regression loss: 0.35627 | Running loss: 0.81037 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4026 | Classification loss: 0.13296 | Regression loss: 0.40208 | Running loss: 0.81068 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4027 | Classification loss: 2.00382 | Regression loss: 0.58259 | Running loss: 0.81398 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4028 | Classification loss: 0.01111 | Regression loss: 0.05657 | Running loss: 0.81280 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4029 | Classification loss: 0.69578 | Regression loss: 0.47864 | Running loss: 0.81483 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4030 | Classification loss: 0.17576 | Regression loss: 0.34173 | Running loss: 0.81574 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4031 | Classification loss: 0.65677 | Regression loss: 0.48592 | Running loss: 0.81446 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4032 | Classification loss: 0.27978 | Regression loss: 0.29173 | Running loss: 0.81189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4033 | Classification loss: 0.81687 | Regression loss: 0.68266 | Running loss: 0.81224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4034 | Classification loss: 0.86442 | Regression loss: 0.65818 | Running loss: 0.81476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4035 | Classification loss: 0.01785 | Regression loss: 0.06635 | Running loss: 0.81354 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4036 | Classification loss: 0.93456 | Regression loss: 0.47802 | Running loss: 0.81366 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4037 | Classification loss: 1.23346 | Regression loss: 0.50281 | Running loss: 0.81665 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4038 | Classification loss: 0.10524 | Regression loss: 0.27203 | Running loss: 0.81491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4039 | Classification loss: 0.57170 | Regression loss: 0.41114 | Running loss: 0.81573 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4040 | Classification loss: 0.00046 | Regression loss: 0.14043 | Running loss: 0.81463 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4041 | Classification loss: 0.41919 | Regression loss: 0.43187 | Running loss: 0.81364 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4042 | Classification loss: 0.31079 | Regression loss: 0.28748 | Running loss: 0.81182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4043 | Classification loss: 0.11337 | Regression loss: 0.15720 | Running loss: 0.81075 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4044 | Classification loss: 0.28784 | Regression loss: 0.30901 | Running loss: 0.80967 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4045 | Classification loss: 0.44403 | Regression loss: 0.35550 | Running loss: 0.81054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4046 | Classification loss: 0.11350 | Regression loss: 0.36478 | Running loss: 0.81025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4047 | Classification loss: 1.01804 | Regression loss: 0.62170 | Running loss: 0.80978 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4048 | Classification loss: 1.28570 | Regression loss: 0.33517 | Running loss: 0.81235 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4049 | Classification loss: 0.45367 | Regression loss: 0.15178 | Running loss: 0.81332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4050 | Classification loss: 0.30338 | Regression loss: 0.37866 | Running loss: 0.81447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4051 | Classification loss: 0.19287 | Regression loss: 0.36576 | Running loss: 0.80937 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4052 | Classification loss: 0.00399 | Regression loss: 0.19374 | Running loss: 0.80940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4053 | Classification loss: 0.23584 | Regression loss: 0.46190 | Running loss: 0.81055 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4054 | Classification loss: 0.23181 | Regression loss: 0.21190 | Running loss: 0.80980 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4055 | Classification loss: 0.79179 | Regression loss: 0.59436 | Running loss: 0.81239 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4056 | Classification loss: 0.01121 | Regression loss: 0.18003 | Running loss: 0.80686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4057 | Classification loss: 0.35972 | Regression loss: 0.30741 | Running loss: 0.80646 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4058 | Classification loss: 0.02267 | Regression loss: 0.15763 | Running loss: 0.80321 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4059 | Classification loss: 0.18745 | Regression loss: 0.26232 | Running loss: 0.80139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4060 | Classification loss: 0.19977 | Regression loss: 0.27621 | Running loss: 0.79956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4061 | Classification loss: 0.94215 | Regression loss: 0.36850 | Running loss: 0.80139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4062 | Classification loss: 0.56760 | Regression loss: 0.19758 | Running loss: 0.79931 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4063 | Classification loss: 0.87808 | Regression loss: 0.56868 | Running loss: 0.80179 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4064 | Classification loss: 0.26127 | Regression loss: 0.34006 | Running loss: 0.79876 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4065 | Classification loss: 1.91867 | Regression loss: 0.26562 | Running loss: 0.80156 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4066 | Classification loss: 0.89636 | Regression loss: 0.54915 | Running loss: 0.80401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4067 | Classification loss: 0.00342 | Regression loss: 0.18585 | Running loss: 0.80345 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4068 | Classification loss: 0.40735 | Regression loss: 0.39305 | Running loss: 0.80484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4069 | Classification loss: 0.11723 | Regression loss: 0.24217 | Running loss: 0.80508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4070 | Classification loss: 0.10581 | Regression loss: 0.45387 | Running loss: 0.80355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4071 | Classification loss: 0.66054 | Regression loss: 0.27504 | Running loss: 0.80347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4072 | Classification loss: 0.01433 | Regression loss: 0.22677 | Running loss: 0.80298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4073 | Classification loss: 4.12827 | Regression loss: 0.22928 | Running loss: 0.80834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4074 | Classification loss: 0.98725 | Regression loss: 0.44897 | Running loss: 0.81044 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4075 | Classification loss: 0.11701 | Regression loss: 0.24533 | Running loss: 0.80954 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4076 | Classification loss: 0.44927 | Regression loss: 0.47230 | Running loss: 0.80803 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4077 | Classification loss: 0.46140 | Regression loss: 0.31431 | Running loss: 0.80790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4078 | Classification loss: 0.12865 | Regression loss: 0.28497 | Running loss: 0.80744 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4079 | Classification loss: 0.39600 | Regression loss: 0.38207 | Running loss: 0.80767 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4080 | Classification loss: 0.04323 | Regression loss: 0.30874 | Running loss: 0.80617 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4081 | Classification loss: 0.05672 | Regression loss: 0.30395 | Running loss: 0.80635 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4082 | Classification loss: 0.79710 | Regression loss: 0.71570 | Running loss: 0.80887 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4083 | Classification loss: 0.25823 | Regression loss: 0.57845 | Running loss: 0.81019 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4084 | Classification loss: 0.19857 | Regression loss: 0.30951 | Running loss: 0.80941 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4085 | Classification loss: 0.53435 | Regression loss: 0.45838 | Running loss: 0.81038 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4086 | Classification loss: 1.14924 | Regression loss: 0.56273 | Running loss: 0.81113 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4087 | Classification loss: 0.02891 | Regression loss: 0.08725 | Running loss: 0.81052 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4088 | Classification loss: 1.39619 | Regression loss: 0.62280 | Running loss: 0.81062 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4089 | Classification loss: 0.16684 | Regression loss: 0.32075 | Running loss: 0.80833 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4090 | Classification loss: 0.24868 | Regression loss: 0.42153 | Running loss: 0.80898 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4091 | Classification loss: 0.68008 | Regression loss: 0.39378 | Running loss: 0.81053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4092 | Classification loss: 0.06328 | Regression loss: 0.26187 | Running loss: 0.81006 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4093 | Classification loss: 0.25916 | Regression loss: 0.25908 | Running loss: 0.81109 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4094 | Classification loss: 0.06324 | Regression loss: 0.29406 | Running loss: 0.81152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4095 | Classification loss: 0.33217 | Regression loss: 0.42166 | Running loss: 0.81093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4096 | Classification loss: 0.08659 | Regression loss: 0.16466 | Running loss: 0.81078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4097 | Classification loss: 0.02567 | Regression loss: 0.09974 | Running loss: 0.80906 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4098 | Classification loss: 1.04114 | Regression loss: 0.74826 | Running loss: 0.80737 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4099 | Classification loss: 0.07624 | Regression loss: 0.23666 | Running loss: 0.80551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4100 | Classification loss: 0.09649 | Regression loss: 0.15481 | Running loss: 0.80345 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4101 | Classification loss: 0.37716 | Regression loss: 0.54018 | Running loss: 0.80500 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4102 | Classification loss: 0.49106 | Regression loss: 0.32823 | Running loss: 0.80397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4103 | Classification loss: 0.08105 | Regression loss: 0.22911 | Running loss: 0.80410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4104 | Classification loss: 0.85041 | Regression loss: 0.49841 | Running loss: 0.80610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4105 | Classification loss: 0.00422 | Regression loss: 0.08580 | Running loss: 0.80602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4106 | Classification loss: 0.03661 | Regression loss: 0.28023 | Running loss: 0.80395 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4107 | Classification loss: 0.04239 | Regression loss: 0.32649 | Running loss: 0.80371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4108 | Classification loss: 0.97032 | Regression loss: 0.44918 | Running loss: 0.80426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4109 | Classification loss: 0.04741 | Regression loss: 0.14692 | Running loss: 0.80423 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4110 | Classification loss: 0.08847 | Regression loss: 0.22553 | Running loss: 0.80186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4111 | Classification loss: 0.18973 | Regression loss: 0.15965 | Running loss: 0.80127 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4112 | Classification loss: 1.26973 | Regression loss: 0.54502 | Running loss: 0.80438 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4113 | Classification loss: 0.53623 | Regression loss: 0.60467 | Running loss: 0.80431 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4114 | Classification loss: 0.02098 | Regression loss: 0.20007 | Running loss: 0.80376 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4115 | Classification loss: 0.44597 | Regression loss: 0.58642 | Running loss: 0.80375 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4116 | Classification loss: 0.01482 | Regression loss: 0.04913 | Running loss: 0.80204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4117 | Classification loss: 0.01301 | Regression loss: 0.11179 | Running loss: 0.80170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4118 | Classification loss: 0.00415 | Regression loss: 0.15018 | Running loss: 0.80141 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4119 | Classification loss: 0.38266 | Regression loss: 0.40287 | Running loss: 0.80248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4120 | Classification loss: 1.13774 | Regression loss: 0.60548 | Running loss: 0.80578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4121 | Classification loss: 0.24643 | Regression loss: 0.37878 | Running loss: 0.80413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4122 | Classification loss: 0.08957 | Regression loss: 0.14063 | Running loss: 0.80379 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4123 | Classification loss: 0.48529 | Regression loss: 0.27799 | Running loss: 0.80514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4124 | Classification loss: 0.19403 | Regression loss: 0.37667 | Running loss: 0.80424 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4125 | Classification loss: 0.21336 | Regression loss: 0.27243 | Running loss: 0.80410 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4126 | Classification loss: 0.00037 | Regression loss: 0.05003 | Running loss: 0.80199 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4127 | Classification loss: 0.43885 | Regression loss: 0.14694 | Running loss: 0.80187 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4128 | Classification loss: 0.47528 | Regression loss: 0.36345 | Running loss: 0.80338 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4129 | Classification loss: 0.09917 | Regression loss: 0.31068 | Running loss: 0.80390 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4130 | Classification loss: 0.24444 | Regression loss: 0.58174 | Running loss: 0.80467 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4131 | Classification loss: 0.00813 | Regression loss: 0.21788 | Running loss: 0.80421 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4132 | Classification loss: 0.00979 | Regression loss: 0.10924 | Running loss: 0.80359 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4133 | Classification loss: 0.12591 | Regression loss: 0.41415 | Running loss: 0.80320 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4134 | Classification loss: 0.21462 | Regression loss: 0.23528 | Running loss: 0.80287 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4135 | Classification loss: 2.31747 | Regression loss: 0.53459 | Running loss: 0.80503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4136 | Classification loss: 2.64376 | Regression loss: 0.65702 | Running loss: 0.80847 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4137 | Classification loss: 0.97370 | Regression loss: 0.52195 | Running loss: 0.81113 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4138 | Classification loss: 0.68088 | Regression loss: 0.43446 | Running loss: 0.80961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4139 | Classification loss: 0.39384 | Regression loss: 0.49093 | Running loss: 0.81102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4140 | Classification loss: 0.07804 | Regression loss: 0.24251 | Running loss: 0.80867 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4141 | Classification loss: 0.00049 | Regression loss: 0.03404 | Running loss: 0.80815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4142 | Classification loss: 0.47234 | Regression loss: 0.20078 | Running loss: 0.80822 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4143 | Classification loss: 0.38210 | Regression loss: 0.32921 | Running loss: 0.80738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4144 | Classification loss: 0.71615 | Regression loss: 0.55417 | Running loss: 0.80971 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4145 | Classification loss: 0.07140 | Regression loss: 0.14033 | Running loss: 0.80903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4146 | Classification loss: 0.11361 | Regression loss: 0.23713 | Running loss: 0.80630 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4147 | Classification loss: 0.15987 | Regression loss: 0.34957 | Running loss: 0.80699 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4148 | Classification loss: 0.05846 | Regression loss: 0.13344 | Running loss: 0.80418 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4149 | Classification loss: 0.19341 | Regression loss: 0.07697 | Running loss: 0.80370 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4150 | Classification loss: 0.81521 | Regression loss: 0.04868 | Running loss: 0.80343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4151 | Classification loss: 0.04736 | Regression loss: 0.08668 | Running loss: 0.80306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4152 | Classification loss: 0.18397 | Regression loss: 0.42275 | Running loss: 0.80388 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4153 | Classification loss: 0.01155 | Regression loss: 0.12534 | Running loss: 0.80383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4154 | Classification loss: 1.08736 | Regression loss: 0.36202 | Running loss: 0.80583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4155 | Classification loss: 0.00819 | Regression loss: 0.15144 | Running loss: 0.80537 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4156 | Classification loss: 0.60778 | Regression loss: 0.59377 | Running loss: 0.80756 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4157 | Classification loss: 0.64313 | Regression loss: 0.38677 | Running loss: 0.80918 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4158 | Classification loss: 0.36402 | Regression loss: 0.39177 | Running loss: 0.80735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4159 | Classification loss: 0.02546 | Regression loss: 0.19503 | Running loss: 0.80538 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4160 | Classification loss: 0.11267 | Regression loss: 0.22886 | Running loss: 0.80459 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4161 | Classification loss: 0.09587 | Regression loss: 0.25696 | Running loss: 0.80508 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4162 | Classification loss: 0.00271 | Regression loss: 0.10586 | Running loss: 0.80481 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4163 | Classification loss: 0.39791 | Regression loss: 0.44139 | Running loss: 0.80606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4164 | Classification loss: 2.73990 | Regression loss: 0.81282 | Running loss: 0.81008 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4165 | Classification loss: 0.10290 | Regression loss: 0.20679 | Running loss: 0.80927 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4166 | Classification loss: 0.28393 | Regression loss: 0.39832 | Running loss: 0.80956 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4167 | Classification loss: 0.16191 | Regression loss: 0.29915 | Running loss: 0.81020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4168 | Classification loss: 0.15442 | Regression loss: 0.26217 | Running loss: 0.80899 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4169 | Classification loss: 0.66575 | Regression loss: 0.40986 | Running loss: 0.80813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4170 | Classification loss: 0.00597 | Regression loss: 0.16366 | Running loss: 0.80844 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4171 | Classification loss: 0.79055 | Regression loss: 0.41335 | Running loss: 0.81004 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4172 | Classification loss: 0.03489 | Regression loss: 0.10595 | Running loss: 0.80787 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4173 | Classification loss: 0.39599 | Regression loss: 0.40238 | Running loss: 0.80754 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4174 | Classification loss: 0.50085 | Regression loss: 0.51922 | Running loss: 0.80949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4175 | Classification loss: 1.02726 | Regression loss: 0.68494 | Running loss: 0.81204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4176 | Classification loss: 3.13746 | Regression loss: 0.75894 | Running loss: 0.81788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4177 | Classification loss: 1.99704 | Regression loss: 0.68820 | Running loss: 0.82120 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4178 | Classification loss: 0.70258 | Regression loss: 0.58513 | Running loss: 0.82299 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4179 | Classification loss: 1.33989 | Regression loss: 0.44997 | Running loss: 0.82584 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4180 | Classification loss: 0.70783 | Regression loss: 0.61311 | Running loss: 0.82612 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4181 | Classification loss: 0.42289 | Regression loss: 0.11133 | Running loss: 0.82686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4182 | Classification loss: 0.30229 | Regression loss: 0.42507 | Running loss: 0.82790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4183 | Classification loss: 0.40480 | Regression loss: 0.45072 | Running loss: 0.82827 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4184 | Classification loss: 1.09930 | Regression loss: 0.66420 | Running loss: 0.82987 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4185 | Classification loss: 0.49585 | Regression loss: 0.46219 | Running loss: 0.82931 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4186 | Classification loss: 0.10638 | Regression loss: 0.26219 | Running loss: 0.82914 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4187 | Classification loss: 0.00214 | Regression loss: 0.13839 | Running loss: 0.82718 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4188 | Classification loss: 0.66938 | Regression loss: 0.45976 | Running loss: 0.82837 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4189 | Classification loss: 1.05741 | Regression loss: 0.64694 | Running loss: 0.83094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4190 | Classification loss: 1.16764 | Regression loss: 0.30859 | Running loss: 0.83311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4191 | Classification loss: 0.29138 | Regression loss: 0.50086 | Running loss: 0.83380 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4192 | Classification loss: 0.08395 | Regression loss: 0.21235 | Running loss: 0.83254 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4193 | Classification loss: 0.25590 | Regression loss: 0.29668 | Running loss: 0.83208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4194 | Classification loss: 0.02119 | Regression loss: 0.16258 | Running loss: 0.83144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4195 | Classification loss: 0.05587 | Regression loss: 0.10502 | Running loss: 0.83116 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4196 | Classification loss: 0.08128 | Regression loss: 0.24828 | Running loss: 0.82835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4197 | Classification loss: 0.17248 | Regression loss: 0.24608 | Running loss: 0.82917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4198 | Classification loss: 0.30766 | Regression loss: 0.24290 | Running loss: 0.82942 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4199 | Classification loss: 0.82482 | Regression loss: 0.44170 | Running loss: 0.83057 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4200 | Classification loss: 0.00079 | Regression loss: 0.06740 | Running loss: 0.82906 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4201 | Classification loss: 0.85813 | Regression loss: 0.50081 | Running loss: 0.82792 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4202 | Classification loss: 0.53677 | Regression loss: 0.29991 | Running loss: 0.82940 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4203 | Classification loss: 0.23665 | Regression loss: 0.23781 | Running loss: 0.83024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4204 | Classification loss: 0.78513 | Regression loss: 0.51245 | Running loss: 0.83118 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4205 | Classification loss: 0.20472 | Regression loss: 0.34932 | Running loss: 0.83102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4206 | Classification loss: 0.06008 | Regression loss: 0.22912 | Running loss: 0.83020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4207 | Classification loss: 0.00157 | Regression loss: 0.09679 | Running loss: 0.82877 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4208 | Classification loss: 0.72373 | Regression loss: 0.25792 | Running loss: 0.82764 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4209 | Classification loss: 0.04522 | Regression loss: 0.17544 | Running loss: 0.82781 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4210 | Classification loss: 0.11685 | Regression loss: 0.28712 | Running loss: 0.82667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4211 | Classification loss: 0.84421 | Regression loss: 0.40054 | Running loss: 0.82809 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4212 | Classification loss: 1.80624 | Regression loss: 0.86861 | Running loss: 0.83145 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4213 | Classification loss: 1.08909 | Regression loss: 0.66460 | Running loss: 0.83407 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4214 | Classification loss: 0.00484 | Regression loss: 0.08967 | Running loss: 0.83391 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4215 | Classification loss: 0.00889 | Regression loss: 0.09084 | Running loss: 0.83329 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4216 | Classification loss: 1.00486 | Regression loss: 0.85225 | Running loss: 0.83650 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4217 | Classification loss: 1.12821 | Regression loss: 0.60761 | Running loss: 0.83898 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4218 | Classification loss: 0.28446 | Regression loss: 0.38983 | Running loss: 0.83839 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4219 | Classification loss: 0.00164 | Regression loss: 0.09542 | Running loss: 0.83713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4220 | Classification loss: 0.98518 | Regression loss: 0.58340 | Running loss: 0.83703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4221 | Classification loss: 1.31439 | Regression loss: 0.62133 | Running loss: 0.83966 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4222 | Classification loss: 0.14644 | Regression loss: 0.30304 | Running loss: 0.83736 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4223 | Classification loss: 0.01751 | Regression loss: 0.11163 | Running loss: 0.83758 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4224 | Classification loss: 0.49539 | Regression loss: 0.44846 | Running loss: 0.83889 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4225 | Classification loss: 0.83814 | Regression loss: 0.55464 | Running loss: 0.83982 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4226 | Classification loss: 0.18429 | Regression loss: 0.19465 | Running loss: 0.84001 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4227 | Classification loss: 0.36855 | Regression loss: 0.18921 | Running loss: 0.84082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4228 | Classification loss: 0.32664 | Regression loss: 0.56336 | Running loss: 0.83959 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4229 | Classification loss: 0.30516 | Regression loss: 0.22684 | Running loss: 0.83903 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4230 | Classification loss: 0.00742 | Regression loss: 0.04869 | Running loss: 0.83629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4231 | Classification loss: 0.42030 | Regression loss: 0.48963 | Running loss: 0.83406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4232 | Classification loss: 0.65598 | Regression loss: 0.60762 | Running loss: 0.83631 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4233 | Classification loss: 0.57318 | Regression loss: 0.55949 | Running loss: 0.83491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4234 | Classification loss: 0.04260 | Regression loss: 0.17555 | Running loss: 0.83412 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4235 | Classification loss: 0.02648 | Regression loss: 0.20524 | Running loss: 0.83398 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4236 | Classification loss: 0.81640 | Regression loss: 0.27610 | Running loss: 0.83365 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4237 | Classification loss: 0.06234 | Regression loss: 0.31633 | Running loss: 0.83111 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4238 | Classification loss: 0.72090 | Regression loss: 0.26601 | Running loss: 0.83234 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4239 | Classification loss: 0.20638 | Regression loss: 0.23331 | Running loss: 0.83237 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4240 | Classification loss: 0.12513 | Regression loss: 0.47387 | Running loss: 0.83204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4241 | Classification loss: 0.10181 | Regression loss: 0.27120 | Running loss: 0.82691 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4242 | Classification loss: 0.02121 | Regression loss: 0.12663 | Running loss: 0.82641 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4243 | Classification loss: 0.85149 | Regression loss: 0.54541 | Running loss: 0.82862 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4244 | Classification loss: 0.15409 | Regression loss: 0.20130 | Running loss: 0.82697 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4245 | Classification loss: 0.32232 | Regression loss: 0.28877 | Running loss: 0.82600 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4246 | Classification loss: 0.04869 | Regression loss: 0.18204 | Running loss: 0.82337 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4247 | Classification loss: 0.00148 | Regression loss: 0.06304 | Running loss: 0.82223 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4248 | Classification loss: 0.78158 | Regression loss: 0.43724 | Running loss: 0.82417 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4249 | Classification loss: 0.85392 | Regression loss: 0.42913 | Running loss: 0.82632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4250 | Classification loss: 2.19009 | Regression loss: 0.64442 | Running loss: 0.83162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4251 | Classification loss: 1.79345 | Regression loss: 0.60486 | Running loss: 0.83406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4252 | Classification loss: 2.28194 | Regression loss: 0.54431 | Running loss: 0.83652 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4253 | Classification loss: 0.00920 | Regression loss: 0.15739 | Running loss: 0.83249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4254 | Classification loss: 1.91289 | Regression loss: 0.47369 | Running loss: 0.83206 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4255 | Classification loss: 0.30645 | Regression loss: 0.33228 | Running loss: 0.83318 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4256 | Classification loss: 0.04104 | Regression loss: 0.35275 | Running loss: 0.83389 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4257 | Classification loss: 0.73183 | Regression loss: 0.59704 | Running loss: 0.83634 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4258 | Classification loss: 0.60672 | Regression loss: 0.30067 | Running loss: 0.83495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4259 | Classification loss: 0.11082 | Regression loss: 0.27102 | Running loss: 0.83461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4260 | Classification loss: 0.58218 | Regression loss: 0.73203 | Running loss: 0.83590 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4261 | Classification loss: 4.35980 | Regression loss: 0.58284 | Running loss: 0.84538 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4262 | Classification loss: 0.41036 | Regression loss: 0.30646 | Running loss: 0.84401 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4263 | Classification loss: 1.98311 | Regression loss: 0.58021 | Running loss: 0.84657 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4264 | Classification loss: 0.85797 | Regression loss: 0.26689 | Running loss: 0.84831 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4265 | Classification loss: 0.31757 | Regression loss: 0.45459 | Running loss: 0.84904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4266 | Classification loss: 0.46585 | Regression loss: 0.53121 | Running loss: 0.85051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4267 | Classification loss: 0.87135 | Regression loss: 0.56088 | Running loss: 0.85320 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4268 | Classification loss: 0.00910 | Regression loss: 0.11558 | Running loss: 0.85078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4269 | Classification loss: 0.07827 | Regression loss: 0.26048 | Running loss: 0.85121 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4270 | Classification loss: 0.09536 | Regression loss: 0.31380 | Running loss: 0.84999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4271 | Classification loss: 0.08959 | Regression loss: 0.25251 | Running loss: 0.85028 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4272 | Classification loss: 0.07864 | Regression loss: 0.20416 | Running loss: 0.84662 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4273 | Classification loss: 1.77741 | Regression loss: 0.39326 | Running loss: 0.84936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4274 | Classification loss: 0.15697 | Regression loss: 0.30990 | Running loss: 0.84918 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4275 | Classification loss: 0.01545 | Regression loss: 0.05964 | Running loss: 0.84648 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4276 | Classification loss: 0.01610 | Regression loss: 0.00000 | Running loss: 0.84611 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 4277 | Classification loss: 0.50881 | Regression loss: 0.45678 | Running loss: 0.84411 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4278 | Classification loss: 0.02188 | Regression loss: 0.27933 | Running loss: 0.84328 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4279 | Classification loss: 0.15888 | Regression loss: 0.34601 | Running loss: 0.84278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4280 | Classification loss: 0.08074 | Regression loss: 0.21866 | Running loss: 0.84242 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4281 | Classification loss: 0.03720 | Regression loss: 0.16911 | Running loss: 0.84220 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4282 | Classification loss: 0.00807 | Regression loss: 0.11571 | Running loss: 0.83922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4283 | Classification loss: 0.16065 | Regression loss: 0.22874 | Running loss: 0.83671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4284 | Classification loss: 0.41821 | Regression loss: 0.51772 | Running loss: 0.83594 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4285 | Classification loss: 0.02093 | Regression loss: 0.10199 | Running loss: 0.83582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4286 | Classification loss: 0.01609 | Regression loss: 0.15883 | Running loss: 0.83174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4287 | Classification loss: 0.06250 | Regression loss: 0.10097 | Running loss: 0.83116 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4288 | Classification loss: 0.13202 | Regression loss: 0.29716 | Running loss: 0.83078 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4289 | Classification loss: 0.02498 | Regression loss: 0.21340 | Running loss: 0.82817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4290 | Classification loss: 0.83989 | Regression loss: 0.40118 | Running loss: 0.82933 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4291 | Classification loss: 0.06234 | Regression loss: 0.19543 | Running loss: 0.82720 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4292 | Classification loss: 0.02019 | Regression loss: 0.28114 | Running loss: 0.82674 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4293 | Classification loss: 0.20768 | Regression loss: 0.31986 | Running loss: 0.82746 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4294 | Classification loss: 0.04139 | Regression loss: 0.14977 | Running loss: 0.82704 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4295 | Classification loss: 0.18210 | Regression loss: 0.16609 | Running loss: 0.82597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4296 | Classification loss: 1.56933 | Regression loss: 0.63905 | Running loss: 0.82828 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4297 | Classification loss: 0.31583 | Regression loss: 0.22699 | Running loss: 0.82583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4298 | Classification loss: 0.05704 | Regression loss: 0.10152 | Running loss: 0.82448 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4299 | Classification loss: 0.61352 | Regression loss: 0.58884 | Running loss: 0.82503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4300 | Classification loss: 0.86799 | Regression loss: 0.61486 | Running loss: 0.82656 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4301 | Classification loss: 0.39690 | Regression loss: 0.47199 | Running loss: 0.82566 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4302 | Classification loss: 0.15174 | Regression loss: 0.19088 | Running loss: 0.82526 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4303 | Classification loss: 0.42416 | Regression loss: 0.46299 | Running loss: 0.82608 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4304 | Classification loss: 0.68998 | Regression loss: 0.66069 | Running loss: 0.82866 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4305 | Classification loss: 0.28636 | Regression loss: 0.14981 | Running loss: 0.82713 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4306 | Classification loss: 0.17197 | Regression loss: 0.25558 | Running loss: 0.82561 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4307 | Classification loss: 0.49369 | Regression loss: 0.46142 | Running loss: 0.82602 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4308 | Classification loss: 0.57313 | Regression loss: 0.40960 | Running loss: 0.82710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4309 | Classification loss: 0.25771 | Regression loss: 0.65737 | Running loss: 0.82768 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4310 | Classification loss: 0.00236 | Regression loss: 0.04125 | Running loss: 0.82630 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4311 | Classification loss: 1.60953 | Regression loss: 0.52335 | Running loss: 0.82824 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4312 | Classification loss: 0.61168 | Regression loss: 0.38926 | Running loss: 0.82726 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4313 | Classification loss: 0.15480 | Regression loss: 0.25544 | Running loss: 0.82668 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4314 | Classification loss: 0.16877 | Regression loss: 0.31955 | Running loss: 0.82736 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4315 | Classification loss: 0.55887 | Regression loss: 0.53339 | Running loss: 0.82520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4316 | Classification loss: 1.98598 | Regression loss: 0.80762 | Running loss: 0.82847 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4317 | Classification loss: 0.64703 | Regression loss: 0.36928 | Running loss: 0.82561 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4318 | Classification loss: 0.22596 | Regression loss: 0.23507 | Running loss: 0.82378 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4319 | Classification loss: 0.01210 | Regression loss: 0.12456 | Running loss: 0.82373 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4320 | Classification loss: 0.77909 | Regression loss: 0.40037 | Running loss: 0.82272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4321 | Classification loss: 0.34512 | Regression loss: 0.41815 | Running loss: 0.81904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4322 | Classification loss: 0.26543 | Regression loss: 0.41003 | Running loss: 0.81975 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4323 | Classification loss: 0.00026 | Regression loss: 0.11460 | Running loss: 0.81960 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4324 | Classification loss: 0.09292 | Regression loss: 0.33165 | Running loss: 0.81577 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4325 | Classification loss: 0.00112 | Regression loss: 0.11308 | Running loss: 0.81567 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4326 | Classification loss: 0.23308 | Regression loss: 0.36879 | Running loss: 0.81646 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4327 | Classification loss: 0.02154 | Regression loss: 0.08442 | Running loss: 0.81390 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4328 | Classification loss: 0.00117 | Regression loss: 0.09057 | Running loss: 0.81383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4329 | Classification loss: 0.15705 | Regression loss: 0.17971 | Running loss: 0.81174 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4330 | Classification loss: 0.10206 | Regression loss: 0.45574 | Running loss: 0.81130 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4331 | Classification loss: 0.02517 | Regression loss: 0.26219 | Running loss: 0.81149 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4332 | Classification loss: 0.00174 | Regression loss: 0.09483 | Running loss: 0.81130 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4333 | Classification loss: 0.54659 | Regression loss: 0.62348 | Running loss: 0.81011 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4334 | Classification loss: 0.34941 | Regression loss: 0.28307 | Running loss: 0.81029 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4335 | Classification loss: 1.00590 | Regression loss: 0.27037 | Running loss: 0.80973 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4336 | Classification loss: 0.00027 | Regression loss: 0.09999 | Running loss: 0.80971 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4337 | Classification loss: 1.68210 | Regression loss: 0.66222 | Running loss: 0.81313 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4338 | Classification loss: 0.26699 | Regression loss: 0.12498 | Running loss: 0.81102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4339 | Classification loss: 0.21681 | Regression loss: 0.25398 | Running loss: 0.81093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4340 | Classification loss: 0.40848 | Regression loss: 0.48371 | Running loss: 0.81213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4341 | Classification loss: 0.08231 | Regression loss: 0.09026 | Running loss: 0.81202 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4342 | Classification loss: 0.25375 | Regression loss: 0.39139 | Running loss: 0.81208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4343 | Classification loss: 0.00289 | Regression loss: 0.12216 | Running loss: 0.80987 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4344 | Classification loss: 0.00045 | Regression loss: 0.07601 | Running loss: 0.80973 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4345 | Classification loss: 0.12716 | Regression loss: 0.25719 | Running loss: 0.80865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4346 | Classification loss: 0.47499 | Regression loss: 0.54786 | Running loss: 0.80999 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4347 | Classification loss: 0.42627 | Regression loss: 0.42108 | Running loss: 0.80703 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4348 | Classification loss: 0.00210 | Regression loss: 0.12584 | Running loss: 0.80696 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4349 | Classification loss: 0.00617 | Regression loss: 0.08212 | Running loss: 0.80481 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4350 | Classification loss: 0.76214 | Regression loss: 0.32439 | Running loss: 0.80565 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4351 | Classification loss: 0.45991 | Regression loss: 0.79768 | Running loss: 0.80637 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4352 | Classification loss: 0.85321 | Regression loss: 0.66481 | Running loss: 0.80698 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4353 | Classification loss: 0.05753 | Regression loss: 0.23593 | Running loss: 0.80693 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4354 | Classification loss: 0.26422 | Regression loss: 0.43809 | Running loss: 0.80693 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4355 | Classification loss: 0.09576 | Regression loss: 0.20934 | Running loss: 0.80645 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4356 | Classification loss: 0.02385 | Regression loss: 0.16026 | Running loss: 0.80312 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4357 | Classification loss: 0.03254 | Regression loss: 0.16154 | Running loss: 0.80143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4358 | Classification loss: 0.32348 | Regression loss: 0.63473 | Running loss: 0.80036 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4359 | Classification loss: 0.30496 | Regression loss: 0.28654 | Running loss: 0.79851 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4360 | Classification loss: 0.03824 | Regression loss: 0.21608 | Running loss: 0.79826 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4361 | Classification loss: 1.21821 | Regression loss: 0.31814 | Running loss: 0.79789 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4362 | Classification loss: 0.15216 | Regression loss: 0.12418 | Running loss: 0.79810 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4363 | Classification loss: 0.02975 | Regression loss: 0.21665 | Running loss: 0.79585 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4364 | Classification loss: 0.10803 | Regression loss: 0.31881 | Running loss: 0.79565 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4365 | Classification loss: 0.40195 | Regression loss: 0.43997 | Running loss: 0.79702 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4366 | Classification loss: 0.09468 | Regression loss: 0.00000 | Running loss: 0.79616 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4367 | Classification loss: 2.09635 | Regression loss: 0.44723 | Running loss: 0.80094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4368 | Classification loss: 0.14137 | Regression loss: 0.33691 | Running loss: 0.80103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4369 | Classification loss: 1.01396 | Regression loss: 0.29263 | Running loss: 0.79778 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4370 | Classification loss: 0.61711 | Regression loss: 0.43325 | Running loss: 0.79832 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4371 | Classification loss: 0.76697 | Regression loss: 0.61000 | Running loss: 0.79997 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4372 | Classification loss: 0.00046 | Regression loss: 0.05433 | Running loss: 0.79686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4373 | Classification loss: 0.51607 | Regression loss: 0.59195 | Running loss: 0.79801 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4374 | Classification loss: 0.51859 | Regression loss: 0.34996 | Running loss: 0.79714 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4375 | Classification loss: 1.05472 | Regression loss: 0.20951 | Running loss: 0.79849 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4376 | Classification loss: 2.13000 | Regression loss: 0.68914 | Running loss: 0.80246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4377 | Classification loss: 0.55985 | Regression loss: 0.36310 | Running loss: 0.80400 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4378 | Classification loss: 0.37533 | Regression loss: 0.57265 | Running loss: 0.80407 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4379 | Classification loss: 0.52808 | Regression loss: 0.44754 | Running loss: 0.80578 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4380 | Classification loss: 0.06543 | Regression loss: 0.20989 | Running loss: 0.80617 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4381 | Classification loss: 0.79970 | Regression loss: 0.38826 | Running loss: 0.80736 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4382 | Classification loss: 0.61969 | Regression loss: 0.60582 | Running loss: 0.80926 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4383 | Classification loss: 0.29789 | Regression loss: 0.43522 | Running loss: 0.80691 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4384 | Classification loss: 0.39243 | Regression loss: 0.49296 | Running loss: 0.80798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4385 | Classification loss: 0.12490 | Regression loss: 0.24318 | Running loss: 0.80812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4386 | Classification loss: 0.08453 | Regression loss: 0.14230 | Running loss: 0.80368 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4387 | Classification loss: 0.00252 | Regression loss: 0.10436 | Running loss: 0.80311 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4388 | Classification loss: 0.00008 | Regression loss: 0.00000 | Running loss: 0.80288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4389 | Classification loss: 0.26941 | Regression loss: 0.22366 | Running loss: 0.80309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4390 | Classification loss: 1.31499 | Regression loss: 0.64111 | Running loss: 0.80531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4391 | Classification loss: 0.71271 | Regression loss: 0.48662 | Running loss: 0.80728 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4392 | Classification loss: 0.10275 | Regression loss: 0.09271 | Running loss: 0.80757 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4393 | Classification loss: 2.12811 | Regression loss: 0.41157 | Running loss: 0.81118 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4394 | Classification loss: 0.08327 | Regression loss: 0.04538 | Running loss: 0.80906 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4395 | Classification loss: 0.01715 | Regression loss: 0.14440 | Running loss: 0.80912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4396 | Classification loss: 1.14388 | Regression loss: 0.58845 | Running loss: 0.81232 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4397 | Classification loss: 0.35683 | Regression loss: 0.18855 | Running loss: 0.81117 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4398 | Classification loss: 0.14807 | Regression loss: 0.40970 | Running loss: 0.81002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4399 | Classification loss: 1.69069 | Regression loss: 0.32124 | Running loss: 0.80860 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4400 | Classification loss: 0.04011 | Regression loss: 0.11219 | Running loss: 0.80780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4401 | Classification loss: 0.03704 | Regression loss: 0.27119 | Running loss: 0.80392 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4402 | Classification loss: 0.02137 | Regression loss: 0.23693 | Running loss: 0.80112 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4403 | Classification loss: 0.19871 | Regression loss: 0.41083 | Running loss: 0.80160 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4404 | Classification loss: 0.02201 | Regression loss: 0.16898 | Running loss: 0.79863 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4405 | Classification loss: 0.75566 | Regression loss: 0.58200 | Running loss: 0.79933 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4406 | Classification loss: 0.12938 | Regression loss: 0.40191 | Running loss: 0.79726 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4407 | Classification loss: 1.65434 | Regression loss: 0.56242 | Running loss: 0.79943 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4408 | Classification loss: 0.89466 | Regression loss: 0.11012 | Running loss: 0.80088 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4409 | Classification loss: 0.50117 | Regression loss: 0.47350 | Running loss: 0.80274 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4410 | Classification loss: 0.09077 | Regression loss: 0.25779 | Running loss: 0.80232 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4411 | Classification loss: 0.01163 | Regression loss: 0.08727 | Running loss: 0.80062 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4412 | Classification loss: 0.00395 | Regression loss: 0.20796 | Running loss: 0.80074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4413 | Classification loss: 0.05554 | Regression loss: 0.18890 | Running loss: 0.80066 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4414 | Classification loss: 0.01086 | Regression loss: 0.11667 | Running loss: 0.79981 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4415 | Classification loss: 0.45314 | Regression loss: 0.35560 | Running loss: 0.80056 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4416 | Classification loss: 0.17463 | Regression loss: 0.31450 | Running loss: 0.79945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4417 | Classification loss: 0.91514 | Regression loss: 0.32370 | Running loss: 0.80173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4418 | Classification loss: 0.81401 | Regression loss: 0.46698 | Running loss: 0.80284 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4419 | Classification loss: 0.55135 | Regression loss: 0.47145 | Running loss: 0.80346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4420 | Classification loss: 0.03254 | Regression loss: 0.10378 | Running loss: 0.80086 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4421 | Classification loss: 0.35114 | Regression loss: 0.35464 | Running loss: 0.79937 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4422 | Classification loss: 0.14430 | Regression loss: 0.43054 | Running loss: 0.79923 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4423 | Classification loss: 0.14007 | Regression loss: 0.31656 | Running loss: 0.79830 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4424 | Classification loss: 0.22813 | Regression loss: 0.52603 | Running loss: 0.79913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4425 | Classification loss: 0.17830 | Regression loss: 0.32636 | Running loss: 0.79908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4426 | Classification loss: 0.94008 | Regression loss: 0.57050 | Running loss: 0.80154 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4427 | Classification loss: 0.07295 | Regression loss: 0.20273 | Running loss: 0.80189 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4428 | Classification loss: 0.50918 | Regression loss: 0.34070 | Running loss: 0.80222 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4429 | Classification loss: 0.04068 | Regression loss: 0.00986 | Running loss: 0.80200 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4430 | Classification loss: 0.05414 | Regression loss: 0.23618 | Running loss: 0.79849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4431 | Classification loss: 0.11405 | Regression loss: 0.19200 | Running loss: 0.79607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4432 | Classification loss: 0.93578 | Regression loss: 0.67880 | Running loss: 0.79909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4433 | Classification loss: 0.00169 | Regression loss: 0.05098 | Running loss: 0.79848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4434 | Classification loss: 0.25723 | Regression loss: 0.38716 | Running loss: 0.79718 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4435 | Classification loss: 0.04630 | Regression loss: 0.25413 | Running loss: 0.79741 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4436 | Classification loss: 0.12958 | Regression loss: 0.27586 | Running loss: 0.79696 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4437 | Classification loss: 0.88218 | Regression loss: 0.40218 | Running loss: 0.79863 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4438 | Classification loss: 1.19022 | Regression loss: 0.62023 | Running loss: 0.80152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4439 | Classification loss: 0.09222 | Regression loss: 0.20639 | Running loss: 0.80107 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4440 | Classification loss: 0.26798 | Regression loss: 0.62901 | Running loss: 0.79932 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4441 | Classification loss: 0.53095 | Regression loss: 0.56995 | Running loss: 0.80070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4442 | Classification loss: 0.02479 | Regression loss: 0.10799 | Running loss: 0.80052 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4443 | Classification loss: 0.49246 | Regression loss: 0.44984 | Running loss: 0.80198 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4444 | Classification loss: 0.20908 | Regression loss: 0.33213 | Running loss: 0.79985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4445 | Classification loss: 0.27709 | Regression loss: 0.22416 | Running loss: 0.79812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4446 | Classification loss: 1.02031 | Regression loss: 0.53024 | Running loss: 0.79919 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4447 | Classification loss: 0.01439 | Regression loss: 0.09757 | Running loss: 0.79887 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4448 | Classification loss: 0.12954 | Regression loss: 0.22490 | Running loss: 0.79913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4449 | Classification loss: 1.60528 | Regression loss: 0.67883 | Running loss: 0.80275 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4450 | Classification loss: 0.60683 | Regression loss: 0.33192 | Running loss: 0.80391 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4451 | Classification loss: 0.16923 | Regression loss: 0.32699 | Running loss: 0.80351 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4452 | Classification loss: 0.21973 | Regression loss: 0.36541 | Running loss: 0.80170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4453 | Classification loss: 0.02495 | Regression loss: 0.27675 | Running loss: 0.79905 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4454 | Classification loss: 0.00283 | Regression loss: 0.22610 | Running loss: 0.79872 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4455 | Classification loss: 1.18318 | Regression loss: 0.76067 | Running loss: 0.80209 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4456 | Classification loss: 0.96685 | Regression loss: 0.51922 | Running loss: 0.80479 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4457 | Classification loss: 0.05294 | Regression loss: 0.13390 | Running loss: 0.80261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4458 | Classification loss: 1.12493 | Regression loss: 0.76897 | Running loss: 0.80526 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4459 | Classification loss: 0.19876 | Regression loss: 0.19706 | Running loss: 0.80348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4460 | Classification loss: 0.18179 | Regression loss: 0.34940 | Running loss: 0.80365 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4461 | Classification loss: 1.33564 | Regression loss: 0.81438 | Running loss: 0.80534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4462 | Classification loss: 0.30822 | Regression loss: 0.53320 | Running loss: 0.80503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4463 | Classification loss: 0.48074 | Regression loss: 0.45988 | Running loss: 0.80346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4464 | Classification loss: 0.00797 | Regression loss: 0.19435 | Running loss: 0.79810 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4465 | Classification loss: 0.61754 | Regression loss: 0.12190 | Running loss: 0.79908 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4466 | Classification loss: 0.02373 | Regression loss: 0.26810 | Running loss: 0.79808 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4467 | Classification loss: 0.81495 | Regression loss: 0.53382 | Running loss: 0.80040 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4468 | Classification loss: 2.20011 | Regression loss: 0.64232 | Running loss: 0.80507 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4469 | Classification loss: 0.03191 | Regression loss: 0.10724 | Running loss: 0.80514 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4470 | Classification loss: 0.91406 | Regression loss: 0.20232 | Running loss: 0.80543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4471 | Classification loss: 0.58958 | Regression loss: 0.56507 | Running loss: 0.80712 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4472 | Classification loss: 0.17085 | Regression loss: 0.36773 | Running loss: 0.80790 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4473 | Classification loss: 1.22183 | Regression loss: 0.72825 | Running loss: 0.81094 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4474 | Classification loss: 0.17339 | Regression loss: 0.23895 | Running loss: 0.80834 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4475 | Classification loss: 0.07486 | Regression loss: 0.10705 | Running loss: 0.80815 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4476 | Classification loss: 2.97428 | Regression loss: 0.79698 | Running loss: 0.81492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4477 | Classification loss: 0.83019 | Regression loss: 0.38496 | Running loss: 0.81685 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4478 | Classification loss: 0.37718 | Regression loss: 0.39362 | Running loss: 0.81700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4479 | Classification loss: 0.07908 | Regression loss: 0.19890 | Running loss: 0.81613 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4480 | Classification loss: 0.01629 | Regression loss: 0.00000 | Running loss: 0.81377 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4481 | Classification loss: 0.07036 | Regression loss: 0.14462 | Running loss: 0.81302 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4482 | Classification loss: 1.21215 | Regression loss: 0.46672 | Running loss: 0.81231 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4483 | Classification loss: 0.02647 | Regression loss: 0.15875 | Running loss: 0.80925 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4484 | Classification loss: 0.00762 | Regression loss: 0.08141 | Running loss: 0.80890 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4485 | Classification loss: 0.25828 | Regression loss: 0.14795 | Running loss: 0.80707 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4486 | Classification loss: 0.00092 | Regression loss: 0.02939 | Running loss: 0.80633 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4487 | Classification loss: 1.00283 | Regression loss: 0.62027 | Running loss: 0.80554 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4488 | Classification loss: 0.12197 | Regression loss: 0.50394 | Running loss: 0.80646 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4489 | Classification loss: 0.05874 | Regression loss: 0.16262 | Running loss: 0.80361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4490 | Classification loss: 0.11419 | Regression loss: 0.22447 | Running loss: 0.80240 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4491 | Classification loss: 0.46231 | Regression loss: 0.25895 | Running loss: 0.80325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4492 | Classification loss: 0.53935 | Regression loss: 0.80604 | Running loss: 0.80503 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4493 | Classification loss: 0.48779 | Regression loss: 0.53400 | Running loss: 0.80662 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4494 | Classification loss: 0.08977 | Regression loss: 0.31445 | Running loss: 0.80341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4495 | Classification loss: 0.04474 | Regression loss: 0.13000 | Running loss: 0.80205 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4496 | Classification loss: 0.00154 | Regression loss: 0.05564 | Running loss: 0.79918 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4497 | Classification loss: 0.21410 | Regression loss: 0.49554 | Running loss: 0.80002 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4498 | Classification loss: 1.29105 | Regression loss: 0.28183 | Running loss: 0.80239 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4499 | Classification loss: 1.97644 | Regression loss: 0.74069 | Running loss: 0.80599 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4500 | Classification loss: 0.03146 | Regression loss: 0.07929 | Running loss: 0.80557 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4501 | Classification loss: 0.71129 | Regression loss: 0.31865 | Running loss: 0.80735 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4502 | Classification loss: 0.05235 | Regression loss: 0.09261 | Running loss: 0.80452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4503 | Classification loss: 0.86855 | Regression loss: 0.43889 | Running loss: 0.80601 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4504 | Classification loss: 0.09279 | Regression loss: 0.14524 | Running loss: 0.80322 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4505 | Classification loss: 0.58243 | Regression loss: 0.46054 | Running loss: 0.80412 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4506 | Classification loss: 0.42748 | Regression loss: 0.56486 | Running loss: 0.80484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4507 | Classification loss: 0.00024 | Regression loss: 0.10074 | Running loss: 0.80461 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4508 | Classification loss: 0.09137 | Regression loss: 0.28083 | Running loss: 0.80360 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4509 | Classification loss: 0.32224 | Regression loss: 0.36935 | Running loss: 0.80025 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4510 | Classification loss: 0.05417 | Regression loss: 0.15885 | Running loss: 0.79985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4511 | Classification loss: 0.02029 | Regression loss: 0.18620 | Running loss: 0.79627 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4512 | Classification loss: 0.40144 | Regression loss: 0.31626 | Running loss: 0.79717 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4513 | Classification loss: 0.11738 | Regression loss: 0.16548 | Running loss: 0.79625 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4514 | Classification loss: 0.00150 | Regression loss: 0.09227 | Running loss: 0.79541 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4515 | Classification loss: 1.60442 | Regression loss: 0.17771 | Running loss: 0.79545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4516 | Classification loss: 0.00054 | Regression loss: 0.07642 | Running loss: 0.79458 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4517 | Classification loss: 0.13997 | Regression loss: 0.15079 | Running loss: 0.79409 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4518 | Classification loss: 0.33858 | Regression loss: 0.39745 | Running loss: 0.79484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4519 | Classification loss: 0.01204 | Regression loss: 0.18165 | Running loss: 0.79385 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4520 | Classification loss: 0.02338 | Regression loss: 0.14193 | Running loss: 0.79248 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4521 | Classification loss: 0.23978 | Regression loss: 0.21201 | Running loss: 0.79260 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4522 | Classification loss: 0.61450 | Regression loss: 0.64077 | Running loss: 0.79172 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4523 | Classification loss: 0.09400 | Regression loss: 0.14756 | Running loss: 0.79076 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4524 | Classification loss: 1.19211 | Regression loss: 0.46235 | Running loss: 0.79259 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4525 | Classification loss: 1.04674 | Regression loss: 0.49706 | Running loss: 0.79424 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4526 | Classification loss: 0.36403 | Regression loss: 0.08580 | Running loss: 0.79407 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4527 | Classification loss: 0.12855 | Regression loss: 0.30389 | Running loss: 0.78976 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4528 | Classification loss: 0.31240 | Regression loss: 0.48626 | Running loss: 0.79123 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4529 | Classification loss: 0.11530 | Regression loss: 0.23905 | Running loss: 0.78959 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4530 | Classification loss: 0.37326 | Regression loss: 0.37521 | Running loss: 0.79005 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4531 | Classification loss: 0.03166 | Regression loss: 0.15191 | Running loss: 0.78813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4532 | Classification loss: 0.67242 | Regression loss: 0.20974 | Running loss: 0.78875 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4533 | Classification loss: 0.21965 | Regression loss: 0.33382 | Running loss: 0.78686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4534 | Classification loss: 0.02240 | Regression loss: 0.10205 | Running loss: 0.78406 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4535 | Classification loss: 0.40724 | Regression loss: 0.47559 | Running loss: 0.78566 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4536 | Classification loss: 1.39439 | Regression loss: 0.47150 | Running loss: 0.78657 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4537 | Classification loss: 0.67749 | Regression loss: 0.26984 | Running loss: 0.78499 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4538 | Classification loss: 0.11412 | Regression loss: 0.31545 | Running loss: 0.78509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4539 | Classification loss: 0.57588 | Regression loss: 0.49041 | Running loss: 0.78526 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4540 | Classification loss: 0.03570 | Regression loss: 0.15559 | Running loss: 0.78536 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4541 | Classification loss: 0.56883 | Regression loss: 0.62049 | Running loss: 0.78604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4542 | Classification loss: 0.32329 | Regression loss: 0.41831 | Running loss: 0.78632 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4543 | Classification loss: 0.13201 | Regression loss: 0.36834 | Running loss: 0.78678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4544 | Classification loss: 0.97515 | Regression loss: 0.49124 | Running loss: 0.78852 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4545 | Classification loss: 0.08211 | Regression loss: 0.30651 | Running loss: 0.78770 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4546 | Classification loss: 0.49410 | Regression loss: 0.41320 | Running loss: 0.78856 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4547 | Classification loss: 0.00294 | Regression loss: 0.06138 | Running loss: 0.78541 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4548 | Classification loss: 0.07787 | Regression loss: 0.18868 | Running loss: 0.78270 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4549 | Classification loss: 0.03483 | Regression loss: 0.15246 | Running loss: 0.78186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4550 | Classification loss: 0.39761 | Regression loss: 0.36187 | Running loss: 0.78202 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4551 | Classification loss: 0.03382 | Regression loss: 0.23448 | Running loss: 0.78144 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4552 | Classification loss: 0.02268 | Regression loss: 0.12534 | Running loss: 0.78134 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4553 | Classification loss: 0.61094 | Regression loss: 0.34743 | Running loss: 0.78186 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4554 | Classification loss: 0.00202 | Regression loss: 0.13761 | Running loss: 0.78125 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4555 | Classification loss: 0.04567 | Regression loss: 0.12922 | Running loss: 0.77883 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4556 | Classification loss: 0.99341 | Regression loss: 0.62258 | Running loss: 0.78168 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4557 | Classification loss: 0.55805 | Regression loss: 0.35972 | Running loss: 0.78218 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4558 | Classification loss: 0.34786 | Regression loss: 0.20424 | Running loss: 0.78292 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4559 | Classification loss: 1.82332 | Regression loss: 0.59504 | Running loss: 0.78686 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4560 | Classification loss: 0.44998 | Regression loss: 0.64004 | Running loss: 0.78809 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4561 | Classification loss: 0.12645 | Regression loss: 0.18906 | Running loss: 0.78610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4562 | Classification loss: 0.00289 | Regression loss: 0.09914 | Running loss: 0.78477 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4563 | Classification loss: 0.01383 | Regression loss: 0.12858 | Running loss: 0.78216 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4564 | Classification loss: 0.02057 | Regression loss: 0.19321 | Running loss: 0.78139 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4565 | Classification loss: 0.20943 | Regression loss: 0.23383 | Running loss: 0.77791 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4566 | Classification loss: 0.42945 | Regression loss: 0.44835 | Running loss: 0.77677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4567 | Classification loss: 0.87520 | Regression loss: 0.63977 | Running loss: 0.77942 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4568 | Classification loss: 1.14645 | Regression loss: 0.37949 | Running loss: 0.78087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4569 | Classification loss: 0.74859 | Regression loss: 0.45631 | Running loss: 0.78256 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4570 | Classification loss: 0.99383 | Regression loss: 0.53110 | Running loss: 0.78449 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4571 | Classification loss: 0.07486 | Regression loss: 0.24653 | Running loss: 0.78327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4572 | Classification loss: 1.16812 | Regression loss: 0.67739 | Running loss: 0.78647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4573 | Classification loss: 0.00039 | Regression loss: 0.04924 | Running loss: 0.77786 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4574 | Classification loss: 0.02240 | Regression loss: 0.12434 | Running loss: 0.77528 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4575 | Classification loss: 0.16519 | Regression loss: 0.61375 | Running loss: 0.77611 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4576 | Classification loss: 2.10063 | Regression loss: 0.32963 | Running loss: 0.77913 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4577 | Classification loss: 0.17132 | Regression loss: 0.37395 | Running loss: 0.77867 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4578 | Classification loss: 0.09064 | Regression loss: 0.08041 | Running loss: 0.77818 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4579 | Classification loss: 0.42060 | Regression loss: 0.34945 | Running loss: 0.77817 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4580 | Classification loss: 0.31314 | Regression loss: 0.42282 | Running loss: 0.77894 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4581 | Classification loss: 0.10029 | Regression loss: 0.11735 | Running loss: 0.77865 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4582 | Classification loss: 0.00700 | Regression loss: 0.20705 | Running loss: 0.77605 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4583 | Classification loss: 0.04761 | Regression loss: 0.20567 | Running loss: 0.77489 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4584 | Classification loss: 1.63854 | Regression loss: 0.63160 | Running loss: 0.77841 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4585 | Classification loss: 0.66060 | Regression loss: 0.37007 | Running loss: 0.77849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4586 | Classification loss: 0.04530 | Regression loss: 0.15424 | Running loss: 0.77546 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 4587 | Classification loss: 1.56952 | Regression loss: 0.60874 | Running loss: 0.77959 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4588 | Classification loss: 0.14758 | Regression loss: 0.25867 | Running loss: 0.77636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4589 | Classification loss: 0.00701 | Regression loss: 0.18351 | Running loss: 0.77577 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4590 | Classification loss: 0.12002 | Regression loss: 0.40340 | Running loss: 0.77547 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4591 | Classification loss: 0.00696 | Regression loss: 0.10494 | Running loss: 0.77355 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4592 | Classification loss: 0.50059 | Regression loss: 0.38845 | Running loss: 0.77468 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4593 | Classification loss: 1.53654 | Regression loss: 0.11497 | Running loss: 0.77694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4594 | Classification loss: 0.05907 | Regression loss: 0.14755 | Running loss: 0.77664 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4595 | Classification loss: 1.84103 | Regression loss: 0.82184 | Running loss: 0.78046 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4596 | Classification loss: 0.00100 | Regression loss: 0.06582 | Running loss: 0.78009 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4597 | Classification loss: 0.16979 | Regression loss: 0.31427 | Running loss: 0.78081 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4598 | Classification loss: 0.28486 | Regression loss: 0.28352 | Running loss: 0.77837 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4599 | Classification loss: 0.18584 | Regression loss: 0.32027 | Running loss: 0.77875 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4600 | Classification loss: 0.59449 | Regression loss: 0.47051 | Running loss: 0.78038 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4601 | Classification loss: 0.07618 | Regression loss: 0.12829 | Running loss: 0.77895 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4602 | Classification loss: 0.49800 | Regression loss: 0.39121 | Running loss: 0.77909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4603 | Classification loss: 1.55219 | Regression loss: 0.28403 | Running loss: 0.78215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4604 | Classification loss: 0.23065 | Regression loss: 0.37268 | Running loss: 0.78065 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4605 | Classification loss: 0.96474 | Regression loss: 0.68798 | Running loss: 0.78378 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4606 | Classification loss: 0.08841 | Regression loss: 0.15051 | Running loss: 0.78362 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4607 | Classification loss: 0.01497 | Regression loss: 0.25057 | Running loss: 0.78342 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4608 | Classification loss: 0.00056 | Regression loss: 0.07247 | Running loss: 0.78072 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4609 | Classification loss: 0.04611 | Regression loss: 0.17975 | Running loss: 0.78079 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4610 | Classification loss: 0.16611 | Regression loss: 0.25344 | Running loss: 0.78100 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4611 | Classification loss: 0.40738 | Regression loss: 0.42730 | Running loss: 0.78197 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4612 | Classification loss: 0.05431 | Regression loss: 0.17971 | Running loss: 0.77881 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4613 | Classification loss: 0.03014 | Regression loss: 0.12266 | Running loss: 0.77683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4614 | Classification loss: 0.11581 | Regression loss: 0.09451 | Running loss: 0.77681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4615 | Classification loss: 0.06279 | Regression loss: 0.16202 | Running loss: 0.77520 | Spend Time:0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4616 | Classification loss: 0.46630 | Regression loss: 0.52583 | Running loss: 0.77705 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4617 | Classification loss: 0.24033 | Regression loss: 0.35047 | Running loss: 0.77798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4618 | Classification loss: 1.96759 | Regression loss: 0.63541 | Running loss: 0.78288 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4619 | Classification loss: 0.34711 | Regression loss: 0.63044 | Running loss: 0.78326 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4620 | Classification loss: 0.76005 | Regression loss: 0.33183 | Running loss: 0.78196 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4621 | Classification loss: 0.25687 | Regression loss: 0.22246 | Running loss: 0.78167 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4622 | Classification loss: 0.03042 | Regression loss: 0.09060 | Running loss: 0.78145 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4623 | Classification loss: 0.89013 | Regression loss: 0.54412 | Running loss: 0.78279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4624 | Classification loss: 0.17713 | Regression loss: 0.13836 | Running loss: 0.78228 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4625 | Classification loss: 0.16423 | Regression loss: 0.24907 | Running loss: 0.78214 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4626 | Classification loss: 0.28889 | Regression loss: 0.44873 | Running loss: 0.78351 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4627 | Classification loss: 0.07557 | Regression loss: 0.28144 | Running loss: 0.78306 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4628 | Classification loss: 0.07518 | Regression loss: 0.24863 | Running loss: 0.78203 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4629 | Classification loss: 0.10877 | Regression loss: 0.28023 | Running loss: 0.78198 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4630 | Classification loss: 1.05929 | Regression loss: 0.62871 | Running loss: 0.78371 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4631 | Classification loss: 0.01441 | Regression loss: 0.11240 | Running loss: 0.78351 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4632 | Classification loss: 3.28575 | Regression loss: 0.78621 | Running loss: 0.79142 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4633 | Classification loss: 1.12788 | Regression loss: 0.39942 | Running loss: 0.79339 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4634 | Classification loss: 0.22749 | Regression loss: 0.24694 | Running loss: 0.79344 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4635 | Classification loss: 0.27263 | Regression loss: 0.41921 | Running loss: 0.78912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4636 | Classification loss: 1.16750 | Regression loss: 0.47435 | Running loss: 0.78580 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4637 | Classification loss: 2.41265 | Regression loss: 0.60304 | Running loss: 0.78884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4638 | Classification loss: 0.41790 | Regression loss: 0.45247 | Running loss: 0.78835 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4639 | Classification loss: 0.15533 | Regression loss: 0.17679 | Running loss: 0.78725 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4640 | Classification loss: 0.25485 | Regression loss: 0.49027 | Running loss: 0.78809 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4641 | Classification loss: 0.14545 | Regression loss: 0.24941 | Running loss: 0.78881 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4642 | Classification loss: 0.43412 | Regression loss: 0.31213 | Running loss: 0.78896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4643 | Classification loss: 0.03511 | Regression loss: 0.20398 | Running loss: 0.78802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4644 | Classification loss: 0.00629 | Regression loss: 0.10294 | Running loss: 0.78569 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4645 | Classification loss: 0.04842 | Regression loss: 0.19273 | Running loss: 0.78575 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4646 | Classification loss: 0.27612 | Regression loss: 0.34091 | Running loss: 0.78629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4647 | Classification loss: 2.91751 | Regression loss: 0.70676 | Running loss: 0.79252 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4648 | Classification loss: 0.11201 | Regression loss: 0.33755 | Running loss: 0.79303 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4649 | Classification loss: 0.34213 | Regression loss: 0.11629 | Running loss: 0.79341 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4650 | Classification loss: 0.09066 | Regression loss: 0.15573 | Running loss: 0.79217 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4651 | Classification loss: 1.01177 | Regression loss: 0.45560 | Running loss: 0.79484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4652 | Classification loss: 0.03866 | Regression loss: 0.32413 | Running loss: 0.79435 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4653 | Classification loss: 0.31102 | Regression loss: 0.58168 | Running loss: 0.79586 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4654 | Classification loss: 0.00383 | Regression loss: 0.08839 | Running loss: 0.79315 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4655 | Classification loss: 2.27464 | Regression loss: 0.72947 | Running loss: 0.79884 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4656 | Classification loss: 0.26834 | Regression loss: 0.31279 | Running loss: 0.79760 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4657 | Classification loss: 0.23849 | Regression loss: 0.46510 | Running loss: 0.79694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4658 | Classification loss: 0.50245 | Regression loss: 0.26099 | Running loss: 0.79696 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4659 | Classification loss: 0.06607 | Regression loss: 0.11206 | Running loss: 0.79687 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4660 | Classification loss: 0.57864 | Regression loss: 0.32113 | Running loss: 0.79799 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4661 | Classification loss: 0.06187 | Regression loss: 0.17622 | Running loss: 0.79776 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4662 | Classification loss: 0.00940 | Regression loss: 0.11706 | Running loss: 0.79780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4663 | Classification loss: 0.03662 | Regression loss: 0.04875 | Running loss: 0.79629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4664 | Classification loss: 0.21593 | Regression loss: 0.29122 | Running loss: 0.79020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4665 | Classification loss: 0.08583 | Regression loss: 0.21163 | Running loss: 0.79017 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4666 | Classification loss: 0.48820 | Regression loss: 0.25935 | Running loss: 0.79030 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4667 | Classification loss: 0.17637 | Regression loss: 0.21398 | Running loss: 0.79016 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4668 | Classification loss: 0.00474 | Regression loss: 0.12743 | Running loss: 0.78959 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4669 | Classification loss: 0.81644 | Regression loss: 0.68678 | Running loss: 0.79045 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4670 | Classification loss: 0.18610 | Regression loss: 0.23574 | Running loss: 0.79095 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4671 | Classification loss: 0.08712 | Regression loss: 0.26004 | Running loss: 0.78924 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4672 | Classification loss: 0.06673 | Regression loss: 0.32234 | Running loss: 0.78974 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4673 | Classification loss: 0.01017 | Regression loss: 0.13535 | Running loss: 0.78843 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4674 | Classification loss: 0.16838 | Regression loss: 0.28889 | Running loss: 0.78731 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4675 | Classification loss: 0.01257 | Regression loss: 0.14277 | Running loss: 0.78419 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4676 | Classification loss: 0.07420 | Regression loss: 0.22865 | Running loss: 0.77700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4677 | Classification loss: 0.00027 | Regression loss: 0.13287 | Running loss: 0.77190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4678 | Classification loss: 1.13398 | Regression loss: 0.39595 | Running loss: 0.77238 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4679 | Classification loss: 0.22884 | Regression loss: 0.39782 | Running loss: 0.77006 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4680 | Classification loss: 0.00408 | Regression loss: 0.17738 | Running loss: 0.76778 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4681 | Classification loss: 1.00272 | Regression loss: 0.42994 | Running loss: 0.76958 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4682 | Classification loss: 0.02372 | Regression loss: 0.29375 | Running loss: 0.76876 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4683 | Classification loss: 0.06962 | Regression loss: 0.15805 | Running loss: 0.76750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4684 | Classification loss: 0.20723 | Regression loss: 0.25408 | Running loss: 0.76490 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4685 | Classification loss: 0.00328 | Regression loss: 0.14103 | Running loss: 0.76327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4686 | Classification loss: 0.37002 | Regression loss: 0.30598 | Running loss: 0.76388 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4687 | Classification loss: 0.16470 | Regression loss: 0.16078 | Running loss: 0.76425 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4688 | Classification loss: 0.15150 | Regression loss: 0.14495 | Running loss: 0.76259 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4689 | Classification loss: 1.76087 | Regression loss: 0.68611 | Running loss: 0.76407 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4690 | Classification loss: 0.07077 | Regression loss: 0.27005 | Running loss: 0.76180 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4691 | Classification loss: 0.07640 | Regression loss: 0.26800 | Running loss: 0.76091 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4692 | Classification loss: 0.01604 | Regression loss: 0.13950 | Running loss: 0.76063 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4693 | Classification loss: 0.16142 | Regression loss: 0.19727 | Running loss: 0.76024 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4694 | Classification loss: 1.34179 | Regression loss: 0.41197 | Running loss: 0.76338 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4695 | Classification loss: 0.33965 | Regression loss: 0.36633 | Running loss: 0.76447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4696 | Classification loss: 0.12256 | Regression loss: 0.18328 | Running loss: 0.76442 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4697 | Classification loss: 0.20422 | Regression loss: 0.26192 | Running loss: 0.76452 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4698 | Classification loss: 0.13260 | Regression loss: 0.22280 | Running loss: 0.76413 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4699 | Classification loss: 0.24903 | Regression loss: 0.26877 | Running loss: 0.76263 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4700 | Classification loss: 0.15074 | Regression loss: 0.27812 | Running loss: 0.76335 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4701 | Classification loss: 0.99964 | Regression loss: 0.36364 | Running loss: 0.76336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4702 | Classification loss: 0.20850 | Regression loss: 0.19684 | Running loss: 0.76250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4703 | Classification loss: 0.00288 | Regression loss: 0.05149 | Running loss: 0.76166 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4704 | Classification loss: 0.25981 | Regression loss: 0.22847 | Running loss: 0.76004 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4705 | Classification loss: 0.78834 | Regression loss: 0.50853 | Running loss: 0.76152 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4706 | Classification loss: 0.40644 | Regression loss: 0.61966 | Running loss: 0.76300 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4707 | Classification loss: 1.47299 | Regression loss: 0.59837 | Running loss: 0.76694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4708 | Classification loss: 0.21944 | Regression loss: 0.20221 | Running loss: 0.76582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4709 | Classification loss: 0.12631 | Regression loss: 0.33225 | Running loss: 0.76630 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4710 | Classification loss: 1.13335 | Regression loss: 0.68205 | Running loss: 0.76912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4711 | Classification loss: 0.27173 | Regression loss: 0.45423 | Running loss: 0.76808 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4712 | Classification loss: 0.41633 | Regression loss: 0.48741 | Running loss: 0.76454 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4713 | Classification loss: 0.06253 | Regression loss: 0.16571 | Running loss: 0.76149 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4714 | Classification loss: 0.03443 | Regression loss: 0.14237 | Running loss: 0.76165 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4715 | Classification loss: 0.03457 | Regression loss: 0.18341 | Running loss: 0.76189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4716 | Classification loss: 0.83023 | Regression loss: 0.51912 | Running loss: 0.76088 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4717 | Classification loss: 0.00057 | Regression loss: 0.04732 | Running loss: 0.75750 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4718 | Classification loss: 0.68018 | Regression loss: 0.46696 | Running loss: 0.75845 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4719 | Classification loss: 0.57998 | Regression loss: 0.16548 | Running loss: 0.75974 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4720 | Classification loss: 0.33861 | Regression loss: 0.45256 | Running loss: 0.75819 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4721 | Classification loss: 0.68805 | Regression loss: 0.45435 | Running loss: 0.75660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4722 | Classification loss: 1.31004 | Regression loss: 0.49092 | Running loss: 0.75930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4723 | Classification loss: 0.55965 | Regression loss: 0.43891 | Running loss: 0.76104 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4724 | Classification loss: 0.00072 | Regression loss: 0.07572 | Running loss: 0.75931 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4725 | Classification loss: 0.14053 | Regression loss: 0.09640 | Running loss: 0.75700 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4726 | Classification loss: 0.24097 | Regression loss: 0.36452 | Running loss: 0.75745 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4727 | Classification loss: 0.17198 | Regression loss: 0.35541 | Running loss: 0.75739 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4728 | Classification loss: 0.12065 | Regression loss: 0.25294 | Running loss: 0.75636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4729 | Classification loss: 2.83733 | Regression loss: 1.33629 | Running loss: 0.76364 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4730 | Classification loss: 0.03306 | Regression loss: 0.13103 | Running loss: 0.76385 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4731 | Classification loss: 0.15478 | Regression loss: 0.25555 | Running loss: 0.76286 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4732 | Classification loss: 0.73824 | Regression loss: 0.45883 | Running loss: 0.76272 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4733 | Classification loss: 0.00188 | Regression loss: 0.17722 | Running loss: 0.76082 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4734 | Classification loss: 1.47059 | Regression loss: 0.72160 | Running loss: 0.76476 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4735 | Classification loss: 0.09863 | Regression loss: 0.22581 | Running loss: 0.76495 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4736 | Classification loss: 1.48484 | Regression loss: 0.53898 | Running loss: 0.76681 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4737 | Classification loss: 0.03403 | Regression loss: 0.05723 | Running loss: 0.76624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4738 | Classification loss: 0.30065 | Regression loss: 0.36164 | Running loss: 0.76559 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4739 | Classification loss: 2.02798 | Regression loss: 0.88745 | Running loss: 0.77054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4740 | Classification loss: 0.03866 | Regression loss: 0.12567 | Running loss: 0.76967 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4741 | Classification loss: 0.35478 | Regression loss: 0.25617 | Running loss: 0.77015 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4742 | Classification loss: 1.24063 | Regression loss: 0.46137 | Running loss: 0.77325 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4743 | Classification loss: 0.24901 | Regression loss: 0.23626 | Running loss: 0.77143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4744 | Classification loss: 1.71906 | Regression loss: 0.62238 | Running loss: 0.77540 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4745 | Classification loss: 0.01401 | Regression loss: 0.15977 | Running loss: 0.77453 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4746 | Classification loss: 0.43134 | Regression loss: 0.30863 | Running loss: 0.77555 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4747 | Classification loss: 0.08205 | Regression loss: 0.13817 | Running loss: 0.77586 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4748 | Classification loss: 0.00080 | Regression loss: 0.10000 | Running loss: 0.77362 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4749 | Classification loss: 0.15232 | Regression loss: 0.37736 | Running loss: 0.77211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4750 | Classification loss: 0.87889 | Regression loss: 0.64525 | Running loss: 0.76949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4751 | Classification loss: 0.16088 | Regression loss: 0.30473 | Running loss: 0.76563 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4752 | Classification loss: 0.24268 | Regression loss: 0.25695 | Running loss: 0.76098 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4753 | Classification loss: 0.00702 | Regression loss: 0.04649 | Running loss: 0.76075 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4754 | Classification loss: 0.21873 | Regression loss: 0.14761 | Running loss: 0.75671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4755 | Classification loss: 0.10137 | Regression loss: 0.18901 | Running loss: 0.75601 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4756 | Classification loss: 0.00015 | Regression loss: 0.02390 | Running loss: 0.75527 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4757 | Classification loss: 0.79852 | Regression loss: 0.35061 | Running loss: 0.75491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4758 | Classification loss: 0.60708 | Regression loss: 0.38702 | Running loss: 0.75509 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4759 | Classification loss: 0.88422 | Regression loss: 0.15709 | Running loss: 0.75641 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4760 | Classification loss: 2.56985 | Regression loss: 0.59780 | Running loss: 0.76011 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4761 | Classification loss: 0.28959 | Regression loss: 0.53983 | Running loss: 0.75189 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4762 | Classification loss: 0.44714 | Regression loss: 0.39157 | Running loss: 0.75213 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4763 | Classification loss: 0.18192 | Regression loss: 0.45787 | Running loss: 0.74828 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4764 | Classification loss: 0.39281 | Regression loss: 0.40443 | Running loss: 0.74763 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4765 | Classification loss: 0.00934 | Regression loss: 0.14844 | Running loss: 0.74640 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4766 | Classification loss: 0.56785 | Regression loss: 0.61625 | Running loss: 0.74677 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4767 | Classification loss: 0.00577 | Regression loss: 0.14854 | Running loss: 0.74422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4768 | Classification loss: 0.12937 | Regression loss: 0.42813 | Running loss: 0.74508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4769 | Classification loss: 0.06541 | Regression loss: 0.08239 | Running loss: 0.74470 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4770 | Classification loss: 0.03069 | Regression loss: 0.11850 | Running loss: 0.74418 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4771 | Classification loss: 1.27809 | Regression loss: 0.47624 | Running loss: 0.74701 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4772 | Classification loss: 0.04770 | Regression loss: 0.14602 | Running loss: 0.74683 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4773 | Classification loss: 0.01112 | Regression loss: 0.14153 | Running loss: 0.74279 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4774 | Classification loss: 0.00097 | Regression loss: 0.08741 | Running loss: 0.74203 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4775 | Classification loss: 1.07783 | Regression loss: 0.60398 | Running loss: 0.74525 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4776 | Classification loss: 0.48689 | Regression loss: 0.30299 | Running loss: 0.74680 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4777 | Classification loss: 0.04256 | Regression loss: 0.24852 | Running loss: 0.74545 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4778 | Classification loss: 0.00420 | Regression loss: 0.05670 | Running loss: 0.74497 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4779 | Classification loss: 0.03466 | Regression loss: 0.17330 | Running loss: 0.74437 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4780 | Classification loss: 0.44841 | Regression loss: 0.43701 | Running loss: 0.74554 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4781 | Classification loss: 0.80355 | Regression loss: 0.34465 | Running loss: 0.74743 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4782 | Classification loss: 1.17777 | Regression loss: 0.78703 | Running loss: 0.75111 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4783 | Classification loss: 0.03313 | Regression loss: 0.10204 | Running loss: 0.75060 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4784 | Classification loss: 0.35329 | Regression loss: 0.42743 | Running loss: 0.75029 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4785 | Classification loss: 0.32830 | Regression loss: 0.52632 | Running loss: 0.75175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4786 | Classification loss: 0.00517 | Regression loss: 0.14063 | Running loss: 0.75170 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4787 | Classification loss: 0.62058 | Regression loss: 0.48445 | Running loss: 0.75358 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4788 | Classification loss: 0.21778 | Regression loss: 0.32961 | Running loss: 0.75382 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4789 | Classification loss: 0.08000 | Regression loss: 0.29224 | Running loss: 0.75408 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4790 | Classification loss: 0.51008 | Regression loss: 0.20295 | Running loss: 0.75303 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4791 | Classification loss: 0.07155 | Regression loss: 0.21712 | Running loss: 0.75309 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4792 | Classification loss: 0.19105 | Regression loss: 0.24424 | Running loss: 0.75336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4793 | Classification loss: 0.06465 | Regression loss: 0.16803 | Running loss: 0.75277 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4794 | Classification loss: 0.05785 | Regression loss: 0.24722 | Running loss: 0.75299 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4795 | Classification loss: 0.86072 | Regression loss: 0.41213 | Running loss: 0.75484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4796 | Classification loss: 0.39856 | Regression loss: 0.52618 | Running loss: 0.75228 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4797 | Classification loss: 0.02118 | Regression loss: 0.17013 | Running loss: 0.75157 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4798 | Classification loss: 0.14360 | Regression loss: 0.19069 | Running loss: 0.75193 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4799 | Classification loss: 0.47983 | Regression loss: 0.26744 | Running loss: 0.75102 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4800 | Classification loss: 0.07804 | Regression loss: 0.36439 | Running loss: 0.74893 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4801 | Classification loss: 0.62827 | Regression loss: 0.46918 | Running loss: 0.74939 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4802 | Classification loss: 1.59258 | Regression loss: 0.61536 | Running loss: 0.75312 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4803 | Classification loss: 0.46876 | Regression loss: 0.44282 | Running loss: 0.75317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4804 | Classification loss: 0.08673 | Regression loss: 0.22708 | Running loss: 0.75110 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4805 | Classification loss: 0.01218 | Regression loss: 0.17292 | Running loss: 0.75059 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4806 | Classification loss: 0.08666 | Regression loss: 0.17690 | Running loss: 0.75027 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4807 | Classification loss: 0.12993 | Regression loss: 0.23652 | Running loss: 0.74909 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4808 | Classification loss: 0.08053 | Regression loss: 0.34070 | Running loss: 0.74797 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4809 | Classification loss: 0.06640 | Regression loss: 0.22620 | Running loss: 0.74672 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4810 | Classification loss: 1.44709 | Regression loss: 0.08757 | Running loss: 0.74970 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4811 | Classification loss: 0.64988 | Regression loss: 0.74491 | Running loss: 0.74823 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4812 | Classification loss: 0.77347 | Regression loss: 0.51886 | Running loss: 0.74881 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4813 | Classification loss: 0.45465 | Regression loss: 0.35588 | Running loss: 0.74961 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4814 | Classification loss: 0.10897 | Regression loss: 0.27949 | Running loss: 0.74941 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 4815 | Classification loss: 0.68317 | Regression loss: 0.44167 | Running loss: 0.74948 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4816 | Classification loss: 0.22185 | Regression loss: 0.36408 | Running loss: 0.74506 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4817 | Classification loss: 0.23526 | Regression loss: 0.19071 | Running loss: 0.74388 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4818 | Classification loss: 0.00087 | Regression loss: 0.01949 | Running loss: 0.74300 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4819 | Classification loss: 1.24249 | Regression loss: 0.59059 | Running loss: 0.74639 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4820 | Classification loss: 1.19927 | Regression loss: 0.67067 | Running loss: 0.74777 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4821 | Classification loss: 0.16943 | Regression loss: 0.17967 | Running loss: 0.74694 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4822 | Classification loss: 0.59905 | Regression loss: 0.35528 | Running loss: 0.74750 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4823 | Classification loss: 0.09949 | Regression loss: 0.12963 | Running loss: 0.74773 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4824 | Classification loss: 0.29695 | Regression loss: 0.36224 | Running loss: 0.74820 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4825 | Classification loss: 0.01616 | Regression loss: 0.14758 | Running loss: 0.74830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4826 | Classification loss: 0.01223 | Regression loss: 0.15133 | Running loss: 0.74742 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4827 | Classification loss: 0.01664 | Regression loss: 0.20627 | Running loss: 0.74766 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4828 | Classification loss: 0.23012 | Regression loss: 0.48944 | Running loss: 0.74891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4829 | Classification loss: 0.18146 | Regression loss: 0.06933 | Running loss: 0.74874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4830 | Classification loss: 0.89150 | Regression loss: 0.52335 | Running loss: 0.75045 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4831 | Classification loss: 0.63799 | Regression loss: 0.32505 | Running loss: 0.75181 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4832 | Classification loss: 1.33844 | Regression loss: 0.92630 | Running loss: 0.75614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4833 | Classification loss: 1.44619 | Regression loss: 0.76301 | Running loss: 0.75822 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4834 | Classification loss: 0.29235 | Regression loss: 0.17198 | Running loss: 0.75788 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4835 | Classification loss: 0.32732 | Regression loss: 0.49367 | Running loss: 0.75697 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4836 | Classification loss: 0.09312 | Regression loss: 0.22099 | Running loss: 0.75740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4837 | Classification loss: 1.75574 | Regression loss: 0.60183 | Running loss: 0.75743 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4838 | Classification loss: 0.16217 | Regression loss: 0.20430 | Running loss: 0.75738 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4839 | Classification loss: 1.92549 | Regression loss: 0.77828 | Running loss: 0.76184 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4840 | Classification loss: 0.65706 | Regression loss: 0.55791 | Running loss: 0.76249 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4841 | Classification loss: 1.28026 | Regression loss: 0.41035 | Running loss: 0.76552 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4842 | Classification loss: 0.24178 | Regression loss: 0.24706 | Running loss: 0.76521 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4843 | Classification loss: 1.09879 | Regression loss: 0.66790 | Running loss: 0.76849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4844 | Classification loss: 0.11430 | Regression loss: 0.29357 | Running loss: 0.76916 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4845 | Classification loss: 0.02155 | Regression loss: 0.08737 | Running loss: 0.76861 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4846 | Classification loss: 0.00317 | Regression loss: 0.08249 | Running loss: 0.76673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4847 | Classification loss: 0.49094 | Regression loss: 0.32551 | Running loss: 0.76667 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4848 | Classification loss: 4.77017 | Regression loss: 0.78449 | Running loss: 0.77752 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4849 | Classification loss: 0.14298 | Regression loss: 0.21382 | Running loss: 0.77806 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4850 | Classification loss: 0.49303 | Regression loss: 0.44977 | Running loss: 0.77777 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4851 | Classification loss: 0.04549 | Regression loss: 0.11539 | Running loss: 0.77558 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4852 | Classification loss: 1.37961 | Regression loss: 0.56848 | Running loss: 0.77644 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4853 | Classification loss: 0.00231 | Regression loss: 0.12198 | Running loss: 0.77610 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4854 | Classification loss: 0.45277 | Regression loss: 0.57603 | Running loss: 0.77675 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4855 | Classification loss: 0.54067 | Regression loss: 0.53926 | Running loss: 0.77830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4856 | Classification loss: 0.00099 | Regression loss: 0.08817 | Running loss: 0.77811 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4857 | Classification loss: 0.04491 | Regression loss: 0.10130 | Running loss: 0.77802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4858 | Classification loss: 0.02974 | Regression loss: 0.13128 | Running loss: 0.77642 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4859 | Classification loss: 1.11942 | Regression loss: 0.78027 | Running loss: 0.77904 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4860 | Classification loss: 1.40646 | Regression loss: 0.45234 | Running loss: 0.78225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4861 | Classification loss: 0.20334 | Regression loss: 0.30925 | Running loss: 0.78020 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4862 | Classification loss: 0.55371 | Regression loss: 0.66341 | Running loss: 0.78208 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4863 | Classification loss: 0.13696 | Regression loss: 0.30233 | Running loss: 0.78247 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4864 | Classification loss: 0.15348 | Regression loss: 0.16680 | Running loss: 0.78226 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4865 | Classification loss: 0.03804 | Regression loss: 0.31515 | Running loss: 0.78128 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4866 | Classification loss: 0.09409 | Regression loss: 0.22486 | Running loss: 0.78173 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4867 | Classification loss: 2.03214 | Regression loss: 0.72427 | Running loss: 0.78215 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4868 | Classification loss: 2.66677 | Regression loss: 0.79480 | Running loss: 0.78812 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4869 | Classification loss: 2.13812 | Regression loss: 0.57207 | Running loss: 0.79093 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4870 | Classification loss: 0.49507 | Regression loss: 0.71096 | Running loss: 0.79124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4871 | Classification loss: 0.87148 | Regression loss: 0.37118 | Running loss: 0.79097 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4872 | Classification loss: 0.07635 | Regression loss: 0.20807 | Running loss: 0.79143 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4873 | Classification loss: 0.01376 | Regression loss: 0.08117 | Running loss: 0.78940 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4874 | Classification loss: 0.23298 | Regression loss: 0.59364 | Running loss: 0.78932 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4875 | Classification loss: 0.01199 | Regression loss: 0.14429 | Running loss: 0.78710 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4876 | Classification loss: 1.19833 | Regression loss: 0.77690 | Running loss: 0.78542 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4877 | Classification loss: 0.33112 | Regression loss: 0.48586 | Running loss: 0.78520 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4878 | Classification loss: 0.10804 | Regression loss: 0.14748 | Running loss: 0.78382 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4879 | Classification loss: 0.04114 | Regression loss: 0.10781 | Running loss: 0.78217 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4880 | Classification loss: 0.10009 | Regression loss: 0.11244 | Running loss: 0.78204 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4881 | Classification loss: 0.58133 | Regression loss: 0.64087 | Running loss: 0.78211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4882 | Classification loss: 0.05418 | Regression loss: 0.08791 | Running loss: 0.77994 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4883 | Classification loss: 0.14355 | Regression loss: 0.27471 | Running loss: 0.77931 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4884 | Classification loss: 0.02060 | Regression loss: 0.21745 | Running loss: 0.77802 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4885 | Classification loss: 0.07517 | Regression loss: 0.35057 | Running loss: 0.77813 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4886 | Classification loss: 0.40785 | Regression loss: 0.48536 | Running loss: 0.77946 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4887 | Classification loss: 0.41101 | Regression loss: 0.40971 | Running loss: 0.78089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4888 | Classification loss: 0.02974 | Regression loss: 0.17290 | Running loss: 0.78130 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4889 | Classification loss: 0.01614 | Regression loss: 0.13610 | Running loss: 0.78062 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4890 | Classification loss: 0.02631 | Regression loss: 0.20150 | Running loss: 0.77716 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4891 | Classification loss: 0.00449 | Regression loss: 0.10596 | Running loss: 0.77498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4892 | Classification loss: 0.09979 | Regression loss: 0.18833 | Running loss: 0.77517 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4893 | Classification loss: 0.04105 | Regression loss: 0.26402 | Running loss: 0.77070 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4894 | Classification loss: 0.27513 | Regression loss: 0.38553 | Running loss: 0.77176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4895 | Classification loss: 0.01531 | Regression loss: 0.07422 | Running loss: 0.77162 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4896 | Classification loss: 0.42969 | Regression loss: 0.67184 | Running loss: 0.77036 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4897 | Classification loss: 0.15583 | Regression loss: 0.46670 | Running loss: 0.77051 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4898 | Classification loss: 0.30626 | Regression loss: 0.26724 | Running loss: 0.77054 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4899 | Classification loss: 0.44166 | Regression loss: 0.31633 | Running loss: 0.76803 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4900 | Classification loss: 0.00085 | Regression loss: 0.16004 | Running loss: 0.76805 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4901 | Classification loss: 0.32487 | Regression loss: 0.22605 | Running loss: 0.76854 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4902 | Classification loss: 1.62444 | Regression loss: 0.71099 | Running loss: 0.77269 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4903 | Classification loss: 0.67459 | Regression loss: 0.50564 | Running loss: 0.77383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4904 | Classification loss: 0.04419 | Regression loss: 0.21189 | Running loss: 0.77396 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4905 | Classification loss: 1.53897 | Regression loss: 0.72618 | Running loss: 0.77582 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4906 | Classification loss: 0.60909 | Regression loss: 0.40525 | Running loss: 0.77678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4907 | Classification loss: 0.06639 | Regression loss: 0.20236 | Running loss: 0.77289 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4908 | Classification loss: 0.57493 | Regression loss: 0.68201 | Running loss: 0.77339 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4909 | Classification loss: 0.00457 | Regression loss: 0.17988 | Running loss: 0.77181 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4910 | Classification loss: 0.00204 | Regression loss: 0.10074 | Running loss: 0.77132 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4911 | Classification loss: 0.07556 | Regression loss: 0.24264 | Running loss: 0.77176 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4912 | Classification loss: 1.85289 | Regression loss: 0.49782 | Running loss: 0.77604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4913 | Classification loss: 0.05306 | Regression loss: 0.29687 | Running loss: 0.77625 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4914 | Classification loss: 0.02813 | Regression loss: 0.09718 | Running loss: 0.77624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4915 | Classification loss: 1.04846 | Regression loss: 0.67214 | Running loss: 0.77807 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4916 | Classification loss: 0.27125 | Regression loss: 0.42259 | Running loss: 0.77848 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4917 | Classification loss: 0.00161 | Regression loss: 0.03382 | Running loss: 0.77607 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4918 | Classification loss: 1.15065 | Regression loss: 0.46976 | Running loss: 0.77675 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4919 | Classification loss: 0.08515 | Regression loss: 0.01943 | Running loss: 0.77491 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4920 | Classification loss: 0.34522 | Regression loss: 0.36514 | Running loss: 0.77606 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4921 | Classification loss: 0.00042 | Regression loss: 0.02493 | Running loss: 0.77470 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4922 | Classification loss: 0.05560 | Regression loss: 0.18292 | Running loss: 0.77403 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4923 | Classification loss: 0.28381 | Regression loss: 0.31390 | Running loss: 0.77431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4924 | Classification loss: 0.43918 | Regression loss: 0.59762 | Running loss: 0.77487 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4925 | Classification loss: 0.24110 | Regression loss: 0.29417 | Running loss: 0.77493 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4926 | Classification loss: 0.28901 | Regression loss: 0.30400 | Running loss: 0.77310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4927 | Classification loss: 0.13654 | Regression loss: 0.37693 | Running loss: 0.77358 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4928 | Classification loss: 0.27007 | Regression loss: 0.52165 | Running loss: 0.77346 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4929 | Classification loss: 0.20708 | Regression loss: 0.25776 | Running loss: 0.77429 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4930 | Classification loss: 0.76863 | Regression loss: 0.43100 | Running loss: 0.77611 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4931 | Classification loss: 0.49601 | Regression loss: 0.55665 | Running loss: 0.77760 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4932 | Classification loss: 0.83163 | Regression loss: 0.68472 | Running loss: 0.77740 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4933 | Classification loss: 0.51292 | Regression loss: 0.54860 | Running loss: 0.77942 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4934 | Classification loss: 0.71125 | Regression loss: 0.51470 | Running loss: 0.78058 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4935 | Classification loss: 0.49983 | Regression loss: 0.48094 | Running loss: 0.78194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4936 | Classification loss: 0.88016 | Regression loss: 0.40424 | Running loss: 0.78370 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4937 | Classification loss: 0.07186 | Regression loss: 0.18504 | Running loss: 0.78165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4938 | Classification loss: 0.67938 | Regression loss: 0.67733 | Running loss: 0.78074 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4939 | Classification loss: 0.84711 | Regression loss: 0.68294 | Running loss: 0.78320 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 4940 | Classification loss: 0.01485 | Regression loss: 0.15493 | Running loss: 0.78175 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4941 | Classification loss: 0.09792 | Regression loss: 0.43022 | Running loss: 0.78060 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4942 | Classification loss: 0.05989 | Regression loss: 0.26489 | Running loss: 0.78099 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4943 | Classification loss: 0.12192 | Regression loss: 0.26645 | Running loss: 0.77988 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4944 | Classification loss: 0.11959 | Regression loss: 0.13326 | Running loss: 0.77930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4945 | Classification loss: 0.00610 | Regression loss: 0.06633 | Running loss: 0.77844 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4946 | Classification loss: 0.11561 | Regression loss: 0.27646 | Running loss: 0.77613 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4947 | Classification loss: 0.09420 | Regression loss: 0.25330 | Running loss: 0.77660 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4948 | Classification loss: 0.00097 | Regression loss: 0.07322 | Running loss: 0.77604 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4949 | Classification loss: 0.00853 | Regression loss: 0.20870 | Running loss: 0.77190 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4950 | Classification loss: 1.00282 | Regression loss: 0.30929 | Running loss: 0.77265 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4951 | Classification loss: 0.05435 | Regression loss: 0.24119 | Running loss: 0.77225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4952 | Classification loss: 0.66963 | Regression loss: 0.17929 | Running loss: 0.77278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4953 | Classification loss: 0.16446 | Regression loss: 0.21324 | Running loss: 0.77293 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4954 | Classification loss: 0.72797 | Regression loss: 0.45810 | Running loss: 0.77484 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4955 | Classification loss: 0.00303 | Regression loss: 0.14569 | Running loss: 0.77125 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4956 | Classification loss: 1.78664 | Regression loss: 0.56879 | Running loss: 0.77299 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4957 | Classification loss: 0.88624 | Regression loss: 0.44302 | Running loss: 0.77528 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4958 | Classification loss: 0.00012 | Regression loss: 0.02174 | Running loss: 0.77153 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4959 | Classification loss: 0.19662 | Regression loss: 0.34172 | Running loss: 0.77182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4960 | Classification loss: 0.34884 | Regression loss: 0.44710 | Running loss: 0.77235 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4961 | Classification loss: 0.00057 | Regression loss: 0.18750 | Running loss: 0.76842 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4962 | Classification loss: 0.00103 | Regression loss: 0.01049 | Running loss: 0.76676 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4963 | Classification loss: 0.04681 | Regression loss: 0.18383 | Running loss: 0.76534 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4964 | Classification loss: 0.25674 | Regression loss: 0.18850 | Running loss: 0.76583 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4965 | Classification loss: 0.30262 | Regression loss: 0.59055 | Running loss: 0.76614 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4966 | Classification loss: 1.41196 | Regression loss: 0.26517 | Running loss: 0.76891 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4967 | Classification loss: 0.06060 | Regression loss: 0.25294 | Running loss: 0.76684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4968 | Classification loss: 0.13406 | Regression loss: 0.28936 | Running loss: 0.76200 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4969 | Classification loss: 0.36702 | Regression loss: 0.27646 | Running loss: 0.76301 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 4970 | Classification loss: 0.07027 | Regression loss: 0.16109 | Running loss: 0.76124 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4971 | Classification loss: 0.36011 | Regression loss: 0.28223 | Running loss: 0.76021 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4972 | Classification loss: 0.11684 | Regression loss: 0.24251 | Running loss: 0.75985 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4973 | Classification loss: 0.04401 | Regression loss: 0.15639 | Running loss: 0.75636 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4974 | Classification loss: 0.79098 | Regression loss: 0.43396 | Running loss: 0.75798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4975 | Classification loss: 0.08753 | Regression loss: 0.13250 | Running loss: 0.75806 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4976 | Classification loss: 0.03397 | Regression loss: 0.22651 | Running loss: 0.75104 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4977 | Classification loss: 0.03500 | Regression loss: 0.14445 | Running loss: 0.74896 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4978 | Classification loss: 1.86935 | Regression loss: 0.58390 | Running loss: 0.75233 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4979 | Classification loss: 0.10446 | Regression loss: 0.12824 | Running loss: 0.75224 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4980 | Classification loss: 0.02336 | Regression loss: 0.10237 | Running loss: 0.75246 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4981 | Classification loss: 2.16981 | Regression loss: 0.78910 | Running loss: 0.75794 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4982 | Classification loss: 0.01466 | Regression loss: 0.10023 | Running loss: 0.75482 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4983 | Classification loss: 0.39483 | Regression loss: 0.54731 | Running loss: 0.75633 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4984 | Classification loss: 0.15261 | Regression loss: 0.34983 | Running loss: 0.75716 | Spend Time:0.03s\n",
      "Epoch: 50 | Iteration: 4985 | Classification loss: 0.87056 | Regression loss: 0.55757 | Running loss: 0.75920 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4986 | Classification loss: 0.05999 | Regression loss: 0.21642 | Running loss: 0.75969 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4987 | Classification loss: 0.03556 | Regression loss: 0.15896 | Running loss: 0.75684 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4988 | Classification loss: 0.31009 | Regression loss: 0.39126 | Running loss: 0.75699 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4989 | Classification loss: 0.90535 | Regression loss: 0.23362 | Running loss: 0.75882 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4990 | Classification loss: 0.09365 | Regression loss: 0.55851 | Running loss: 0.75945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4991 | Classification loss: 0.43640 | Regression loss: 0.50201 | Running loss: 0.75988 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4992 | Classification loss: 0.40568 | Regression loss: 0.38873 | Running loss: 0.75878 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4993 | Classification loss: 0.06870 | Regression loss: 0.16968 | Running loss: 0.75722 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4994 | Classification loss: 0.79061 | Regression loss: 0.61589 | Running loss: 0.75922 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4995 | Classification loss: 0.23409 | Regression loss: 0.40185 | Running loss: 0.76014 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4996 | Classification loss: 0.31357 | Regression loss: 0.31734 | Running loss: 0.76129 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4997 | Classification loss: 0.07747 | Regression loss: 0.13058 | Running loss: 0.76029 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4998 | Classification loss: 0.23196 | Regression loss: 0.85143 | Running loss: 0.75931 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 4999 | Classification loss: 1.49582 | Regression loss: 0.45717 | Running loss: 0.75778 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5000 | Classification loss: 0.00851 | Regression loss: 0.09480 | Running loss: 0.75776 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5001 | Classification loss: 0.29141 | Regression loss: 0.38968 | Running loss: 0.75707 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5002 | Classification loss: 0.05806 | Regression loss: 0.14704 | Running loss: 0.75719 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5003 | Classification loss: 0.08203 | Regression loss: 0.21320 | Running loss: 0.75516 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5004 | Classification loss: 0.00635 | Regression loss: 0.10377 | Running loss: 0.75491 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 5005 | Classification loss: 0.04580 | Regression loss: 0.24079 | Running loss: 0.75339 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5006 | Classification loss: 1.97487 | Regression loss: 0.67617 | Running loss: 0.75671 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5007 | Classification loss: 0.13661 | Regression loss: 0.15107 | Running loss: 0.75708 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5008 | Classification loss: 0.51573 | Regression loss: 0.46371 | Running loss: 0.75830 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5009 | Classification loss: 0.31520 | Regression loss: 0.27692 | Running loss: 0.75810 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5010 | Classification loss: 0.52902 | Regression loss: 0.39795 | Running loss: 0.75953 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5011 | Classification loss: 0.01124 | Regression loss: 0.04723 | Running loss: 0.75923 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5012 | Classification loss: 0.09246 | Regression loss: 0.28292 | Running loss: 0.75855 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5013 | Classification loss: 0.02052 | Regression loss: 0.03329 | Running loss: 0.75809 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5014 | Classification loss: 0.06163 | Regression loss: 0.26285 | Running loss: 0.75855 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5015 | Classification loss: 2.87709 | Regression loss: 0.75448 | Running loss: 0.76225 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5016 | Classification loss: 0.17568 | Regression loss: 0.40963 | Running loss: 0.76327 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5017 | Classification loss: 0.14349 | Regression loss: 0.18371 | Running loss: 0.76334 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5018 | Classification loss: 0.75534 | Regression loss: 0.27658 | Running loss: 0.76393 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5019 | Classification loss: 0.10700 | Regression loss: 0.30451 | Running loss: 0.76437 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5020 | Classification loss: 0.42042 | Regression loss: 0.21542 | Running loss: 0.76531 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5021 | Classification loss: 0.62283 | Regression loss: 0.29605 | Running loss: 0.76624 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5022 | Classification loss: 0.00261 | Regression loss: 0.12709 | Running loss: 0.76399 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5023 | Classification loss: 0.40046 | Regression loss: 0.23812 | Running loss: 0.76478 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5024 | Classification loss: 0.02164 | Regression loss: 0.09740 | Running loss: 0.76171 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5025 | Classification loss: 0.42048 | Regression loss: 0.34884 | Running loss: 0.76016 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5026 | Classification loss: 0.56551 | Regression loss: 0.20743 | Running loss: 0.76081 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5027 | Classification loss: 0.04949 | Regression loss: 0.16326 | Running loss: 0.76037 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5028 | Classification loss: 0.01346 | Regression loss: 0.18183 | Running loss: 0.75917 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5029 | Classification loss: 0.00523 | Regression loss: 0.27350 | Running loss: 0.75901 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5030 | Classification loss: 0.57711 | Regression loss: 0.57748 | Running loss: 0.75983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5031 | Classification loss: 0.84688 | Regression loss: 0.38862 | Running loss: 0.76193 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5032 | Classification loss: 0.14818 | Regression loss: 0.28421 | Running loss: 0.76103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5033 | Classification loss: 0.02967 | Regression loss: 0.12033 | Running loss: 0.76022 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5034 | Classification loss: 0.02683 | Regression loss: 0.25143 | Running loss: 0.76053 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5035 | Classification loss: 0.14212 | Regression loss: 0.19217 | Running loss: 0.75943 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5036 | Classification loss: 0.01674 | Regression loss: 0.11519 | Running loss: 0.75597 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5037 | Classification loss: 0.02333 | Regression loss: 0.17551 | Running loss: 0.75447 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5038 | Classification loss: 0.00658 | Regression loss: 0.09870 | Running loss: 0.75382 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5039 | Classification loss: 0.48872 | Regression loss: 0.34844 | Running loss: 0.75336 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5040 | Classification loss: 0.04532 | Regression loss: 0.33581 | Running loss: 0.75374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5041 | Classification loss: 1.76361 | Regression loss: 0.99665 | Running loss: 0.75688 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5042 | Classification loss: 0.01982 | Regression loss: 0.16000 | Running loss: 0.75576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5043 | Classification loss: 0.13032 | Regression loss: 0.37158 | Running loss: 0.75576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5044 | Classification loss: 0.39164 | Regression loss: 0.24353 | Running loss: 0.75410 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5045 | Classification loss: 0.29927 | Regression loss: 0.24151 | Running loss: 0.75441 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5046 | Classification loss: 0.03896 | Regression loss: 0.18208 | Running loss: 0.75303 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5047 | Classification loss: 0.57208 | Regression loss: 0.58643 | Running loss: 0.75522 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5048 | Classification loss: 0.03245 | Regression loss: 0.13152 | Running loss: 0.75502 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5049 | Classification loss: 0.00623 | Regression loss: 0.13062 | Running loss: 0.75492 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5050 | Classification loss: 0.09934 | Regression loss: 0.35934 | Running loss: 0.75431 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5051 | Classification loss: 0.06018 | Regression loss: 0.16624 | Running loss: 0.75423 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5052 | Classification loss: 2.08267 | Regression loss: 0.50826 | Running loss: 0.75912 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5053 | Classification loss: 0.70786 | Regression loss: 0.34209 | Running loss: 0.75930 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5054 | Classification loss: 0.16415 | Regression loss: 0.21658 | Running loss: 0.75978 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5055 | Classification loss: 1.19042 | Regression loss: 0.64278 | Running loss: 0.76310 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5056 | Classification loss: 0.26077 | Regression loss: 0.24980 | Running loss: 0.76089 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5057 | Classification loss: 0.02199 | Regression loss: 0.20885 | Running loss: 0.75951 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5058 | Classification loss: 0.15805 | Regression loss: 0.03121 | Running loss: 0.75879 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5059 | Classification loss: 0.01289 | Regression loss: 0.13609 | Running loss: 0.75425 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5060 | Classification loss: 0.38694 | Regression loss: 0.23772 | Running loss: 0.75332 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5061 | Classification loss: 1.61830 | Regression loss: 0.70282 | Running loss: 0.75733 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5062 | Classification loss: 0.05694 | Regression loss: 0.10714 | Running loss: 0.75745 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5063 | Classification loss: 0.47769 | Regression loss: 0.36494 | Running loss: 0.75885 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5064 | Classification loss: 1.53642 | Regression loss: 0.72449 | Running loss: 0.76295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5065 | Classification loss: 0.46058 | Regression loss: 0.48486 | Running loss: 0.76395 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5066 | Classification loss: 0.22982 | Regression loss: 0.06210 | Running loss: 0.76278 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5067 | Classification loss: 0.65332 | Regression loss: 0.52878 | Running loss: 0.76211 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5068 | Classification loss: 0.30475 | Regression loss: 0.38967 | Running loss: 0.76045 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5069 | Classification loss: 0.07247 | Regression loss: 0.18755 | Running loss: 0.75856 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5070 | Classification loss: 0.97282 | Regression loss: 0.34135 | Running loss: 0.75814 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 5071 | Classification loss: 0.69723 | Regression loss: 0.37637 | Running loss: 0.75964 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5072 | Classification loss: 1.29324 | Regression loss: 0.45434 | Running loss: 0.75945 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5073 | Classification loss: 0.91727 | Regression loss: 0.67499 | Running loss: 0.76253 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5074 | Classification loss: 0.25237 | Regression loss: 0.34501 | Running loss: 0.76344 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5075 | Classification loss: 0.04351 | Regression loss: 0.12527 | Running loss: 0.76221 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5076 | Classification loss: 0.82200 | Regression loss: 0.25632 | Running loss: 0.75951 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5077 | Classification loss: 0.68412 | Regression loss: 0.51237 | Running loss: 0.76081 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5078 | Classification loss: 0.98377 | Regression loss: 0.43569 | Running loss: 0.76331 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5079 | Classification loss: 0.11760 | Regression loss: 0.11481 | Running loss: 0.76223 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5080 | Classification loss: 0.02410 | Regression loss: 0.13787 | Running loss: 0.76109 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5081 | Classification loss: 0.02933 | Regression loss: 0.20414 | Running loss: 0.76112 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5082 | Classification loss: 0.17640 | Regression loss: 0.23075 | Running loss: 0.76150 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5083 | Classification loss: 0.05228 | Regression loss: 0.22693 | Running loss: 0.76156 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5084 | Classification loss: 0.11814 | Regression loss: 0.28989 | Running loss: 0.75783 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5085 | Classification loss: 0.05817 | Regression loss: 0.29135 | Running loss: 0.75647 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5086 | Classification loss: 0.38609 | Regression loss: 0.42372 | Running loss: 0.75769 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5087 | Classification loss: 0.17364 | Regression loss: 0.26846 | Running loss: 0.75422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5088 | Classification loss: 1.09735 | Regression loss: 0.43729 | Running loss: 0.75648 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5089 | Classification loss: 0.54792 | Regression loss: 0.48563 | Running loss: 0.75816 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5090 | Classification loss: 0.16819 | Regression loss: 0.51973 | Running loss: 0.75849 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5091 | Classification loss: 0.30232 | Regression loss: 0.46528 | Running loss: 0.75980 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5092 | Classification loss: 0.34648 | Regression loss: 0.32377 | Running loss: 0.75936 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5093 | Classification loss: 0.75574 | Regression loss: 0.30142 | Running loss: 0.75818 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5094 | Classification loss: 2.51490 | Regression loss: 0.47430 | Running loss: 0.76374 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5095 | Classification loss: 0.00251 | Regression loss: 0.16020 | Running loss: 0.75874 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5096 | Classification loss: 0.27506 | Regression loss: 0.21689 | Running loss: 0.75959 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5097 | Classification loss: 0.00056 | Regression loss: 0.13549 | Running loss: 0.75889 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5098 | Classification loss: 1.04072 | Regression loss: 0.62671 | Running loss: 0.76109 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5099 | Classification loss: 0.15719 | Regression loss: 0.31700 | Running loss: 0.76103 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5100 | Classification loss: 0.40236 | Regression loss: 0.34261 | Running loss: 0.76039 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5101 | Classification loss: 0.10419 | Regression loss: 0.18072 | Running loss: 0.76055 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5102 | Classification loss: 0.16610 | Regression loss: 0.30467 | Running loss: 0.75971 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5103 | Classification loss: 1.15192 | Regression loss: 0.32371 | Running loss: 0.75899 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5104 | Classification loss: 0.50286 | Regression loss: 0.35180 | Running loss: 0.75949 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5105 | Classification loss: 0.08646 | Regression loss: 0.21083 | Running loss: 0.75678 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5106 | Classification loss: 0.55835 | Regression loss: 0.51910 | Running loss: 0.75846 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5107 | Classification loss: 0.10961 | Regression loss: 0.35766 | Running loss: 0.75886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5108 | Classification loss: 0.22322 | Regression loss: 0.33079 | Running loss: 0.75983 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5109 | Classification loss: 0.82813 | Regression loss: 0.39399 | Running loss: 0.76182 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5110 | Classification loss: 1.29889 | Regression loss: 0.58540 | Running loss: 0.76475 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5111 | Classification loss: 0.01880 | Regression loss: 0.18103 | Running loss: 0.76348 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5112 | Classification loss: 0.92081 | Regression loss: 0.51384 | Running loss: 0.76588 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5113 | Classification loss: 0.59384 | Regression loss: 0.47030 | Running loss: 0.76770 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5114 | Classification loss: 0.32923 | Regression loss: 0.44891 | Running loss: 0.76884 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5115 | Classification loss: 1.15590 | Regression loss: 0.84962 | Running loss: 0.77240 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5116 | Classification loss: 0.04145 | Regression loss: 0.23558 | Running loss: 0.77097 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5117 | Classification loss: 0.00021 | Regression loss: 0.01801 | Running loss: 0.76982 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5118 | Classification loss: 0.06780 | Regression loss: 0.21643 | Running loss: 0.76519 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5119 | Classification loss: 0.01967 | Regression loss: 0.21654 | Running loss: 0.76370 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5120 | Classification loss: 0.90810 | Regression loss: 0.48487 | Running loss: 0.76431 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5121 | Classification loss: 0.34090 | Regression loss: 0.11505 | Running loss: 0.76426 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5122 | Classification loss: 0.43359 | Regression loss: 0.27499 | Running loss: 0.76543 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5123 | Classification loss: 2.27146 | Regression loss: 0.43710 | Running loss: 0.76798 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5124 | Classification loss: 0.31934 | Regression loss: 0.43664 | Running loss: 0.76886 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5125 | Classification loss: 0.03067 | Regression loss: 0.14007 | Running loss: 0.76838 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5126 | Classification loss: 0.44755 | Regression loss: 0.28758 | Running loss: 0.76837 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5127 | Classification loss: 0.11670 | Regression loss: 0.17265 | Running loss: 0.76824 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5128 | Classification loss: 0.01503 | Regression loss: 0.16454 | Running loss: 0.76795 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5129 | Classification loss: 0.24683 | Regression loss: 0.31473 | Running loss: 0.76829 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5130 | Classification loss: 0.29033 | Regression loss: 0.39638 | Running loss: 0.76629 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5131 | Classification loss: 0.11031 | Regression loss: 0.38264 | Running loss: 0.76702 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5132 | Classification loss: 2.48527 | Regression loss: 0.61498 | Running loss: 0.76508 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5133 | Classification loss: 0.00505 | Regression loss: 0.13902 | Running loss: 0.76231 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5134 | Classification loss: 0.35145 | Regression loss: 0.40937 | Running loss: 0.76289 | Spend Time:0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Iteration: 5135 | Classification loss: 0.00575 | Regression loss: 0.06539 | Running loss: 0.76165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5136 | Classification loss: 0.00184 | Regression loss: 0.17320 | Running loss: 0.75871 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5137 | Classification loss: 0.29946 | Regression loss: 0.47055 | Running loss: 0.75422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5138 | Classification loss: 0.00335 | Regression loss: 0.02347 | Running loss: 0.75253 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5139 | Classification loss: 0.38171 | Regression loss: 0.16017 | Running loss: 0.75295 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5140 | Classification loss: 0.33297 | Regression loss: 0.42688 | Running loss: 0.75298 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5141 | Classification loss: 0.07273 | Regression loss: 0.27843 | Running loss: 0.75290 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5142 | Classification loss: 0.04440 | Regression loss: 0.22215 | Running loss: 0.75194 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5143 | Classification loss: 0.70428 | Regression loss: 0.41579 | Running loss: 0.75370 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5144 | Classification loss: 3.03574 | Regression loss: 0.65970 | Running loss: 0.76087 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5145 | Classification loss: 0.17426 | Regression loss: 0.45735 | Running loss: 0.76165 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5146 | Classification loss: 0.60702 | Regression loss: 0.33002 | Running loss: 0.76229 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5147 | Classification loss: 0.00835 | Regression loss: 0.07637 | Running loss: 0.75521 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5148 | Classification loss: 0.09041 | Regression loss: 0.42026 | Running loss: 0.75533 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5149 | Classification loss: 0.00831 | Regression loss: 0.00000 | Running loss: 0.75443 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5150 | Classification loss: 1.52397 | Regression loss: 0.27090 | Running loss: 0.75753 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5151 | Classification loss: 0.37089 | Regression loss: 0.21007 | Running loss: 0.75576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5152 | Classification loss: 0.43039 | Regression loss: 0.41976 | Running loss: 0.75673 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5153 | Classification loss: 0.68948 | Regression loss: 0.64283 | Running loss: 0.75761 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5154 | Classification loss: 0.00838 | Regression loss: 0.08594 | Running loss: 0.75762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5155 | Classification loss: 0.13208 | Regression loss: 0.11584 | Running loss: 0.75210 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5156 | Classification loss: 0.34821 | Regression loss: 0.33721 | Running loss: 0.75231 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5157 | Classification loss: 1.35792 | Regression loss: 0.61415 | Running loss: 0.75485 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5158 | Classification loss: 0.51092 | Regression loss: 0.41950 | Running loss: 0.75518 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5159 | Classification loss: 0.31889 | Regression loss: 0.14780 | Running loss: 0.75576 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5160 | Classification loss: 0.02501 | Regression loss: 0.17724 | Running loss: 0.75437 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5161 | Classification loss: 0.00014 | Regression loss: 0.02855 | Running loss: 0.75395 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5162 | Classification loss: 0.12423 | Regression loss: 0.14023 | Running loss: 0.75422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5163 | Classification loss: 0.00063 | Regression loss: 0.08344 | Running loss: 0.75422 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5164 | Classification loss: 0.17496 | Regression loss: 0.20637 | Running loss: 0.75397 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5165 | Classification loss: 0.07676 | Regression loss: 0.17191 | Running loss: 0.75387 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5166 | Classification loss: 0.25841 | Regression loss: 0.14048 | Running loss: 0.75317 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5167 | Classification loss: 0.20191 | Regression loss: 0.33741 | Running loss: 0.75347 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5168 | Classification loss: 0.18311 | Regression loss: 0.12884 | Running loss: 0.75383 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5169 | Classification loss: 0.37859 | Regression loss: 0.51352 | Running loss: 0.75261 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5170 | Classification loss: 0.09970 | Regression loss: 0.26506 | Running loss: 0.75250 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5171 | Classification loss: 0.37235 | Regression loss: 0.44106 | Running loss: 0.75343 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5172 | Classification loss: 0.29383 | Regression loss: 0.18610 | Running loss: 0.75361 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5173 | Classification loss: 0.20803 | Regression loss: 0.21432 | Running loss: 0.75416 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5174 | Classification loss: 0.66185 | Regression loss: 0.46757 | Running loss: 0.75551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5175 | Classification loss: 0.00133 | Regression loss: 0.15453 | Running loss: 0.75551 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5176 | Classification loss: 0.15613 | Regression loss: 0.40781 | Running loss: 0.75603 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5177 | Classification loss: 0.58031 | Regression loss: 0.41722 | Running loss: 0.75776 | Spend Time:0.05s\n",
      "Epoch: 50 | Iteration: 5178 | Classification loss: 0.03032 | Regression loss: 0.16990 | Running loss: 0.75510 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5179 | Classification loss: 0.00844 | Regression loss: 0.10654 | Running loss: 0.75408 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5180 | Classification loss: 0.00184 | Regression loss: 0.08037 | Running loss: 0.75388 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5181 | Classification loss: 0.08245 | Regression loss: 0.16481 | Running loss: 0.75151 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5182 | Classification loss: 0.08820 | Regression loss: 0.22513 | Running loss: 0.75150 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5183 | Classification loss: 1.46551 | Regression loss: 0.60111 | Running loss: 0.75518 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5184 | Classification loss: 0.57664 | Regression loss: 0.28936 | Running loss: 0.75599 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5185 | Classification loss: 0.88276 | Regression loss: 0.16752 | Running loss: 0.75780 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5186 | Classification loss: 0.07115 | Regression loss: 0.10268 | Running loss: 0.75679 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5187 | Classification loss: 0.56526 | Regression loss: 0.17145 | Running loss: 0.75762 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5188 | Classification loss: 0.81983 | Regression loss: 0.31488 | Running loss: 0.75929 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5189 | Classification loss: 0.01907 | Regression loss: 0.09083 | Running loss: 0.75462 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5190 | Classification loss: 0.25840 | Regression loss: 0.26456 | Running loss: 0.75498 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5191 | Classification loss: 0.13339 | Regression loss: 0.19908 | Running loss: 0.75496 | Spend Time:0.04s\n",
      "Epoch: 50 | Iteration: 5192 | Classification loss: 0.03474 | Regression loss: 0.23604 | Running loss: 0.75519 | Spend Time:0.04s\n",
      "fail_id: []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "retinanet.train()\n",
    "retinanet.freeze_bn()\n",
    "\n",
    "dataset = dataset_train\n",
    "\n",
    "fail_id = []\n",
    "losses = collections.defaultdict(list)\n",
    "for idx, data in enumerate(dataset):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.device(0):\n",
    "            if torch.cuda.is_available():\n",
    "                classification_loss, regression_loss = retinanet([data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0), data['annot'].cuda().unsqueeze(dim=0)])\n",
    "            else:\n",
    "                print('not have gpu')\n",
    "                break\n",
    "\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            img_id = dataset.image_ids[idx]\n",
    "\n",
    "\n",
    "            classification_loss = float(classification_loss)\n",
    "            regression_loss = float(regression_loss)\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            losses[img_id] = [classification_loss, regression_loss, loss]\n",
    "\n",
    "            optimizer.step()\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            #epoch_loss.append(float(loss))\n",
    "            end = time.time()\n",
    "\n",
    "            print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} | Spend Time:{:1.2f}s'.format(\n",
    "                                              '50', \n",
    "                                              idx, \n",
    "                                              float(classification_loss), \n",
    "                                              float(regression_loss), \n",
    "                                              np.mean(loss_hist),\n",
    "                                              end - start))\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        fail_id.append(idx)\n",
    "        continue\n",
    "\n",
    "print('fail_id:',fail_id)\n",
    "\n",
    "\n",
    "with open(os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50).split('/')[:-1]), 'losses.pickle'), 'wb') as f:\n",
    "    pickle.dump(losses, f)\n",
    "#print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:47:23.719916Z",
     "start_time": "2021-05-27T13:47:23.706442Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50).split('/')[:-1]), 'losses.pickle'), 'rb') as f:\n",
    "    losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:55:41.434110Z",
     "start_time": "2021-05-27T13:55:41.348495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {2008006657: [0.716161847114563,\n",
       "              0.4635643910121687,\n",
       "              1.1797262381267317],\n",
       "             2010005505: [0.45598888397216797,\n",
       "              0.27079466057667584,\n",
       "              0.7267835445488438],\n",
       "             2008006663: [0.6163512468338013,\n",
       "              0.3891051729317211,\n",
       "              1.0054564197655225],\n",
       "             2010005512: [0.3910413384437561,\n",
       "              0.33380469482784547,\n",
       "              0.7248460332716016],\n",
       "             2010005511: [0.7221482992172241,\n",
       "              0.38564046976438165,\n",
       "              1.1077887689816057],\n",
       "             2010005513: [0.13401061296463013,\n",
       "              0.3216137092220177,\n",
       "              0.45562432218664783],\n",
       "             2008006667: [0.23653225600719452,\n",
       "              0.23851639101961417,\n",
       "              0.4750486470268087],\n",
       "             2010005518: [0.2557826340198517,\n",
       "              0.39032391157939467,\n",
       "              0.6461065455992463],\n",
       "             2010005522: [0.11753395944833755,\n",
       "              0.1693105103940093,\n",
       "              0.28684446984234685],\n",
       "             2008006677: [0.032838478684425354,\n",
       "              0.21300701815901021,\n",
       "              0.24584549684343557],\n",
       "             2008006682: [0.9864447116851807,\n",
       "              0.21448181462268573,\n",
       "              1.2009265263078663],\n",
       "             2010005535: [0.01766284927725792,\n",
       "              0.12359087749614192,\n",
       "              0.14125372677339984],\n",
       "             2008006691: [0.060007717460393906,\n",
       "              0.15424650047577498,\n",
       "              0.2142542179361689],\n",
       "             2008006692: [0.2606896758079529,\n",
       "              0.38768821906503204,\n",
       "              0.6483778948729849],\n",
       "             2010005540: [0.49374252557754517,\n",
       "              0.5210996027759637,\n",
       "              1.014842128353509],\n",
       "             2010005546: [0.003197341226041317,\n",
       "              0.19134407250108948,\n",
       "              0.1945414137271308],\n",
       "             2008006700: [0.21032162010669708,\n",
       "              0.3476291371550726,\n",
       "              0.5579507572617697],\n",
       "             2008006705: [0.5677039623260498,\n",
       "              0.5586995110677172,\n",
       "              1.1264034733937671],\n",
       "             2010005557: [0.018851131200790405,\n",
       "              0.158465600473366,\n",
       "              0.1773167316741564],\n",
       "             2008006712: [0.30631208419799805,\n",
       "              0.28777811366205347,\n",
       "              0.5940901978600515],\n",
       "             2010005561: [0.013111150823533535,\n",
       "              0.15607436110484263,\n",
       "              0.16918551192837616],\n",
       "             2008006715: [0.035229120403528214,\n",
       "              0.12664134634960653,\n",
       "              0.16187046675313474],\n",
       "             2008006717: [0.000773547391872853,\n",
       "              0.09164375918625674,\n",
       "              0.09241730657812959],\n",
       "             2008006718: [0.4274715781211853,\n",
       "              0.27218074895511224,\n",
       "              0.6996523270762975],\n",
       "             2008006719: [1.539063811302185,\n",
       "              0.4965544343486452,\n",
       "              2.0356182456508303],\n",
       "             2008006720: [0.08607104420661926,\n",
       "              0.24239673751909072,\n",
       "              0.32846778172571],\n",
       "             2010005570: [0.28797340393066406,\n",
       "              0.24278500376058243,\n",
       "              0.5307584076912465],\n",
       "             2010005571: [1.379388689994812,\n",
       "              0.4603108035422384,\n",
       "              1.8396994935370503],\n",
       "             2008006724: [0.431774377822876,\n",
       "              0.5507787732091579,\n",
       "              0.9825531510320339],\n",
       "             2010005573: [0.012358341366052628,\n",
       "              0.06743736635449504,\n",
       "              0.07979570772054767],\n",
       "             2008006728: [0.10388291627168655,\n",
       "              0.1237912386256578,\n",
       "              0.22767415489734436],\n",
       "             2008006730: [1.0043349266052246,\n",
       "              0.6456022675822913,\n",
       "              1.6499371941875158],\n",
       "             2010005578: [0.2723778486251831,\n",
       "              0.42075217779995144,\n",
       "              0.6931300264251345],\n",
       "             2008006733: [0.30167636275291443,\n",
       "              0.6027322834176946,\n",
       "              0.904408646170609],\n",
       "             2010005584: [0.35987383127212524,\n",
       "              0.25637087099098893,\n",
       "              0.6162447022631141],\n",
       "             2010005585: [0.1747615933418274,\n",
       "              0.36355684735027033,\n",
       "              0.5383184406920978],\n",
       "             2010005588: [0.07462923228740692,\n",
       "              0.31410936329574074,\n",
       "              0.38873859558314766],\n",
       "             2010005595: [0.4721772372722626,\n",
       "              0.26247820221277673,\n",
       "              0.7346554394850393],\n",
       "             2010005596: [0.6349719166755676,\n",
       "              0.40704616343348177,\n",
       "              1.0420180801090493],\n",
       "             2008006748: [0.0001772694813553244,\n",
       "              0.03219357348826917,\n",
       "              0.032370842969624496],\n",
       "             2008006750: [0.5137237906455994,\n",
       "              0.3903334466206861,\n",
       "              0.9040572372662854],\n",
       "             2010005597: [0.2816597819328308,\n",
       "              0.2038455124420772,\n",
       "              0.48550529437490797],\n",
       "             2008003520: [0.0033571526873856783,\n",
       "              0.17465448869768996,\n",
       "              0.17801164138507564],\n",
       "             2008006753: [0.19052451848983765,\n",
       "              0.2071636549947115,\n",
       "              0.39768817348454916],\n",
       "             2010005603: [0.02930809184908867,\n",
       "              0.18770840908431363,\n",
       "              0.2170165009334023],\n",
       "             2010005604: [0.08969787508249283,\n",
       "              0.05780608102160433,\n",
       "              0.14750395610409717],\n",
       "             2010005608: [1.5070998668670654,\n",
       "              0.5994431714439681,\n",
       "              2.1065430383110337],\n",
       "             2008006761: [1.926074743270874,\n",
       "              0.7221553805427692,\n",
       "              2.648230123813643],\n",
       "             2008006762: [0.22925551235675812,\n",
       "              0.2059198456633132,\n",
       "              0.43517535802007135],\n",
       "             2008006764: [0.1918904334306717,\n",
       "              0.24178932340890133,\n",
       "              0.43367975683957305],\n",
       "             2010005614: [0.21088354289531708,\n",
       "              0.24385084595352363,\n",
       "              0.4547343888488407],\n",
       "             2008006767: [0.915273904800415,\n",
       "              0.3908927851922631,\n",
       "              1.3061666899926783],\n",
       "             2010005616: [0.27152884006500244,\n",
       "              0.28384156023930074,\n",
       "              0.5553704003043032],\n",
       "             2010005615: [0.15779271721839905,\n",
       "              0.11728418417322699,\n",
       "              0.27507690139162605],\n",
       "             2010005619: [0.3262202739715576,\n",
       "              0.46521782168886566,\n",
       "              0.7914380956604232],\n",
       "             2008006778: [0.3079052269458771,\n",
       "              0.19156614201021563,\n",
       "              0.4994713689560927],\n",
       "             2010005627: [0.042558301240205765,\n",
       "              0.22929827372238645,\n",
       "              0.2718565749625922],\n",
       "             2010005628: [0.37000420689582825,\n",
       "              0.27679791974039714,\n",
       "              0.6468021266362254],\n",
       "             2010005629: [0.12573343515396118,\n",
       "              0.1986263457274648,\n",
       "              0.324359780881426],\n",
       "             2008006785: [0.0022961751092225313,\n",
       "              0.023794431776641003,\n",
       "              0.026090606885863534],\n",
       "             2010005640: [0.20905177295207977,\n",
       "              0.20554612796893515,\n",
       "              0.4145979009210149],\n",
       "             2011002717: [0.34608152508735657,\n",
       "              0.3189777589997842,\n",
       "              0.6650592840871408],\n",
       "             2010005646: [0.37436774373054504,\n",
       "              0.5858395849701385,\n",
       "              0.9602073287006836],\n",
       "             2008006802: [0.10665962845087051,\n",
       "              0.2530235863667456,\n",
       "              0.3596832148176161],\n",
       "             2010005652: [0.0032516582868993282,\n",
       "              0.04593136946962806,\n",
       "              0.04918302775652739],\n",
       "             2008006807: [1.2098429203033447,\n",
       "              0.4458858675363845,\n",
       "              1.6557287878397293],\n",
       "             2008006808: [0.528831958770752,\n",
       "              0.56520109343951,\n",
       "              1.094033052210262],\n",
       "             2008006810: [0.10867507755756378,\n",
       "              0.07885134436537775,\n",
       "              0.18752642192294153],\n",
       "             2010005663: [0.5899741649627686,\n",
       "              0.6698480117615466,\n",
       "              1.2598221767243152],\n",
       "             2010005665: [0.005204993765801191,\n",
       "              0.047516083962543705,\n",
       "              0.052721077728344896],\n",
       "             2008006818: [0.015278637409210205,\n",
       "              0.14547835024100136,\n",
       "              0.16075698765021157],\n",
       "             2008006819: [1.0078431367874146,\n",
       "              0.4733503016698339,\n",
       "              1.4811934384572485],\n",
       "             2008006820: [0.14759314060211182,\n",
       "              0.3003089025964356,\n",
       "              0.44790204319854743],\n",
       "             2010005668: [0.10782859474420547,\n",
       "              0.2400677189340291,\n",
       "              0.3478963136782346],\n",
       "             2010005670: [0.7748475074768066,\n",
       "              0.7390619744568544,\n",
       "              1.513909481933661],\n",
       "             2010005669: [0.18365803360939026,\n",
       "              0.2783299341720641,\n",
       "              0.46198796778145435],\n",
       "             2010005672: [0.002817066852003336,\n",
       "              0.08392999324419657,\n",
       "              0.08674706009619991],\n",
       "             2008006827: [0.5555817484855652,\n",
       "              0.42717477346521204,\n",
       "              0.9827565219507772],\n",
       "             2008006832: [0.19247089326381683,\n",
       "              0.3030127345954781,\n",
       "              0.49548362785929495],\n",
       "             2008006834: [0.5743950009346008,\n",
       "              0.41896941434482915,\n",
       "              0.99336441527943],\n",
       "             2010005683: [0.20892834663391113,\n",
       "              0.3604598397534958,\n",
       "              0.569388186387407],\n",
       "             2010005684: [0.04466107860207558,\n",
       "              0.3064095286018198,\n",
       "              0.3510706072038954],\n",
       "             2008006843: [0.29510289430618286,\n",
       "              0.2956109883440468,\n",
       "              0.5907138826502296],\n",
       "             2008006847: [0.2920758128166199,\n",
       "              0.5263050457224416,\n",
       "              0.8183808585390615],\n",
       "             2010005696: [0.028735293075442314,\n",
       "              0.13224956058893764,\n",
       "              0.16098485366437995],\n",
       "             2010005700: [0.04936045780777931,\n",
       "              0.3496540950820116,\n",
       "              0.39901455288979093],\n",
       "             2008006857: [0.5478242635726929,\n",
       "              0.6392278081915439,\n",
       "              1.1870520717642368],\n",
       "             2008006864: [0.6035844087600708,\n",
       "              0.5384240093392575,\n",
       "              1.1420084180993282],\n",
       "             2010005715: [0.010273284278810024,\n",
       "              0.1555853257126048,\n",
       "              0.16585860999141483],\n",
       "             2008006868: [0.12274239957332611,\n",
       "              0.21722369560972235,\n",
       "              0.33996609518304843],\n",
       "             2010005716: [0.07394015043973923,\n",
       "              0.18806156512797706,\n",
       "              0.2620017155677163],\n",
       "             2008006872: [1.009694218635559,\n",
       "              0.6373285992324311,\n",
       "              1.6470228178679902],\n",
       "             2008006873: [0.905897319316864,\n",
       "              0.5202880538256569,\n",
       "              1.426185373142521],\n",
       "             2010005721: [0.23884855210781097,\n",
       "              0.22137563091045454,\n",
       "              0.4602241830182655],\n",
       "             2010005723: [0.053379498422145844,\n",
       "              0.12771992448259495,\n",
       "              0.1810994229047408],\n",
       "             2008006877: [0.07837541401386261,\n",
       "              0.19230373553710084,\n",
       "              0.27067914955096345],\n",
       "             2010005725: [0.2426581084728241,\n",
       "              0.2522863365593795,\n",
       "              0.4949444450322036],\n",
       "             2008006879: [0.5909222960472107,\n",
       "              0.7232299853356822,\n",
       "              1.314152281382893],\n",
       "             2008006881: [0.5935459136962891,\n",
       "              0.3671398261393528,\n",
       "              0.9606857398356419],\n",
       "             2008006882: [0.13917873799800873,\n",
       "              0.31909968439236946,\n",
       "              0.4582784223903782],\n",
       "             2010005732: [0.07137953490018845,\n",
       "              0.2731793078952483,\n",
       "              0.34455884279543675],\n",
       "             2010005735: [0.2552441954612732,\n",
       "              0.2635168268584642,\n",
       "              0.5187610223197374],\n",
       "             2008006889: [0.016504012048244476,\n",
       "              0.0972177437169948,\n",
       "              0.11372175576523928],\n",
       "             2008006898: [0.4438299536705017,\n",
       "              0.2988038003542019,\n",
       "              0.7426337540247037],\n",
       "             2010005746: [0.09230978041887283,\n",
       "              0.221879014064532,\n",
       "              0.31418879448340487],\n",
       "             2010005748: [0.13381275534629822,\n",
       "              0.42078811035030106,\n",
       "              0.5546008656965993],\n",
       "             2010005750: [2.070643901824951,\n",
       "              0.5648527354091081,\n",
       "              2.635496637234059],\n",
       "             2008006903: [0.22976289689540863,\n",
       "              0.20485717143918258,\n",
       "              0.4346200683345912],\n",
       "             2010005753: [0.006782231852412224,\n",
       "              0.13160840003998228,\n",
       "              0.1383906318923945],\n",
       "             2010005755: [0.7419393062591553,\n",
       "              0.5217946970921449,\n",
       "              1.2637340033513],\n",
       "             2008006908: [0.012049103155732155,\n",
       "              0.1789251009799648,\n",
       "              0.19097420413569696],\n",
       "             2008006909: [0.04525424912571907,\n",
       "              0.20871776557940938,\n",
       "              0.2539720147051284],\n",
       "             2010005758: [0.7706083059310913,\n",
       "              0.6237922331814684,\n",
       "              1.3944005391125596],\n",
       "             2008006910: [0.062334924936294556,\n",
       "              0.18101489249916713,\n",
       "              0.2433498174354617],\n",
       "             2008006920: [1.0272074937820435,\n",
       "              0.4681840269682425,\n",
       "              1.495391520750286],\n",
       "             2008006921: [0.0025701229460537434,\n",
       "              0.10150595207283394,\n",
       "              0.10407607501888769],\n",
       "             2008006923: [1.8371169567108154,\n",
       "              0.3716977749786563,\n",
       "              2.208814731689472],\n",
       "             2008006926: [0.10389796644449234,\n",
       "              0.39016063002915247,\n",
       "              0.4940585964736448],\n",
       "             2010005775: [0.7506089806556702,\n",
       "              0.3158703659004685,\n",
       "              1.0664793465561386],\n",
       "             2010005776: [0.01754922792315483,\n",
       "              0.2113909061310184,\n",
       "              0.22894013405417324],\n",
       "             2008006933: [0.36086300015449524,\n",
       "              0.4631157500495728,\n",
       "              0.8239787502040681],\n",
       "             2010005782: [0.06945996731519699,\n",
       "              0.13434345066959258,\n",
       "              0.20380341798478957],\n",
       "             2008006936: [0.010730717331171036,\n",
       "              0.12193967306868686,\n",
       "              0.1326703903998579],\n",
       "             2010005785: [0.03697127103805542,\n",
       "              0.1088781571583367,\n",
       "              0.14584942819639213],\n",
       "             2010005791: [0.31234586238861084,\n",
       "              0.25346714361565037,\n",
       "              0.5658130060042612],\n",
       "             2010005794: [0.29383352398872375,\n",
       "              0.2535034867496273,\n",
       "              0.5473370107383511],\n",
       "             2010005796: [0.06371589750051498,\n",
       "              0.19698192428630834,\n",
       "              0.2606978217868233],\n",
       "             2010005800: [0.8480616807937622,\n",
       "              0.4291147331179069,\n",
       "              1.277176413911669],\n",
       "             2008006953: [0.3243456184864044,\n",
       "              0.41163820354160774,\n",
       "              0.7359838220280122],\n",
       "             2008006954: [0.004291268065571785,\n",
       "              0.12866970665967453,\n",
       "              0.13296097472524632],\n",
       "             2010005805: [1.108755111694336,\n",
       "              0.5953504076647316,\n",
       "              1.7041055193590675],\n",
       "             2010005807: [0.018744906410574913,\n",
       "              0.1374542006229284,\n",
       "              0.15619910703350331],\n",
       "             2008006960: [0.5054511427879333,\n",
       "              0.5254073861643095,\n",
       "              1.0308585289522427],\n",
       "             2008006961: [0.8393881320953369,\n",
       "              0.451498358737508,\n",
       "              1.290886490832845],\n",
       "             2010005810: [0.4812126159667969,\n",
       "              0.710594739761312,\n",
       "              1.191807355728109],\n",
       "             2008006962: [0.4676516056060791,\n",
       "              0.8059212763593305,\n",
       "              1.2735728819654097],\n",
       "             2008006965: [0.09267338365316391,\n",
       "              0.15720195409928334,\n",
       "              0.24987533775244725],\n",
       "             2008006969: [0.5634942650794983,\n",
       "              0.5869740371900111,\n",
       "              1.1504683022695095],\n",
       "             2008006973: [1.0094332695007324,\n",
       "              0.1700137314108774,\n",
       "              1.1794470009116098],\n",
       "             2010005821: [0.8933801054954529,\n",
       "              0.2937509116365758,\n",
       "              1.1871310171320286],\n",
       "             2010005823: [0.1269855499267578,\n",
       "              0.29161222911880186,\n",
       "              0.41859777904555967],\n",
       "             2010005825: [0.24782013893127441,\n",
       "              0.38430925536077204,\n",
       "              0.6321293942920465],\n",
       "             2010005826: [0.18288512527942657,\n",
       "              0.3826372968594815,\n",
       "              0.5655224221389081],\n",
       "             2010005830: [0.856796145439148,\n",
       "              0.5528439055612874,\n",
       "              1.4096400510004354],\n",
       "             2010005835: [0.12799035012722015,\n",
       "              0.3196981086800714,\n",
       "              0.4476884588072916],\n",
       "             2010005836: [0.8401212692260742,\n",
       "              0.4430738137215038,\n",
       "              1.283195082947578],\n",
       "             2010005840: [0.28053170442581177,\n",
       "              0.532322206899744,\n",
       "              0.8128539113255557],\n",
       "             2008006992: [0.09154529869556427,\n",
       "              0.14030847914026937,\n",
       "              0.23185377783583364],\n",
       "             2010005845: [0.00036359051591716707,\n",
       "              0.04574485588370951,\n",
       "              0.046108446399626676],\n",
       "             2010005847: [0.31647175550460815,\n",
       "              0.37197412458049084,\n",
       "              0.688445880085099],\n",
       "             2008007003: [0.5041003823280334,\n",
       "              0.4389389082875084,\n",
       "              0.9430392906155418],\n",
       "             2008007004: [0.08880049735307693,\n",
       "              0.24020038603377314,\n",
       "              0.3290008833868501],\n",
       "             2010005854: [0.4042165279388428,\n",
       "              0.6254821273218208,\n",
       "              1.0296986552606637],\n",
       "             2010005855: [0.014262170530855656,\n",
       "              0.07479016924482043,\n",
       "              0.08905233977567609],\n",
       "             2008007009: [0.42157769203186035,\n",
       "              0.29209200189320894,\n",
       "              0.7136696939250693],\n",
       "             2008007012: [0.0010918742045760155,\n",
       "              0.037861890984390606,\n",
       "              0.03895376518896662],\n",
       "             2008007014: [1.870373010635376,\n",
       "              0.1339557899717082,\n",
       "              2.0043288006070843],\n",
       "             2010005865: [0.6827773451805115,\n",
       "              0.37565650866075606,\n",
       "              1.0584338538412674],\n",
       "             2010005867: [0.02519812062382698,\n",
       "              0.17905012424864739,\n",
       "              0.20424824487247437],\n",
       "             2008007022: [1.0543421506881714,\n",
       "              0.4206936106084821,\n",
       "              1.4750357612966534],\n",
       "             2008007026: [0.6103100180625916,\n",
       "              0.4078391128037192,\n",
       "              1.0181491308663109],\n",
       "             2010005875: [0.37743431329727173,\n",
       "              0.3534235707410817,\n",
       "              0.7308578840383535],\n",
       "             2008007028: [0.7793424129486084,\n",
       "              0.27977981332733054,\n",
       "              1.059122226275939],\n",
       "             2010005876: [0.12222418189048767,\n",
       "              0.5375889022398105,\n",
       "              0.6598130841302982],\n",
       "             2008007030: [0.26241204142570496,\n",
       "              0.40404764729666065,\n",
       "              0.6664596887223656],\n",
       "             2010001406: [0.5232170820236206,\n",
       "              0.4362843714085543,\n",
       "              0.9595014534321749],\n",
       "             2010005874: [0.3735845983028412,\n",
       "              0.43329523994548735,\n",
       "              0.8068798382483285],\n",
       "             2008007038: [1.1235612630844116,\n",
       "              0.4868692203968097,\n",
       "              1.6104304834812213],\n",
       "             2008007039: [0.3043074309825897,\n",
       "              0.2310778690956245,\n",
       "              0.5353853000782143],\n",
       "             2008007043: [0.07659980654716492,\n",
       "              0.3745749890683312,\n",
       "              0.4511747956154961],\n",
       "             2010005891: [0.007141931913793087,\n",
       "              0.12445234721025245,\n",
       "              0.13159427912404553],\n",
       "             2010005892: [0.00012929753575008363,\n",
       "              0.0616290666003075,\n",
       "              0.061758364136057585],\n",
       "             2008007045: [0.07029390335083008,\n",
       "              0.29896824690733265,\n",
       "              0.3692621502581627],\n",
       "             2010005898: [0.9806753396987915,\n",
       "              0.4656983342604495,\n",
       "              1.446373673959241],\n",
       "             2008007054: [0.06383735686540604,\n",
       "              0.16107359694010226,\n",
       "              0.2249109538055083],\n",
       "             2010005904: [0.2024795413017273,\n",
       "              0.5725389632970697,\n",
       "              0.775018504598797],\n",
       "             2008007058: [1.2838081121444702,\n",
       "              0.4472216195412342,\n",
       "              1.7310297316857044],\n",
       "             2010005906: [1.4319871664047241,\n",
       "              0.5373686102174259,\n",
       "              1.96935577662215],\n",
       "             2008007060: [0.01283690519630909,\n",
       "              0.1363454651117589,\n",
       "              0.149182370308068],\n",
       "             2008007061: [1.040174961090088,\n",
       "              0.42902347051308687,\n",
       "              1.4691984316031748],\n",
       "             2010005909: [0.004671669099479914,\n",
       "              0.04192648045851571,\n",
       "              0.04659814955799562],\n",
       "             2010005919: [0.1353999376296997,\n",
       "              0.2784973549501848,\n",
       "              0.4138972925798845],\n",
       "             2010005921: [0.31833693385124207,\n",
       "              0.5246300479369603,\n",
       "              0.8429669817882024],\n",
       "             2008007073: [0.21998225152492523,\n",
       "              0.36401018805073854,\n",
       "              0.5839924395756637],\n",
       "             2008007075: [0.17919336259365082,\n",
       "              0.6282029713182535,\n",
       "              0.8073963339119044],\n",
       "             2008007076: [0.4312920570373535,\n",
       "              0.16747163849614444,\n",
       "              0.598763695533498],\n",
       "             2010005927: [0.19939427077770233,\n",
       "              0.27833824463808826,\n",
       "              0.4777325154157906],\n",
       "             2010005928: [0.29982447624206543,\n",
       "              0.4682288334492908,\n",
       "              0.7680533096913562],\n",
       "             2008007081: [0.06209944933652878,\n",
       "              0.21633261825600608,\n",
       "              0.27843206759253486],\n",
       "             2010005930: [0.15047596395015717,\n",
       "              0.37573307833021485,\n",
       "              0.526209042280372],\n",
       "             2008007082: [0.10218651592731476,\n",
       "              0.4644444866735532,\n",
       "              0.566631002600868],\n",
       "             2010005929: [0.4085799753665924,\n",
       "              0.28894633356741484,\n",
       "              0.6975263089340072],\n",
       "             2008007085: [0.3222798705101013,\n",
       "              0.40735474335842525,\n",
       "              0.7296346138685266],\n",
       "             2010005935: [0.09293768554925919,\n",
       "              0.21150813505629198,\n",
       "              0.30444582060555114],\n",
       "             2008007090: [0.9951773285865784,\n",
       "              0.6940181256314347,\n",
       "              1.6891954542180132],\n",
       "             2009001689: [0.008109349757432938,\n",
       "              0.13389594971375657,\n",
       "              0.1420052994711895],\n",
       "             2010005942: [0.9515570998191833,\n",
       "              0.5985817006511386,\n",
       "              1.550138800470322],\n",
       "             2008007095: [0.012729091569781303,\n",
       "              0.16770653894019694,\n",
       "              0.18043563050997824],\n",
       "             2008007097: [1.2295434474945068,\n",
       "              0.7081964509494545,\n",
       "              1.9377398984439613],\n",
       "             2008007098: [0.7480068206787109,\n",
       "              0.3313332180650279,\n",
       "              1.0793400387437388],\n",
       "             2010005948: [0.06930894404649734,\n",
       "              0.31043796094119236,\n",
       "              0.3797469049876897],\n",
       "             2008007101: [0.4712013304233551,\n",
       "              0.5513880644737903,\n",
       "              1.0225893948971454],\n",
       "             2010005949: [0.3766708970069885,\n",
       "              0.15387041675943802,\n",
       "              0.5305413137664265],\n",
       "             2010005951: [0.015737401321530342,\n",
       "              0.13491809654205789,\n",
       "              0.15065549786358823],\n",
       "             2010005952: [0.04353361949324608,\n",
       "              0.28332535139846227,\n",
       "              0.32685897089170834],\n",
       "             2010005954: [0.05396901071071625,\n",
       "              0.21832115940253807,\n",
       "              0.2722901701132543],\n",
       "             2008007106: [0.21790540218353271,\n",
       "              0.17228695084660878,\n",
       "              0.3901923530301415],\n",
       "             2010005957: [0.0009418934932909906,\n",
       "              0.05452622297213229,\n",
       "              0.05546811646542328],\n",
       "             2010005958: [0.05109846591949463,\n",
       "              0.2547820312742072,\n",
       "              0.3058804971937018],\n",
       "             2010005959: [0.4549526870250702,\n",
       "              0.2673468474617978,\n",
       "              0.722299534486868],\n",
       "             2010005960: [0.7714784145355225,\n",
       "              0.7112661250792169,\n",
       "              1.4827445396147394],\n",
       "             2008007118: [0.011112865060567856,\n",
       "              0.25285955710608865,\n",
       "              0.2639724221666565],\n",
       "             2010005967: [0.4945266842842102,\n",
       "              0.4503298833504849,\n",
       "              0.9448565676346952],\n",
       "             2010005968: [0.011431695893406868,\n",
       "              0.04028447515635702,\n",
       "              0.051716171049763886],\n",
       "             2010005972: [0.7630640268325806,\n",
       "              0.5614685208157022,\n",
       "              1.3245325476482828],\n",
       "             2008007124: [0.25624340772628784,\n",
       "              0.4186712585897665,\n",
       "              0.6749146663160543],\n",
       "             2010005974: [0.43515798449516296,\n",
       "              0.43724316755011494,\n",
       "              0.8724011520452779],\n",
       "             2010005975: [0.22544699907302856,\n",
       "              0.34412329534912245,\n",
       "              0.569570294422151],\n",
       "             2010001425: [0.002814039820805192,\n",
       "              0.09319706972256392,\n",
       "              0.09601110954336911],\n",
       "             2008007129: [0.027295652776956558,\n",
       "              0.11762848840816238,\n",
       "              0.14492414118511893],\n",
       "             2010005978: [0.619152307510376,\n",
       "              0.34036284594053456,\n",
       "              0.9595151534509105],\n",
       "             2008007131: [0.036487311124801636,\n",
       "              0.11852410038486569,\n",
       "              0.15501141150966732],\n",
       "             2010005982: [0.2892608344554901,\n",
       "              0.41172610131429166,\n",
       "              0.7009869357697818],\n",
       "             2010005984: [0.0453994981944561,\n",
       "              0.1685066054873874,\n",
       "              0.2139061036818435],\n",
       "             2010005985: [0.4237077832221985,\n",
       "              0.5233300019515489,\n",
       "              0.9470377851737474],\n",
       "             2008007138: [0.44708260893821716,\n",
       "              0.479392863355246,\n",
       "              0.9264754722934632],\n",
       "             2010005987: [0.19942966103553772,\n",
       "              0.32053656765503175,\n",
       "              0.5199662286905695],\n",
       "             2009004392: [0.04864020273089409,\n",
       "              0.13610203501453458,\n",
       "              0.18474223774542867],\n",
       "             2008007142: [0.04457572475075722,\n",
       "              0.09877350090360479,\n",
       "              0.143349225654362],\n",
       "             2008007145: [0.24665546417236328,\n",
       "              0.2178105391434516,\n",
       "              0.4644660033158149],\n",
       "             2008007146: [0.7190782427787781,\n",
       "              0.6066162077169268,\n",
       "              1.325694450495705],\n",
       "             2008007147: [0.6301298141479492,\n",
       "              0.515275141671704,\n",
       "              1.1454049558196533],\n",
       "             2010005996: [1.567496418952942,\n",
       "              0.7432460061882332,\n",
       "              2.310742425141175],\n",
       "             2010005995: [0.017448078840970993,\n",
       "              0.17505628937946294,\n",
       "              0.19250436822043393],\n",
       "             2008007151: [0.04270177334547043,\n",
       "              0.264313557627415,\n",
       "              0.3070153309728854],\n",
       "             2008007156: [0.1649121642112732,\n",
       "              0.2884539904735059,\n",
       "              0.4533661546847791],\n",
       "             2010006009: [0.263541579246521,\n",
       "              0.2834175005658199,\n",
       "              0.5469590798123409],\n",
       "             2008007161: [0.10650043189525604,\n",
       "              0.16257002896501307,\n",
       "              0.2690704608602691],\n",
       "             2010006012: [1.181504487991333,\n",
       "              0.4984976744362867,\n",
       "              1.6800021624276198],\n",
       "             2008007165: [1.678923487663269,\n",
       "              0.5707435117946859,\n",
       "              2.249666999457955],\n",
       "             2010006015: [1.241896629333496,\n",
       "              0.446465848999049,\n",
       "              1.688362478332545],\n",
       "             2008007168: [0.890821635723114,\n",
       "              0.4109497396104018,\n",
       "              1.3017713753335158],\n",
       "             2008007169: [0.3212045431137085,\n",
       "              0.4093346145859439,\n",
       "              0.7305391576996524],\n",
       "             2010006023: [0.7320731282234192,\n",
       "              0.3300140879855998,\n",
       "              1.062087216209019],\n",
       "             2008007179: [1.2613286972045898,\n",
       "              0.5826551568013679,\n",
       "              1.8439838540059577],\n",
       "             2010006028: [0.42500656843185425,\n",
       "              0.7481074214451059,\n",
       "              1.1731139898769603],\n",
       "             2008007185: [1.5593173503875732,\n",
       "              0.6197208980279411,\n",
       "              2.179038248415514],\n",
       "             2010006040: [0.05264873802661896,\n",
       "              0.1402022099656864,\n",
       "              0.19285094799230534],\n",
       "             2010006042: [0.03238563984632492,\n",
       "              0.16349160511783378,\n",
       "              0.1958772449641587],\n",
       "             2008007197: [0.5190513730049133,\n",
       "              0.37497922306239895,\n",
       "              0.8940305960673123],\n",
       "             2011001169: [0.6640082001686096,\n",
       "              0.18913576071839175,\n",
       "              0.8531439608870014],\n",
       "             2010006050: [0.005719698965549469,\n",
       "              0.07951772627780315,\n",
       "              0.08523742524335262],\n",
       "             2008007205: [0.5085813403129578,\n",
       "              0.5141554949074033,\n",
       "              1.022736835220361],\n",
       "             2008007208: [0.010984344407916069,\n",
       "              0.17774667379681738,\n",
       "              0.18873101820473345],\n",
       "             2008007211: [0.03278207406401634,\n",
       "              0.1293116404149754,\n",
       "              0.16209371447899174],\n",
       "             2010006063: [0.016516145318746567,\n",
       "              0.12167616045284581,\n",
       "              0.13819230577159236],\n",
       "             2008007218: [0.12864039838314056,\n",
       "              0.3678993747760875,\n",
       "              0.49653977315922804],\n",
       "             2010006067: [0.05409760773181915,\n",
       "              0.21104541542793287,\n",
       "              0.265143023159752],\n",
       "             2010006066: [0.020749425515532494,\n",
       "              0.09108685309111432,\n",
       "              0.11183627860664681],\n",
       "             2008007221: [0.752922773361206,\n",
       "              0.42487919251738804,\n",
       "              1.177801965878594],\n",
       "             2008007223: [0.4100549519062042,\n",
       "              0.4715357943785834,\n",
       "              0.8815907462847876],\n",
       "             2010006078: [0.03626253828406334,\n",
       "              0.11476823956681159,\n",
       "              0.15103077785087493],\n",
       "             2010006079: [0.09290816634893417,\n",
       "              0.1702646138835399,\n",
       "              0.26317278023247404],\n",
       "             2008007236: [0.0025032279081642628,\n",
       "              0.10407987670962812,\n",
       "              0.10658310461779238],\n",
       "             2008007237: [1.1707804203033447,\n",
       "              0.6011895435339463,\n",
       "              1.771969963837291],\n",
       "             2008007239: [0.014734161086380482,\n",
       "              0.1356200871521308,\n",
       "              0.1503542482385113],\n",
       "             2008007242: [0.4583362340927124,\n",
       "              0.21516059985071107,\n",
       "              0.6734968339434235],\n",
       "             2009001719: [0.07544931024312973,\n",
       "              0.14438245395485771,\n",
       "              0.21983176419798744],\n",
       "             2008007246: [0.34222182631492615,\n",
       "              0.3384687418665072,\n",
       "              0.6806905681814334],\n",
       "             2008007252: [1.9254297018051147,\n",
       "              0.6884828385679752,\n",
       "              2.61391254037309],\n",
       "             2008007254: [0.4076821208000183,\n",
       "              0.45704646300106405,\n",
       "              0.8647285838010823],\n",
       "             2008007260: [0.4157503545284271,\n",
       "              0.46419813571788127,\n",
       "              0.8799484902463084],\n",
       "             2008007261: [0.7917829155921936,\n",
       "              0.38641536341195504,\n",
       "              1.1781982790041488],\n",
       "             2008007265: [0.033885009586811066,\n",
       "              0.08709004265459822,\n",
       "              0.12097505224140928],\n",
       "             2009001724: [0.06016999110579491,\n",
       "              0.13542327715489216,\n",
       "              0.19559326826068707],\n",
       "             2008007274: [0.09977786988019943,\n",
       "              0.1697893133981029,\n",
       "              0.2695671832783023],\n",
       "             2008007280: [0.40582066774368286,\n",
       "              0.18426526777361976,\n",
       "              0.5900859355173026],\n",
       "             2008007281: [1.7343186140060425,\n",
       "              0.6645674518720958,\n",
       "              2.398886065878138],\n",
       "             2008007286: [0.4748472273349762,\n",
       "              0.2941393320314948,\n",
       "              0.768986559366471],\n",
       "             2008007289: [0.3234538733959198,\n",
       "              0.5830884647743814,\n",
       "              0.9065423381703012],\n",
       "             2008007291: [0.41199472546577454,\n",
       "              0.5551589820642082,\n",
       "              0.9671537075299828],\n",
       "             2010003086: [0.005383692681789398,\n",
       "              0.10185469795885856,\n",
       "              0.10723839064064795],\n",
       "             2008007298: [0.35308223962783813,\n",
       "              0.3364610858284335,\n",
       "              0.6895433254562716],\n",
       "             2008007307: [1.3522472381591797,\n",
       "              0.5010751549650032,\n",
       "              1.853322393124183],\n",
       "             2008007312: [0.02173980139195919,\n",
       "              0.1966149792083652,\n",
       "              0.2183547806003244],\n",
       "             2008007313: [0.10629251599311829,\n",
       "              0.29718018705542937,\n",
       "              0.40347270304854765],\n",
       "             2008007320: [0.30988284945487976,\n",
       "              0.3590252672984563,\n",
       "              0.6689081167533361],\n",
       "             2008007321: [0.7009460926055908,\n",
       "              0.3699367675172669,\n",
       "              1.0708828601228577],\n",
       "             2008007325: [0.35226601362228394,\n",
       "              0.47836643383791294,\n",
       "              0.8306324474601969],\n",
       "             2008007335: [0.35852015018463135,\n",
       "              0.44942667609201614,\n",
       "              0.8079468262766475],\n",
       "             2008007343: [0.27549201250076294,\n",
       "              0.3470218881287329,\n",
       "              0.6225139006294959],\n",
       "             2008007346: [0.21166376769542694,\n",
       "              0.38112826378822834,\n",
       "              0.5927920314836552],\n",
       "             2008007353: [7.798364822519943e-05,\n",
       "              0.054594660993518326,\n",
       "              0.054672644641743526],\n",
       "             2008007356: [0.23279628157615662,\n",
       "              0.5531810788743872,\n",
       "              0.7859773604505438],\n",
       "             2008007357: [0.9837626814842224,\n",
       "              0.16167503614894066,\n",
       "              1.1454377176331632],\n",
       "             2008007361: [0.29396897554397583,\n",
       "              0.3039791206283254,\n",
       "              0.5979480961723012],\n",
       "             2008007363: [0.2614555060863495,\n",
       "              0.4402118691390954,\n",
       "              0.7016673752254449],\n",
       "             2008007364: [0.14658518135547638,\n",
       "              0.1077961753504197,\n",
       "              0.25438135670589607],\n",
       "             2008007375: [0.427421897649765,\n",
       "              0.35954779466536435,\n",
       "              0.7869696923151294],\n",
       "             2008007382: [2.2791621685028076,\n",
       "              0.7496490675162013,\n",
       "              3.028811236019009],\n",
       "             2008007388: [1.237724781036377,\n",
       "              0.4899173677679169,\n",
       "              1.727642148804294],\n",
       "             2008007394: [0.283191442489624,\n",
       "              0.26917145198250786,\n",
       "              0.5523628944721319],\n",
       "             2008007397: [0.2337769865989685,\n",
       "              0.3217495654098896,\n",
       "              0.5555265520088581],\n",
       "             2008007410: [0.29198673367500305,\n",
       "              0.3396313584033642,\n",
       "              0.6316180920783673],\n",
       "             2008007421: [0.03540348261594772,\n",
       "              0.28259453906115517,\n",
       "              0.3179980216771029],\n",
       "             2008007423: [1.216066598892212,\n",
       "              0.46935024362537003,\n",
       "              1.685416842517582],\n",
       "             2008007424: [0.00043469489901326597,\n",
       "              0.13343448989805645,\n",
       "              0.13386918479706972],\n",
       "             2008007425: [0.3178669810295105,\n",
       "              0.34044200062779467,\n",
       "              0.6583089816573051],\n",
       "             2008007428: [0.1395612359046936,\n",
       "              0.2801911516622795,\n",
       "              0.4197523875669731],\n",
       "             2008007432: [0.13707390427589417,\n",
       "              0.2652130780752233,\n",
       "              0.4022869823511175],\n",
       "             2008007433: [0.15443503856658936,\n",
       "              0.3313557627968572,\n",
       "              0.48579080136344654],\n",
       "             2008007438: [0.5823352932929993,\n",
       "              0.486501489209781,\n",
       "              1.0688367825027802],\n",
       "             2008007442: [0.46322110295295715,\n",
       "              0.48154199285283966,\n",
       "              0.9447630958057969],\n",
       "             2008007443: [0.45038172602653503,\n",
       "              0.5047432111779828,\n",
       "              0.9551249372045179],\n",
       "             2008007444: [0.4295884966850281,\n",
       "              0.39392931200868925,\n",
       "              0.8235178086937174],\n",
       "             2008007448: [0.2133519947528839,\n",
       "              0.33517365171815106,\n",
       "              0.548525646471035],\n",
       "             2008003659: [0.03283219784498215,\n",
       "              0.15363962202876244,\n",
       "              0.1864718198737446],\n",
       "             2008007456: [0.6132285594940186,\n",
       "              0.45449112237315936,\n",
       "              1.067719681867178],\n",
       "             2008007465: [0.05612611398100853,\n",
       "              0.1899888536298675,\n",
       "              0.24611496761087603],\n",
       "             2008007469: [0.30425959825515747,\n",
       "              0.3834593057187647,\n",
       "              0.6877189039739222],\n",
       "             2008007470: [0.26962336897850037,\n",
       "              0.4688574307466081,\n",
       "              0.7384807997251084],\n",
       "             2008007471: [0.015621964819729328,\n",
       "              0.07339457986276626,\n",
       "              0.08901654468249559],\n",
       "             2008007472: [0.054132960736751556,\n",
       "              0.09848373549625432,\n",
       "              0.1526166962330059],\n",
       "             2008007473: [0.7640914916992188,\n",
       "              0.4908647450240577,\n",
       "              1.2549562367232765],\n",
       "             2008007477: [0.2783071994781494,\n",
       "              0.25375523287798607,\n",
       "              0.5320624323561355],\n",
       "             2009003395: [0.2583489716053009,\n",
       "              0.43905363148110343,\n",
       "              0.6974026030864043],\n",
       "             2008007485: [0.40926384925842285,\n",
       "              0.49016805985271367,\n",
       "              0.8994319091111365],\n",
       "             2008007486: [0.0293317548930645,\n",
       "              0.22663872627639003,\n",
       "              0.25597048116945453],\n",
       "             2008007491: [0.021461322903633118,\n",
       "              0.16706424243112947,\n",
       "              0.18852556533476258],\n",
       "             2008007496: [0.00034857631544582546,\n",
       "              0.22934539367187276,\n",
       "              0.22969396998731859],\n",
       "             2008007500: [0.26433366537094116,\n",
       "              0.19410557223517697,\n",
       "              0.45843923760611816],\n",
       "             2008007504: [0.0015795489307492971,\n",
       "              0.2063813136498725,\n",
       "              0.2079608625806218],\n",
       "             2008007509: [0.19385740160942078,\n",
       "              0.48345408103731724,\n",
       "              0.677311482646738],\n",
       "             2008007510: [0.5390854477882385,\n",
       "              0.739331949593124,\n",
       "              1.2784173973813626],\n",
       "             2008007515: [0.03950899839401245,\n",
       "              0.0899304196959009,\n",
       "              0.12943941808991336],\n",
       "             2008007519: [0.1809970885515213,\n",
       "              0.14211437785807973,\n",
       "              0.32311146640960103],\n",
       "             2009003402: [0.3586885929107666,\n",
       "              0.28774178740332307,\n",
       "              0.6464303803140896],\n",
       "             2008007524: [0.8754700422286987,\n",
       "              0.6575381511603915,\n",
       "              1.5330081933890902],\n",
       "             2008007528: [1.5583776235580444,\n",
       "              0.6253699926545634,\n",
       "              2.183747616212608],\n",
       "             2008007533: [0.00015630904817953706,\n",
       "              0.1577771090231977,\n",
       "              0.15793341807137723],\n",
       "             2008007537: [0.04694608598947525,\n",
       "              0.1782688303374318,\n",
       "              0.22521491632690704],\n",
       "             2008007544: [0.01640985533595085,\n",
       "              0.15608533219419402,\n",
       "              0.17249518753014487],\n",
       "             2008007546: [0.07462354749441147,\n",
       "              0.19363991985258136,\n",
       "              0.2682634673469928],\n",
       "             2010003138: [0.11873059719800949,\n",
       "              0.28129214610108433,\n",
       "              0.4000227432990938],\n",
       "             2008007556: [0.5200065970420837,\n",
       "              0.46919351247365626,\n",
       "              0.98920010951574],\n",
       "             2008007559: [0.03281306475400925,\n",
       "              0.0815712844704644,\n",
       "              0.11438434922447364],\n",
       "             2008007565: [0.8995426893234253,\n",
       "              0.9283582477921974,\n",
       "              1.8279009371156227],\n",
       "             2008007573: [1.6254870891571045,\n",
       "              0.5075538418675217,\n",
       "              2.133040931024626],\n",
       "             2008007576: [0.25801247358322144,\n",
       "              0.3754807122738594,\n",
       "              0.6334931858570808],\n",
       "             2008007581: [0.05122249945998192,\n",
       "              0.3103187105121529,\n",
       "              0.3615412099721348],\n",
       "             2008007584: [0.4805121123790741,\n",
       "              0.6510478040670048,\n",
       "              1.1315599164460788],\n",
       "             2008007588: [0.4996325671672821,\n",
       "              0.3387996226867383,\n",
       "              0.8384321898540203],\n",
       "             2008007589: [0.2959502935409546,\n",
       "              0.39167911429284646,\n",
       "              0.687629407833801],\n",
       "             2008007593: [0.6261152625083923,\n",
       "              0.4882758063645252,\n",
       "              1.1143910688729175],\n",
       "             2008007597: [1.6214544773101807,\n",
       "              0.5583200810633359,\n",
       "              2.1797745583735164],\n",
       "             2008007604: [0.30937761068344116,\n",
       "              0.3111015348465105,\n",
       "              0.6204791455299516],\n",
       "             2008007608: [1.3026663064956665,\n",
       "              0.6648233459692352,\n",
       "              1.9674896524649017],\n",
       "             2008007611: [0.022546865046024323,\n",
       "              0.07075597364997444,\n",
       "              0.09330283869599876],\n",
       "             2008007613: [0.03803497180342674,\n",
       "              0.2285644959081118,\n",
       "              0.26659946771153853],\n",
       "             2008007621: [0.0056288475170731544,\n",
       "              0.04866991934655552,\n",
       "              0.054298766863628674],\n",
       "             2008007625: [0.25879380106925964,\n",
       "              0.31011671811210917,\n",
       "              0.5689105191813688],\n",
       "             2008007629: [3.909680128097534,\n",
       "              1.094547553553454,\n",
       "              5.004227681650988],\n",
       "             2008007630: [0.0816621333360672,\n",
       "              0.2468922465734922,\n",
       "              0.3285543799095594],\n",
       "             2008007640: [0.03253534436225891,\n",
       "              0.10217928375550328,\n",
       "              0.1347146281177622],\n",
       "             2008007641: [0.0004201699048280716,\n",
       "              0.06490399733218712,\n",
       "              0.06532416723701519],\n",
       "             2008007646: [0.12797290086746216,\n",
       "              0.4095977769341824,\n",
       "              0.5375706778016446],\n",
       "             2008007648: [1.5609647035598755,\n",
       "              0.8141444562298015,\n",
       "              2.3751091597896767],\n",
       "             2008007653: [0.5680701732635498,\n",
       "              0.42750681625798537,\n",
       "              0.9955769895215352],\n",
       "             2008007660: [1.1170700788497925,\n",
       "              0.5828624576671494,\n",
       "              1.699932536516942],\n",
       "             2008007664: [1.1408922672271729,\n",
       "              0.4075450149535968,\n",
       "              1.5484372821807697],\n",
       "             2008007665: [0.04980722814798355,\n",
       "              0.14282657321174516,\n",
       "              0.1926338013597287],\n",
       "             2008007666: [0.5802468657493591,\n",
       "              0.5319182640861619,\n",
       "              1.112165129835521],\n",
       "             2010003162: [0.0004825465730391443,\n",
       "              0.10973117348714895,\n",
       "              0.11021372006018809],\n",
       "             2008007675: [1.6754424571990967,\n",
       "              0.48710007171900144,\n",
       "              2.162542528918098],\n",
       "             2008007682: [0.20809338986873627,\n",
       "              0.4460306803451331,\n",
       "              0.6541240702138693],\n",
       "             2008007683: [0.18117669224739075,\n",
       "              0.3223813591084823,\n",
       "              0.5035580513558731],\n",
       "             2008007691: [0.25713402032852173,\n",
       "              0.33605820769764944,\n",
       "              0.5931922280261712],\n",
       "             2008007692: [1.1900142431259155,\n",
       "              0.6734036421983229,\n",
       "              1.8634178853242385],\n",
       "             2008007696: [0.37245044112205505,\n",
       "              0.4437417730315,\n",
       "              0.816192214153555],\n",
       "             2008007697: [0.01948457583785057,\n",
       "              0.21737975263152443,\n",
       "              0.236864328469375],\n",
       "             2008007698: [0.37001481652259827,\n",
       "              0.29955072648872627,\n",
       "              0.6695655430113245],\n",
       "             2008007701: [0.30050042271614075,\n",
       "              0.2639399161296542,\n",
       "              0.5644403388457949],\n",
       "             2008007709: [0.2694312036037445,\n",
       "              0.3086279556602667,\n",
       "              0.5780591592640112],\n",
       "             2008007710: [0.3211594820022583,\n",
       "              0.41499564376702586,\n",
       "              0.7361551257692842],\n",
       "             2008007717: [1.0795124769210815,\n",
       "              0.6015753957804567,\n",
       "              1.6810878727015384],\n",
       "             2008007724: [0.7025837898254395,\n",
       "              0.5428311879933795,\n",
       "              1.245414977818819],\n",
       "             2008007726: [2.996532440185547,\n",
       "              0.23269382950865103,\n",
       "              3.229226269694198],\n",
       "             2008007730: [0.2203597128391266,\n",
       "              0.18998763196572732,\n",
       "              0.4103473448048539],\n",
       "             2008007742: [0.1932622194290161,\n",
       "              0.3401522499963484,\n",
       "              0.5334144694253645],\n",
       "             2008007746: [0.6935919523239136,\n",
       "              0.5629257930847462,\n",
       "              1.2565177454086598],\n",
       "             2008007748: [0.2912718951702118,\n",
       "              0.3248648478626278,\n",
       "              0.6161367430328396],\n",
       "             2008007750: [1.5202171802520752,\n",
       "              0.47046909015880506,\n",
       "              1.9906862704108803],\n",
       "             2008007752: [0.35457301139831543,\n",
       "              0.493617692694183,\n",
       "              0.8481907040924984],\n",
       "             2008007755: [0.3954603374004364,\n",
       "              0.36094143385654737,\n",
       "              0.7564017712569837],\n",
       "             2008007758: [0.5304707884788513,\n",
       "              0.46641545063421846,\n",
       "              0.9968862391130697],\n",
       "             2008007759: [0.04245351254940033,\n",
       "              0.2378848151284661,\n",
       "              0.28033832767786643],\n",
       "             2008007761: [0.9648747444152832,\n",
       "              0.5940673899459935,\n",
       "              1.5589421343612768],\n",
       "             2008007770: [0.5380898118019104,\n",
       "              0.5211334031854202,\n",
       "              1.0592232149873306],\n",
       "             2008007777: [8.040969987632707e-07,\n",
       "              0.03662238152645764,\n",
       "              0.0366231856234564],\n",
       "             2008007779: [1.906436800956726,\n",
       "              0.6899924293039215,\n",
       "              2.5964292302606475],\n",
       "             2008007780: [0.0635853260755539,\n",
       "              0.2496882285753605,\n",
       "              0.3132735546509144],\n",
       "             2008007781: [0.020926326513290405,\n",
       "              0.14957914907244652,\n",
       "              0.17050547558573692],\n",
       "             2008007786: [1.428824782371521,\n",
       "              0.5760030381221259,\n",
       "              2.004827820493647],\n",
       "             2008007788: [0.42567554116249084,\n",
       "              0.38107848017118573,\n",
       "              0.8067540213336766],\n",
       "             2008007789: [0.1672622561454773,\n",
       "              0.3668259031447421,\n",
       "              0.5340881592902194],\n",
       "             2009001828: [0.1417519450187683,\n",
       "              0.2781881439556953,\n",
       "              0.4199400889744636],\n",
       "             2008007805: [0.005354920402169228,\n",
       "              0.08119654431975129,\n",
       "              0.08655146472192052],\n",
       "             2008007812: [0.7404751181602478,\n",
       "              0.30387066489283365,\n",
       "              1.0443457830530813],\n",
       "             2008007817: [0.1152363121509552,\n",
       "              0.33952784868352814,\n",
       "              0.45476416083448334],\n",
       "             2008007825: [0.013394713401794434,\n",
       "              0.1417264171029399,\n",
       "              0.15512113050473433],\n",
       "             2008007833: [0.4619513154029846,\n",
       "              0.3560017351986915,\n",
       "              0.8179530506016761],\n",
       "             2008007837: [0.1916288435459137,\n",
       "              0.3864734339802196,\n",
       "              0.5781022775261333],\n",
       "             2008007840: [0.029829788953065872,\n",
       "              0.17076799372891288,\n",
       "              0.20059778268197875],\n",
       "             2008007842: [0.08311121165752411,\n",
       "              0.09768132854868328,\n",
       "              0.18079254020620739],\n",
       "             2008007843: [0.7950264811515808,\n",
       "              0.5513401798867047,\n",
       "              1.3463666610382856],\n",
       "             2008007848: [0.1019626334309578,\n",
       "              0.24346393785816023,\n",
       "              0.345426571289118],\n",
       "             2010001569: [0.25023558735847473,\n",
       "              0.222314148105954,\n",
       "              0.47254973546442874],\n",
       "             2008007852: [0.2914077937602997,\n",
       "              0.3729254718965893,\n",
       "              0.664333265656889],\n",
       "             2008007858: [0.27578094601631165,\n",
       "              0.21022815178406784,\n",
       "              0.4860090978003795],\n",
       "             2008007861: [0.4164712727069855,\n",
       "              0.4739400528841589,\n",
       "              0.8904113255911443],\n",
       "             2008007864: [0.10330960899591446,\n",
       "              0.09305230867645643,\n",
       "              0.19636191767237088],\n",
       "             2008007869: [1.1038157939910889,\n",
       "              0.790104219162187,\n",
       "              1.8939200131532758],\n",
       "             2008007870: [2.813143014907837,\n",
       "              0.3858904548934468,\n",
       "              3.1990334698012837],\n",
       "             2008007873: [0.032845109701156616,\n",
       "              0.22049129231566175,\n",
       "              0.2533364020168184],\n",
       "             2008007877: [1.311613917350769,\n",
       "              0.540463968436749,\n",
       "              1.852077885787518],\n",
       "             2008007879: [0.001291370252147317,\n",
       "              0.06598501484322691,\n",
       "              0.06727638509537423],\n",
       "             2008007882: [0.07388041168451309,\n",
       "              0.20334537857189566,\n",
       "              0.27722579025640876],\n",
       "             2008007883: [0.4306235611438751,\n",
       "              0.3962408677688291,\n",
       "              0.8268644289127043],\n",
       "             2008007891: [0.021512404084205627,\n",
       "              0.18287756327943921,\n",
       "              0.20438996736364484],\n",
       "             2008007895: [0.5310295820236206,\n",
       "              0.4213540304787488,\n",
       "              0.9523836125023695],\n",
       "             2008007897: [0.48128384351730347,\n",
       "              0.4701821031411893,\n",
       "              0.9514659466584927],\n",
       "             2011002937: [0.5072861909866333,\n",
       "              0.29100025618272585,\n",
       "              0.7982864471693591],\n",
       "             2008007904: [0.17344501614570618,\n",
       "              0.1104699529709357,\n",
       "              0.2839149691166419],\n",
       "             2008007907: [0.013158497400581837,\n",
       "              0.11863889985000295,\n",
       "              0.13179739725058479],\n",
       "             2011002938: [0.1300806850194931,\n",
       "              0.21524359266660054,\n",
       "              0.34532427768609364],\n",
       "             2008007912: [0.014368907548487186,\n",
       "              0.3859835856859697,\n",
       "              0.4003524932344569],\n",
       "             2008007913: [0.42257869243621826,\n",
       "              0.46620582837390095,\n",
       "              0.8887845208101193],\n",
       "             2008007918: [0.015096002258360386,\n",
       "              0.2003294349247317,\n",
       "              0.2154254371830921],\n",
       "             2008007928: [0.47731995582580566,\n",
       "              0.4163656945697713,\n",
       "              0.893685650395577],\n",
       "             2008007937: [0.010026841424405575,\n",
       "              0.077180729478465,\n",
       "              0.08720757090287057],\n",
       "             2008007938: [0.000799994042608887, 0.0, 0.000799994042608887],\n",
       "             2008007940: [0.016094829887151718,\n",
       "              0.22473884440860004,\n",
       "              0.24083367429575175],\n",
       "             2008007941: [1.0997309684753418,\n",
       "              0.4709352451889866,\n",
       "              1.5706662136643283],\n",
       "             2008007947: [1.0203174352645874,\n",
       "              0.4130558778832388,\n",
       "              1.4333733131478261],\n",
       "             2008007950: [0.559998095035553,\n",
       "              0.3894896850022555,\n",
       "              0.9494877800378085],\n",
       "             2008007953: [1.0611435174942017,\n",
       "              0.6236182538399688,\n",
       "              1.6847617713341705],\n",
       "             2008007962: [0.37077730894088745,\n",
       "              0.4397534741728485,\n",
       "              0.8105307831137359],\n",
       "             2008007969: [0.002242842223495245,\n",
       "              0.1919105736824887,\n",
       "              0.19415341590598395],\n",
       "             2008007975: [0.01188331376761198,\n",
       "              0.25512741750921947,\n",
       "              0.26701073127683145],\n",
       "             2008007977: [0.18330344557762146,\n",
       "              0.6366873597161737,\n",
       "              0.8199908052937952],\n",
       "             2010001596: [0.6276183128356934,\n",
       "              0.5446979581747833,\n",
       "              1.1723162710104766],\n",
       "             2008007988: [0.5287653803825378,\n",
       "              0.5305351392746852,\n",
       "              1.059300519657223],\n",
       "             2008007990: [0.41503217816352844,\n",
       "              0.4074448973600306,\n",
       "              0.8224770755235591],\n",
       "             2008007997: [0.1741267591714859,\n",
       "              0.35327929058258917,\n",
       "              0.5274060497540751],\n",
       "             2008007998: [0.4911283552646637,\n",
       "              0.3886528099308784,\n",
       "              0.8797811651955421],\n",
       "             2008008002: [0.18358497321605682,\n",
       "              0.3800665990585708,\n",
       "              0.5636515722746276],\n",
       "             2008008004: [0.0038018980994820595,\n",
       "              0.19839399820125175,\n",
       "              0.2021958963007338],\n",
       "             2008008007: [0.09331128746271133,\n",
       "              0.21705636922182198,\n",
       "              0.3103676566845333],\n",
       "             2008008012: [0.9859686493873596,\n",
       "              0.4014925523291114,\n",
       "              1.3874612017164711],\n",
       "             2010001602: [0.1325756460428238,\n",
       "              0.10269593810615223,\n",
       "              0.23527158414897603],\n",
       "             2008008018: [0.015126814134418964,\n",
       "              0.08233198350455818,\n",
       "              0.09745879763897715],\n",
       "             2008008021: [0.08163780719041824,\n",
       "              0.47861368099936175,\n",
       "              0.56025148818978],\n",
       "             2010001603: [0.033183589577674866,\n",
       "              0.1639137525782339,\n",
       "              0.19709734215590877],\n",
       "             2008008028: [0.1824425905942917,\n",
       "              0.25875486217511096,\n",
       "              0.44119745276940264],\n",
       "             2008008031: [0.10097645968198776,\n",
       "              0.18320097365376686,\n",
       "              0.2841774333357546],\n",
       "             2008008034: [0.45717519521713257,\n",
       "              0.33465578712864186,\n",
       "              0.7918309823457744],\n",
       "             2008008037: [0.09732916951179504,\n",
       "              0.2122112673716784,\n",
       "              0.30954043688347344],\n",
       "             2008008043: [1.7946101427078247,\n",
       "              0.7077479158272452,\n",
       "              2.5023580585350698],\n",
       "             2008008048: [0.8993458151817322,\n",
       "              0.6173542400543512,\n",
       "              1.5167000552360834],\n",
       "             2008008058: [0.26544836163520813,\n",
       "              0.32233578598277257,\n",
       "              0.5877841476179807],\n",
       "             2008008064: [0.3057655394077301,\n",
       "              0.3427310597008123,\n",
       "              0.6484965991085424],\n",
       "             2008008070: [0.45251452922821045,\n",
       "              0.40484490184603394,\n",
       "              0.8573594310742444],\n",
       "             2008008072: [0.35838162899017334,\n",
       "              0.5503928405621797,\n",
       "              0.9087744695523531],\n",
       "             2008008073: [2.7486283779144287,\n",
       "              0.9360670127617151,\n",
       "              3.684695390676144],\n",
       "             2009001885: [0.13007740676403046,\n",
       "              0.1586174046219856,\n",
       "              0.28869481138601605],\n",
       "             2008008080: [0.5546742677688599,\n",
       "              0.3818736246384482,\n",
       "              0.9365478924073081],\n",
       "             2008008083: [1.6993036270141602,\n",
       "              0.6408308873290515,\n",
       "              2.3401345143432115],\n",
       "             2008008092: [0.504641592502594,\n",
       "              0.5859319638974162,\n",
       "              1.09057355640001],\n",
       "             2008008095: [1.418851613998413,\n",
       "              0.5661556212128789,\n",
       "              1.985007235211292],\n",
       "             2008008097: [0.6336533427238464,\n",
       "              0.6141909453319766,\n",
       "              1.247844288055823],\n",
       "             2008008098: [0.297267884016037,\n",
       "              0.5533297792400449,\n",
       "              0.8505976632560819],\n",
       "             2009004105: [1.048134207725525,\n",
       "              0.4632054545275718,\n",
       "              1.5113396622530968],\n",
       "             2008008112: [0.5711703896522522,\n",
       "              0.5161846072432489,\n",
       "              1.0873549968955012],\n",
       "             2008008116: [0.75616455078125,\n",
       "              0.7086098919029847,\n",
       "              1.4647744426842348],\n",
       "             2008008121: [1.7866894006729126,\n",
       "              0.7044200394420258,\n",
       "              2.4911094401149385],\n",
       "             2008008122: [0.04939429089426994,\n",
       "              0.19106992697089664,\n",
       "              0.24046421786516659],\n",
       "             2008008125: [0.2663417160511017,\n",
       "              0.3344587678487297,\n",
       "              0.6008004838998313],\n",
       "             2008008132: [0.014203635044395924,\n",
       "              0.14198932088517904,\n",
       "              0.15619295592957497],\n",
       "             2008008147: [1.6435625553131104,\n",
       "              0.5836918458951831,\n",
       "              2.2272544012082935],\n",
       "             2008008148: [0.4067016541957855,\n",
       "              0.5027516787779291,\n",
       "              0.9094533329737147],\n",
       "             2008008150: [2.5212035179138184,\n",
       "              0.6521285343842474,\n",
       "              3.1733320522980657],\n",
       "             2008008152: [0.2328191101551056,\n",
       "              0.25654381842731827,\n",
       "              0.48936292858242386],\n",
       "             2008008154: [0.3222586214542389,\n",
       "              0.2304060877065035,\n",
       "              0.5526647091607424],\n",
       "             2008008162: [0.9936086535453796,\n",
       "              0.5284619043213528,\n",
       "              1.5220705578667324],\n",
       "             2008008166: [0.011021145619452,\n",
       "              0.09442843958638343,\n",
       "              0.10544958520583543],\n",
       "             2008008169: [0.07582128047943115,\n",
       "              0.12149089604528118,\n",
       "              0.19731217652471233],\n",
       "             2008008170: [0.0936945378780365,\n",
       "              0.25856086223715335,\n",
       "              0.35225540011518985],\n",
       "             2008008176: [1.1684266328811646,\n",
       "              0.5953097858843142,\n",
       "              1.7637364187654787],\n",
       "             2008008180: [0.6945584416389465,\n",
       "              0.4263278219761741,\n",
       "              1.1208862636151207],\n",
       "             2008008184: [0.08877408504486084,\n",
       "              0.19041280903966987,\n",
       "              0.2791868940845307],\n",
       "             2008008193: [0.04197339341044426,\n",
       "              0.13444160335556338,\n",
       "              0.17641499676600764],\n",
       "             2008008194: [0.3355284631252289,\n",
       "              0.4299444900155048,\n",
       "              0.7654729531407336],\n",
       "             2008008197: [0.05344821512699127,\n",
       "              0.10698313487740385,\n",
       "              0.16043135000439512],\n",
       "             2008008199: [0.6770886778831482,\n",
       "              0.49408712528261617,\n",
       "              1.1711758031657644],\n",
       "             2008000008: [0.5426627993583679,\n",
       "              0.5744723069629813,\n",
       "              1.1171351063213493],\n",
       "             2008000015: [0.3625469207763672,\n",
       "              0.42465798165750307,\n",
       "              0.7872049024338703],\n",
       "             2008008211: [0.5651999115943909,\n",
       "              0.3031901688210842,\n",
       "              0.868390080415475],\n",
       "             2008008212: [0.05510924756526947,\n",
       "              0.20406137065890942,\n",
       "              0.25917061822417886],\n",
       "             2008000019: [0.4235990345478058,\n",
       "              0.5322842739555442,\n",
       "              0.95588330850335],\n",
       "             2008000023: [0.3725202977657318,\n",
       "              0.29664901215346356,\n",
       "              0.6691693099191953],\n",
       "             2008008215: [0.04058387875556946,\n",
       "              0.2447774850093527,\n",
       "              0.28536136376492216],\n",
       "             2008008218: [0.287617027759552,\n",
       "              0.4010630247231294,\n",
       "              0.6886800524826814],\n",
       "             2008000028: [0.04061040282249451,\n",
       "              0.190083780186731,\n",
       "              0.2306941830092255],\n",
       "             2008000033: [0.004452625755220652,\n",
       "              0.13637443725542067,\n",
       "              0.14082706301064132],\n",
       "             2008008227: [0.09794601798057556,\n",
       "              0.28514289714870367,\n",
       "              0.38308891512927923],\n",
       "             2008000036: [0.7131739854812622,\n",
       "              0.6545562349279376,\n",
       "              1.3677302204091997],\n",
       "             2008008229: [0.8220883011817932,\n",
       "              0.4425482450629032,\n",
       "              1.2646365462446965],\n",
       "             2008000037: [0.08785736560821533,\n",
       "              0.16831413377516286,\n",
       "              0.2561714993833782],\n",
       "             2008000041: [0.864475667476654,\n",
       "              0.6930610102832996,\n",
       "              1.5575366777599537],\n",
       "             2008008237: [0.18641580641269684,\n",
       "              0.32346767105549823,\n",
       "              0.5098834774681951],\n",
       "             2008008242: [0.05290181189775467,\n",
       "              0.14574675567122264,\n",
       "              0.1986485675689773],\n",
       "             2008000053: [0.002439195988699794,\n",
       "              0.06999834082587608,\n",
       "              0.07243753681457588],\n",
       "             2008008247: [0.5716954469680786,\n",
       "              0.31743890357470084,\n",
       "              0.8891343505427795],\n",
       "             2008000060: [0.006193073466420174,\n",
       "              0.16272151469202006,\n",
       "              0.16891458815844024],\n",
       "             2008000066: [0.01129448227584362,\n",
       "              0.09726092378038609,\n",
       "              0.10855540605622971],\n",
       "             2008008262: [0.6352150440216064,\n",
       "              0.483982905113219,\n",
       "              1.1191979491348254],\n",
       "             2008008263: [0.0461592935025692,\n",
       "              0.20921921284859382,\n",
       "              0.255378506351163],\n",
       "             2008000070: [0.03792640194296837,\n",
       "              0.22947159012658405,\n",
       "              0.2673979920695524],\n",
       "             2008008266: [0.7175650000572205,\n",
       "              0.4352537361279755,\n",
       "              1.152818736185196],\n",
       "             2008000074: [0.0006417312542907894,\n",
       "              0.07974712301649059,\n",
       "              0.08038885427078138],\n",
       "             2008008274: [0.06061533838510513,\n",
       "              0.2164797157208604,\n",
       "              0.2770950541059655],\n",
       "             2008008275: [0.3565795123577118,\n",
       "              0.439868245123202,\n",
       "              0.7964477574809138],\n",
       "             2008000085: [0.8167641758918762,\n",
       "              0.06450000517422937,\n",
       "              0.8812641810661056],\n",
       "             2008008281: [0.22905699908733368,\n",
       "              0.28746256719420943,\n",
       "              0.5165195662815432],\n",
       "             2008000089: [0.016176708042621613,\n",
       "              0.1455977451230651,\n",
       "              0.1617744531656867],\n",
       "             2008000093: [0.5388813018798828,\n",
       "              0.5518538557290552,\n",
       "              1.090735157608938],\n",
       "             2008000095: [0.13848769664764404,\n",
       "              0.2542792717116674,\n",
       "              0.39276696835931146],\n",
       "             2008000096: [0.1653192788362503,\n",
       "              0.36894230456007243,\n",
       "              0.5342615833963227],\n",
       "             2008008288: [0.38166752457618713,\n",
       "              0.42912356314117217,\n",
       "              0.8107910877173593],\n",
       "             2008008287: [1.102602481842041,\n",
       "              0.18615992723980526,\n",
       "              1.2887624090818464],\n",
       "             2008000097: [1.1838890314102173,\n",
       "              0.30823580266041706,\n",
       "              1.4921248340706343],\n",
       "             2008000099: [0.02520478330552578,\n",
       "              0.21632686422773897,\n",
       "              0.24153164753326475],\n",
       "             2008008294: [0.26284071803092957,\n",
       "              0.17929972384476545,\n",
       "              0.442140441875695],\n",
       "             2008000103: [0.3824816942214966,\n",
       "              0.23750372256035093,\n",
       "              0.6199854167818475],\n",
       "             2008000105: [0.02003936469554901,\n",
       "              0.17214850291622896,\n",
       "              0.19218786761177797],\n",
       "             2008008300: [0.35312706232070923,\n",
       "              0.42113857233974317,\n",
       "              0.7742656346604524],\n",
       "             2008000109: [0.2679575979709625,\n",
       "              0.40504263190443596,\n",
       "              0.6730002298753985],\n",
       "             2008000112: [0.08833209425210953,\n",
       "              0.17038232241258996,\n",
       "              0.2587144166646995],\n",
       "             2008008309: [0.01704096980392933,\n",
       "              0.14551807104628464,\n",
       "              0.16255904085021397],\n",
       "             2008008315: [0.43327298760414124,\n",
       "              0.7414157909708967,\n",
       "              1.174688778575038],\n",
       "             2008008319: [0.11214057356119156,\n",
       "              0.3536226550681014,\n",
       "              0.465763228629293],\n",
       "             2008000128: [0.863090455532074,\n",
       "              0.6033348656963387,\n",
       "              1.4664253212284126],\n",
       "             2008000131: [0.09458466619253159,\n",
       "              0.18881791010535373,\n",
       "              0.28340257629788534],\n",
       "             2008000132: [0.07574737071990967,\n",
       "              0.3184385691994294,\n",
       "              0.3941859399193391],\n",
       "             2008008325: [0.7302085161209106,\n",
       "              0.4125177899184647,\n",
       "              1.1427263060393753],\n",
       "             2008008324: [0.08072411268949509,\n",
       "              0.1300670122056717,\n",
       "              0.2107911248951668],\n",
       "             2008008330: [0.968510627746582,\n",
       "              0.5960873628206771,\n",
       "              1.5645979905672591],\n",
       "             2008000141: [0.4115729033946991,\n",
       "              0.3463208569858049,\n",
       "              0.757893760380504],\n",
       "             2008000142: [0.3810545802116394,\n",
       "              0.2623753838403826,\n",
       "              0.643429964052022],\n",
       "             2008000143: [0.9742022752761841,\n",
       "              0.563196107203583,\n",
       "              1.537398382479767],\n",
       "             2008000144: [0.25827643275260925,\n",
       "              0.5092917420079328,\n",
       "              0.7675681747605421],\n",
       "             2008008338: [0.7563444972038269,\n",
       "              0.32246842279359017,\n",
       "              1.078812919997417],\n",
       "             2008000148: [0.3699885606765747,\n",
       "              0.38061455049274573,\n",
       "              0.7506031111693204],\n",
       "             2008008343: [0.16602447628974915,\n",
       "              0.3459395483112133,\n",
       "              0.5119640246009625],\n",
       "             2008000151: [0.06832180917263031,\n",
       "              0.15042220461544342,\n",
       "              0.21874401378807373],\n",
       "             2008008344: [0.0935172289609909,\n",
       "              0.23690634432274615,\n",
       "              0.33042357328373706],\n",
       "             2008008345: [0.06951172649860382,\n",
       "              0.2888512170842629,\n",
       "              0.35836294358286674],\n",
       "             2008000154: [0.023527052253484726,\n",
       "              0.11832013553753304,\n",
       "              0.14184718779101776],\n",
       "             2008008347: [0.3535066843032837,\n",
       "              0.20173964402769362,\n",
       "              0.5552463283309773],\n",
       "             2008000162: [0.016750948503613472,\n",
       "              0.056471267324130776,\n",
       "              0.07322221582774424],\n",
       "             2010002177: [0.03945748135447502,\n",
       "              0.15248539100245545,\n",
       "              0.19194287235693047],\n",
       "             2008008356: [0.2928829789161682,\n",
       "              0.5527247946655194,\n",
       "              0.8456077735816876],\n",
       "             2008008363: [0.3875207006931305,\n",
       "              0.4821092450177985,\n",
       "              0.869629945710929],\n",
       "             2008008364: [0.6725842356681824,\n",
       "              0.48911105606314886,\n",
       "              1.1616952917313312],\n",
       "             2008008366: [0.1117699146270752,\n",
       "              0.21323887534729397,\n",
       "              0.32500878997436916],\n",
       "             2008000176: [0.7447588443756104,\n",
       "              0.5425670012881081,\n",
       "              1.2873258456637184],\n",
       "             2008008368: [0.5581845045089722,\n",
       "              0.4984400378967371,\n",
       "              1.0566245424057092],\n",
       "             2008008370: [0.17889410257339478,\n",
       "              0.3655955615397268,\n",
       "              0.5444896641131216],\n",
       "             2008000181: [0.0033344500698149204,\n",
       "              0.11729747078061989,\n",
       "              0.12063192085043481],\n",
       "             2010003301: [0.0016006999649107456,\n",
       "              0.11226134810428617,\n",
       "              0.11386204806919692],\n",
       "             2008000185: [0.600834310054779,\n",
       "              0.2425982724603484,\n",
       "              0.8434325825151274],\n",
       "             2008000187: [0.025623872876167297,\n",
       "              0.09066353284812534,\n",
       "              0.11628740572429264],\n",
       "             2008000189: [0.2895628809928894,\n",
       "              0.2207361761158136,\n",
       "              0.5102990571087029],\n",
       "             2008008382: [0.0730295330286026,\n",
       "              0.1615389906238091,\n",
       "              0.2345685236524117],\n",
       "             2008000191: [0.5734791159629822,\n",
       "              0.3723657983535702,\n",
       "              0.9458449143165524],\n",
       "             2008000192: [0.09521760791540146,\n",
       "              0.30395181696362206,\n",
       "              0.3991694248790235],\n",
       "             2008000193: [2.158942699432373,\n",
       "              0.7432902627699856,\n",
       "              2.9022329622023584],\n",
       "             2008008384: [0.2483476996421814,\n",
       "              0.23175611982447347,\n",
       "              0.48010381946665487],\n",
       "             2008000196: [1.5792531967163086,\n",
       "              0.5322517522212562,\n",
       "              2.111504948937565],\n",
       "             2008000197: [0.02465238980948925,\n",
       "              0.13740673744233337,\n",
       "              0.16205912725182262],\n",
       "             2008000199: [0.37534672021865845,\n",
       "              0.47095028664689664,\n",
       "              0.8462970068655551],\n",
       "             2008008391: [1.5049352645874023,\n",
       "              0.6825743728137437,\n",
       "              2.187509637401146],\n",
       "             2008000202: [0.37602829933166504,\n",
       "              0.4816170329182285,\n",
       "              0.8576453322498936],\n",
       "             2008000207: [0.0029016996268182993,\n",
       "              0.15664655333670272,\n",
       "              0.15954825296352101],\n",
       "             2008008402: [0.6754370927810669,\n",
       "              0.5621561112911386,\n",
       "              1.2375932040722055],\n",
       "             2008008403: [0.19397562742233276,\n",
       "              0.3188977923519234,\n",
       "              0.5128734197742562],\n",
       "             2008008404: [0.007374049164354801,\n",
       "              0.04403891909089116,\n",
       "              0.05141296825524596],\n",
       "             2008004752: [0.006994826253503561,\n",
       "              0.16042396619637908,\n",
       "              0.16741879244988264],\n",
       "             2008000217: [0.04002603888511658,\n",
       "              0.279252830437729,\n",
       "              0.3192788693228456],\n",
       "             2008008410: [0.37926313281059265,\n",
       "              0.5532753498893336,\n",
       "              0.9325384826999262],\n",
       "             2008008411: [0.2800831198692322,\n",
       "              0.26387006857513384,\n",
       "              0.543953188444366],\n",
       "             2008008416: [0.1203417032957077,\n",
       "              0.28538535309264923,\n",
       "              0.40572705638835693],\n",
       "             2008000226: [0.2215328961610794,\n",
       "              0.34720794349510486,\n",
       "              0.5687408396561843],\n",
       "             2008000227: [0.20732422173023224,\n",
       "              0.3204578335524228,\n",
       "              0.5277820552826551],\n",
       "             2008008423: [0.4216267466545105,\n",
       "              0.16306724636592412,\n",
       "              0.5846939930204347],\n",
       "             2008000235: [0.595744788646698,\n",
       "              0.7386642335363818,\n",
       "              1.3344090221830798],\n",
       "             2008000236: [0.005547432228922844,\n",
       "              0.17069531757428918,\n",
       "              0.17624274980321203],\n",
       "             2008000237: [0.5152489542961121,\n",
       "              0.43707629002183185,\n",
       "              0.952325244317944],\n",
       "             2008000238: [0.016570692881941795,\n",
       "              0.13481705034113284,\n",
       "              0.15138774322307463],\n",
       "             2008008431: [0.2960101366043091,\n",
       "              0.46700100661027516,\n",
       "              0.7630111432145843],\n",
       "             2008008432: [1.1925030946731567,\n",
       "              0.6225220023674373,\n",
       "              1.8150250970405941],\n",
       "             2008008428: [0.024009663611650467,\n",
       "              0.19685175083480685,\n",
       "              0.22086141444645732],\n",
       "             2008008440: [0.5674336552619934,\n",
       "              0.49068432771253023,\n",
       "              1.0581179829745238],\n",
       "             2008000252: [0.6637253761291504,\n",
       "              0.4217788613597767,\n",
       "              1.085504237488927],\n",
       "             2008000255: [0.025375619530677795,\n",
       "              0.15513667375045437,\n",
       "              0.18051229328113216],\n",
       "             2008008447: [0.8825570940971375,\n",
       "              0.3162102304965581,\n",
       "              1.1987673245936956],\n",
       "             2008000259: [0.4085758924484253,\n",
       "              0.4803593186246265,\n",
       "              0.8889352110730517],\n",
       "             2008000260: [1.162848711013794,\n",
       "              0.5221539269615096,\n",
       "              1.6850026379753036],\n",
       "             2008000262: [0.14257535338401794,\n",
       "              0.6222513047909873,\n",
       "              0.7648266581750053],\n",
       "             2008008455: [0.1610889434814453,\n",
       "              0.1607195527438946,\n",
       "              0.3218084962253399],\n",
       "             2008000266: [0.3140949606895447,\n",
       "              0.405359911352514,\n",
       "              0.7194548720420586],\n",
       "             2008008462: [0.016840266063809395,\n",
       "              0.20460724825045576,\n",
       "              0.22144751431426515],\n",
       "             2008008464: [0.07107789069414139,\n",
       "              0.06713978167499846,\n",
       "              0.13821767236913984],\n",
       "             2008000273: [0.4335445463657379,\n",
       "              0.6429634873557434,\n",
       "              1.0765080337214812],\n",
       "             2008000275: [0.9590246081352234,\n",
       "              0.4502469520366613,\n",
       "              1.4092715601718848],\n",
       "             2008008471: [0.01710735261440277,\n",
       "              0.17846140716492878,\n",
       "              0.19556875977933155],\n",
       "             2008000283: [0.30983448028564453,\n",
       "              0.3221154302812222,\n",
       "              0.6319499105668667],\n",
       "             2008000284: [0.04079943150281906,\n",
       "              0.25918489715690635,\n",
       "              0.2999843286597254],\n",
       "             2008008476: [0.01077424455434084,\n",
       "              0.1586779985577519,\n",
       "              0.16945224311209273],\n",
       "             2008008479: [0.4157843291759491,\n",
       "              0.26409491033043947,\n",
       "              0.6798792395063886],\n",
       "             2008008480: [1.184770941734314,\n",
       "              0.5680235459883675,\n",
       "              1.7527944877226815],\n",
       "             2008000289: [0.0014044941635802388,\n",
       "              0.09000587093362651,\n",
       "              0.09141036509720675],\n",
       "             2008000290: [0.28424736857414246,\n",
       "              0.2146106806196505,\n",
       "              0.49885804919379295],\n",
       "             2008000291: [0.2099650651216507,\n",
       "              0.2777626612048302,\n",
       "              0.48772772632648087],\n",
       "             2008008482: [0.031611260026693344,\n",
       "              0.1880129117263712,\n",
       "              0.21962417175306453],\n",
       "             2008008487: [0.8936764597892761,\n",
       "              0.5058757689635948,\n",
       "              1.3995522287528708],\n",
       "             2008000297: [0.5410043001174927,\n",
       "              0.6820996285417844,\n",
       "              1.223103928659277],\n",
       "             2008008490: [0.01009951252490282,\n",
       "              0.18045012867147828,\n",
       "              0.1905496411963811],\n",
       "             2008008496: [0.07214473932981491,\n",
       "              0.1617155972142373,\n",
       "              0.23386033654405222],\n",
       "             2008008497: [0.14211809635162354,\n",
       "              0.28613628375446754,\n",
       "              0.42825438010609107],\n",
       "             2008000311: [0.7059971690177917,\n",
       "              0.39332396767090644,\n",
       "              1.0993211366886981],\n",
       "             2008000313: [1.2395509481430054,\n",
       "              0.1580883487034825,\n",
       "              1.397639296846488],\n",
       "             2008000315: [0.698096513748169,\n",
       "              0.39517843315947204,\n",
       "              1.093274946907641],\n",
       "             2008000316: [0.056504104286432266,\n",
       "              0.26979059011946266,\n",
       "              0.3262946944058949],\n",
       "             2008008507: [0.0014902874827384949,\n",
       "              0.19566710599883297,\n",
       "              0.19715739348157146],\n",
       "             2008008508: [0.6416420936584473,\n",
       "              0.5143912519566872,\n",
       "              1.1560333456151346],\n",
       "             2008008511: [0.5705553293228149,\n",
       "              0.6465984639755028,\n",
       "              1.2171537932983179],\n",
       "             2008000318: [0.03240720182657242,\n",
       "              0.0739385750827734,\n",
       "              0.10634577690934582],\n",
       "             2009001972: [0.11089333146810532,\n",
       "              0.22084003575162378,\n",
       "              0.3317333672197291],\n",
       "             2008008517: [0.07947852462530136,\n",
       "              0.4214580416994881,\n",
       "              0.5009365663247894],\n",
       "             2008008521: [1.5842903852462769,\n",
       "              0.7612746470545613,\n",
       "              2.345565032300838],\n",
       "             2008000330: [0.46371209621429443,\n",
       "              0.2757117089658171,\n",
       "              0.7394238051801115],\n",
       "             2008008522: [0.25403308868408203,\n",
       "              0.39598921601411236,\n",
       "              0.6500223046981943],\n",
       "             2008008523: [0.9284946322441101,\n",
       "              0.5500774132950635,\n",
       "              1.4785720455391735],\n",
       "             2008008525: [0.032901931554079056,\n",
       "              0.2304536120335955,\n",
       "              0.26335554358767455],\n",
       "             2008008526: [0.25473448634147644,\n",
       "              0.3569195493870958,\n",
       "              0.6116540357285722],\n",
       "             2008000335: [0.5913840532302856,\n",
       "              0.46262800938498483,\n",
       "              1.0540120626152705],\n",
       "             2008008528: [0.5399120450019836,\n",
       "              0.5818531693503393,\n",
       "              1.121765214352323],\n",
       "             2008000336: [0.09114178270101547,\n",
       "              0.16105905579794896,\n",
       "              0.25220083849896446],\n",
       "             2008008530: [0.2587983310222626,\n",
       "              0.4018650073168012,\n",
       "              0.6606633383390638],\n",
       "             2008000338: [0.17111721634864807,\n",
       "              0.32349021376259846,\n",
       "              0.4946074301112465],\n",
       "             2008008533: [0.11172449588775635,\n",
       "              0.030765120106056594,\n",
       "              0.14248961599381293],\n",
       "             2008000342: [0.2237245887517929,\n",
       "              0.23617100981822098,\n",
       "              0.4598955985700139],\n",
       "             2008000343: [0.11096224933862686,\n",
       "              0.2559154761243521,\n",
       "              0.366877725462979],\n",
       "             2008000346: [0.16130220890045166,\n",
       "              0.2467657005455276,\n",
       "              0.40806790944597926],\n",
       "             2008008541: [0.10110767185688019,\n",
       "              0.27985246450330187,\n",
       "              0.38096013636018206],\n",
       "             2008000350: [0.8001044392585754,\n",
       "              0.35540525868917494,\n",
       "              1.1555096979477504],\n",
       "             2008008544: [0.000672127993311733,\n",
       "              0.03999090735897267,\n",
       "              0.0406630353522844],\n",
       "             2008008546: [0.1840885579586029,\n",
       "              0.18573280607336676,\n",
       "              0.36982136403196963],\n",
       "             2008008547: [1.3596060276031494,\n",
       "              0.5061297868008413,\n",
       "              1.8657358144039908],\n",
       "             2008000356: [0.15263685584068298,\n",
       "              0.15225359478771086,\n",
       "              0.3048904506283938],\n",
       "             2008008549: [1.0835320949554443,\n",
       "              0.6035639716363379,\n",
       "              1.6870960665917822],\n",
       "             2008008550: [0.5781024694442749,\n",
       "              0.5959558065903784,\n",
       "              1.1740582760346534],\n",
       "             2008000361: [0.008840725757181644,\n",
       "              0.13025856860232052,\n",
       "              0.13909929435950216],\n",
       "             2010001709: [0.19795994460582733,\n",
       "              0.2028161288545473,\n",
       "              0.40077607346037464],\n",
       "             2008000364: [0.05510260909795761,\n",
       "              0.1900111963634148,\n",
       "              0.2451138054613724],\n",
       "             2008000365: [0.04180922359228134,\n",
       "              0.13577374799393285,\n",
       "              0.1775829715862142],\n",
       "             2008000371: [0.9699115753173828,\n",
       "              0.43239288934836445,\n",
       "              1.4023044646657472],\n",
       "             2008008567: [0.10959456861019135,\n",
       "              0.32985798044070336,\n",
       "              0.4394525490508947],\n",
       "             2008000380: [0.1797211617231369,\n",
       "              0.34106359584565826,\n",
       "              0.5207847575687952],\n",
       "             2008008572: [0.3756697475910187,\n",
       "              0.4360839449275568,\n",
       "              0.8117536925185755],\n",
       "             2008008578: [0.2768693268299103,\n",
       "              0.287412423833965,\n",
       "              0.5642817506638753],\n",
       "             2008008579: [0.5073400139808655,\n",
       "              0.617154375235447,\n",
       "              1.1244943892163124],\n",
       "             2010003342: [0.0010633415076881647,\n",
       "              0.03870842116332481,\n",
       "              0.039771762671012977],\n",
       "             2008000392: [0.6394275426864624,\n",
       "              0.4155415577405048,\n",
       "              1.0549691004269672],\n",
       "             2008000393: [0.8124428987503052,\n",
       "              0.5690371440020775,\n",
       "              1.3814800427523828],\n",
       "             2008000397: [0.5556228756904602,\n",
       "              0.6619648445611591,\n",
       "              1.2175877202516192],\n",
       "             2008008589: [0.14925001561641693,\n",
       "              0.29070439397797326,\n",
       "              0.4399544095943902],\n",
       "             2008008591: [0.26400378346443176,\n",
       "              0.45840341046573246,\n",
       "              0.7224071939301642],\n",
       "             2008000399: [0.042589399963617325,\n",
       "              0.0895934552939659,\n",
       "              0.13218285525758322],\n",
       "             2008008593: [0.8213855624198914,\n",
       "              0.8411130293514129,\n",
       "              1.6624985917713042],\n",
       "             2008000400: [0.14547480642795563,\n",
       "              0.2706576897019214,\n",
       "              0.416132496129877],\n",
       "             2008008590: [0.534527063369751,\n",
       "              0.5441013668208129,\n",
       "              1.0786284301905638],\n",
       "             2010003345: [0.3417838513851166,\n",
       "              0.2796614031814673,\n",
       "              0.6214452545665838],\n",
       "             2008000405: [2.756993055343628,\n",
       "              0.6263180714101396,\n",
       "              3.3833111267537674],\n",
       "             2008005514: [0.04915398359298706,\n",
       "              0.09659336530566305,\n",
       "              0.1457473488986501],\n",
       "             2008008600: [0.20008805394172668,\n",
       "              0.39343159437520464,\n",
       "              0.5935196483169314],\n",
       "             2008000415: [0.2059013694524765,\n",
       "              0.263151878830867,\n",
       "              0.4690532482833435],\n",
       "             2008000416: [0.1910102516412735,\n",
       "              0.24166347044544326,\n",
       "              0.4326737220867167],\n",
       "             2008008607: [0.007239538710564375,\n",
       "              0.11113988341020758,\n",
       "              0.11837942212077196],\n",
       "             2008000421: [0.3586583137512207,\n",
       "              0.554016598599963,\n",
       "              0.9126749123511837],\n",
       "             2008000422: [0.008572935126721859,\n",
       "              0.08125578839798954,\n",
       "              0.0898287235247114],\n",
       "             2008008616: [0.3160055875778198,\n",
       "              0.3028351508639034,\n",
       "              0.6188407384417232],\n",
       "             2008000426: [0.04456887021660805,\n",
       "              0.16877407795847843,\n",
       "              0.21334294817508648],\n",
       "             2008008618: [0.09414256364107132,\n",
       "              0.2299138895712979,\n",
       "              0.3240564532123692],\n",
       "             2008000428: [0.044885072857141495,\n",
       "              0.13904344374667316,\n",
       "              0.18392851660381465],\n",
       "             2008000432: [0.3470245897769928,\n",
       "              0.3808271670786819,\n",
       "              0.7278517568556747],\n",
       "             2008008624: [0.1794542372226715,\n",
       "              0.5252834115403202,\n",
       "              0.7047376487629917],\n",
       "             2008000435: [0.05221236124634743,\n",
       "              0.22236701717956872,\n",
       "              0.2745793784259162],\n",
       "             2008000436: [0.1358804553747177,\n",
       "              0.2926440478653678,\n",
       "              0.4285245032400855],\n",
       "             2008000437: [0.5062608122825623,\n",
       "              0.6894750828327904,\n",
       "              1.1957358951153525],\n",
       "             2008000442: [0.09829512983560562,\n",
       "              0.139927686238421,\n",
       "              0.23822281607402662],\n",
       "             2008000443: [0.06560325622558594,\n",
       "              0.23511979557373483,\n",
       "              0.3007230517993208],\n",
       "             2008008635: [0.9963262677192688,\n",
       "              0.6381089686531586,\n",
       "              1.6344352363724273],\n",
       "             2008000445: [0.08309268206357956,\n",
       "              0.2653145802338227,\n",
       "              0.34840726229740226],\n",
       "             2008008637: [0.47229060530662537,\n",
       "              0.20233967283135446,\n",
       "              0.6746302781379798],\n",
       "             2008000447: [0.06656579673290253,\n",
       "              0.2979203001996846,\n",
       "              0.3644860969325871],\n",
       "             2008000448: [0.15642714500427246,\n",
       "              0.39849368370907484,\n",
       "              0.5549208287133474],\n",
       "             2008008641: [0.04501838609576225,\n",
       "              0.12083014971111733,\n",
       "              0.16584853580687958],\n",
       "             2008000455: [0.0024922271259129047,\n",
       "              0.051137828487254844,\n",
       "              0.05363005561316775],\n",
       "             2008008649: [0.2643481194972992,\n",
       "              0.3491733314548006,\n",
       "              0.6135214509520999],\n",
       "             2008000461: [0.7219551205635071,\n",
       "              0.5347017040319698,\n",
       "              1.2566568245954768],\n",
       "             2008008654: [0.34407609701156616,\n",
       "              0.32793425175308405,\n",
       "              0.6720103487646503],\n",
       "             2008000471: [0.996159553527832,\n",
       "              0.4526045205796994,\n",
       "              1.4487640741075314],\n",
       "             2008008671: [0.7047219276428223,\n",
       "              0.5330313637999071,\n",
       "              1.2377532914427294],\n",
       "             2008000480: [0.17532719671726227,\n",
       "              0.39804047382916713,\n",
       "              0.5733676705464295],\n",
       "             2008008674: [0.1602703183889389,\n",
       "              0.282075996848928,\n",
       "              0.4423463152378669],\n",
       "             2008000488: [0.009226617403328419,\n",
       "              0.15379149299814257,\n",
       "              0.163018110401471],\n",
       "             2008008681: [1.9861170053482056,\n",
       "              0.4064655012071215,\n",
       "              2.392582506555327],\n",
       "             2011001464: [0.04422827437520027,\n",
       "              0.18967939069443987,\n",
       "              0.23390766506964014],\n",
       "             2008000493: [0.8701473474502563,\n",
       "              0.5521613009651567,\n",
       "              1.422308648415413],\n",
       "             2008008685: [0.2887086272239685,\n",
       "              0.5914023375557087,\n",
       "              0.8801109647796772],\n",
       "             2008000495: [0.3968571424484253,\n",
       "              0.4940415567975182,\n",
       "              0.8908986992459436],\n",
       "             2008000499: [0.31826701760292053,\n",
       "              0.46206048988672743,\n",
       "              0.780327507489648],\n",
       "             2008008691: [0.153049036860466,\n",
       "              0.29638513847169756,\n",
       "              0.44943417533216357],\n",
       "             2008008694: [0.15652909874916077,\n",
       "              0.24875253458198754,\n",
       "              0.4052816333311483],\n",
       "             2008000502: [0.41348037123680115,\n",
       "              0.5249935885563419,\n",
       "              0.9384739597931431],\n",
       "             2008008697: [0.08245027810335159,\n",
       "              0.27389650175928176,\n",
       "              0.35634677986263336],\n",
       "             2008000505: [0.003982055466622114,\n",
       "              0.07634762802122701,\n",
       "              0.08032968348784912],\n",
       "             2008008701: [0.32205232977867126,\n",
       "              0.25021709887664867,\n",
       "              0.5722694286553199],\n",
       "             2008000512: [0.31828010082244873,\n",
       "              0.378781162720877,\n",
       "              0.6970612635433258],\n",
       "             2008000514: [0.050235550850629807,\n",
       "              0.13470011698341272,\n",
       "              0.18493566783404253],\n",
       "             2008008706: [0.13788971304893494,\n",
       "              0.2001669694340995,\n",
       "              0.33805668248303444],\n",
       "             2008008707: [0.3745571970939636,\n",
       "              0.3595783078189797,\n",
       "              0.7341355049129433],\n",
       "             2008000515: [0.6840477585792542,\n",
       "              0.3060837827399809,\n",
       "              0.9901315413192351],\n",
       "             2008008714: [0.4965711236000061,\n",
       "              0.5023977960632662,\n",
       "              0.9989689196632723],\n",
       "             2008008717: [0.3793473541736603,\n",
       "              0.5505935042807357,\n",
       "              0.929940858454396],\n",
       "             2008000527: [0.04130830615758896,\n",
       "              0.24046435786978332,\n",
       "              0.2817726640273723],\n",
       "             2008008719: [0.6067085862159729,\n",
       "              0.28613219009379004,\n",
       "              0.892840776309763],\n",
       "             2008000531: [0.09996944665908813,\n",
       "              0.2816545928049234,\n",
       "              0.3816240394640115],\n",
       "             2008008725: [0.18243354558944702,\n",
       "              0.1903441422527685,\n",
       "              0.37277768784221554],\n",
       "             2008000540: [0.0188586562871933,\n",
       "              0.21002945458195713,\n",
       "              0.22888811086915042],\n",
       "             2008000544: [0.02707543969154358,\n",
       "              0.24508909280484054,\n",
       "              0.2721645324963841],\n",
       "             2008000545: [0.11460817605257034,\n",
       "              0.43864450206905914,\n",
       "              0.5532526781216295],\n",
       "             2008000548: [0.3741239309310913,\n",
       "              0.40794256293784475,\n",
       "              0.7820664938689361],\n",
       "             2008000552: [0.13478577136993408,\n",
       "              0.3245971480917899,\n",
       "              0.459382919461724],\n",
       "             2008008744: [0.36037760972976685,\n",
       "              0.17637937791783645,\n",
       "              0.5367569876476033],\n",
       "             2008008745: [0.6194797158241272,\n",
       "              0.5030305565822413,\n",
       "              1.1225102724063685],\n",
       "             2008008748: [0.00808286014944315,\n",
       "              0.03748426842702758,\n",
       "              0.04556712857647073],\n",
       "             2008000559: [0.00011199451546417549,\n",
       "              0.012488000050020298,\n",
       "              0.012599994565484473],\n",
       "             2008000561: [0.4099252223968506,\n",
       "              0.5757570272457295,\n",
       "              0.9856822496425801],\n",
       "             2008000563: [1.186701774597168,\n",
       "              0.5202407787851502,\n",
       "              1.7069425533823182],\n",
       "             2008008757: [0.8140897154808044,\n",
       "              0.6256760547662594,\n",
       "              1.4397657702470639],\n",
       "             2008000567: [0.06989146023988724,\n",
       "              0.34446904873693684,\n",
       "              0.4143605089768241],\n",
       "             2008000572: [0.22968068718910217,\n",
       "              0.2823252206684618,\n",
       "              0.512005907857564],\n",
       "             2008000578: [1.6717889308929443,\n",
       "              0.32112034799632216,\n",
       "              1.9929092788892664],\n",
       "             2008008773: [1.133541226387024,\n",
       "              0.5842617042069326,\n",
       "              1.7178029305939564],\n",
       "             2009000006: [0.036032240837812424,\n",
       "              0.14430821265063346,\n",
       "              0.18034045348844588],\n",
       "             2008000583: [1.1285150051116943,\n",
       "              0.49449116935994114,\n",
       "              1.6230061744716355],\n",
       "             2008000584: [0.08179688453674316,\n",
       "              0.24278073843927928,\n",
       "              0.32457762297602244],\n",
       "             2008000585: [0.018363866955041885,\n",
       "              0.11541036400537004,\n",
       "              0.1337742309604119],\n",
       "             2009000010: [0.31809142231941223,\n",
       "              0.22608346393053363,\n",
       "              0.5441748862499458],\n",
       "             2008000588: [0.7394327521324158,\n",
       "              0.6376183582961717,\n",
       "              1.3770511104285874],\n",
       "             2009000014: [0.02329547517001629,\n",
       "              0.2697209794607424,\n",
       "              0.2930164546307587],\n",
       "             2009000015: [0.2517765462398529,\n",
       "              0.2292908508117373,\n",
       "              0.4810673970515902],\n",
       "             2009000016: [0.1412864625453949,\n",
       "              0.2740771353697598,\n",
       "              0.4153635979151547],\n",
       "             2008000595: [0.00036022704443894327,\n",
       "              0.06111804459099679,\n",
       "              0.06147827163543573],\n",
       "             2009000021: [0.5166383981704712,\n",
       "              0.4826884258163119,\n",
       "              0.9993268239867831],\n",
       "             2009000027: [0.5923666954040527,\n",
       "              0.389339362571579,\n",
       "              0.9817060579756317],\n",
       "             2009000028: [0.09018275886774063,\n",
       "              0.17072820412950349,\n",
       "              0.26091096299724414],\n",
       "             2009000029: [0.010795325972139835,\n",
       "              0.09033608312284697,\n",
       "              0.1011314090949868],\n",
       "             2009000030: [0.00013369685621000826,\n",
       "              0.02351626462816965,\n",
       "              0.02364996148437966],\n",
       "             2008000607: [0.4556621313095093,\n",
       "              0.09759976587629136,\n",
       "              0.5532618971858007],\n",
       "             2008000613: [0.2237975150346756,\n",
       "              0.359006404955134,\n",
       "              0.5828039199898096],\n",
       "             2008000615: [0.5076860785484314,\n",
       "              0.507314191020325,\n",
       "              1.0150002695687563],\n",
       "             2009000040: [0.06317673623561859,\n",
       "              0.3167130068038902,\n",
       "              0.3798897430395088],\n",
       "             2009000042: [0.34237512946128845,\n",
       "              0.12151700787760689,\n",
       "              0.46389213733889534],\n",
       "             2008000619: [0.968724250793457,\n",
       "              0.3690643387475615,\n",
       "              1.3377885895410184],\n",
       "             2008000626: [1.8105521202087402,\n",
       "              0.5310395058336744,\n",
       "              2.3415916260424146],\n",
       "             2008000628: [0.2166416198015213,\n",
       "              0.303938346465394,\n",
       "              0.5205799662669153],\n",
       "             2009000052: [0.007696505170315504,\n",
       "              0.1150751872141482,\n",
       "              0.1227716923844637],\n",
       "             2009000054: [0.7098702788352966,\n",
       "              0.31332104180057535,\n",
       "              1.023191320635872],\n",
       "             2009000056: [0.525463342666626,\n",
       "              0.47096046962269195,\n",
       "              0.9964238122893179],\n",
       "             2009000058: [0.903024435043335,\n",
       "              0.42611919466115294,\n",
       "              1.3291436297044879],\n",
       "             2009000059: [0.12042156606912613,\n",
       "              0.30920037928215777,\n",
       "              0.4296219453512839],\n",
       "             2008000636: [0.23333534598350525,\n",
       "              0.33073433285746867,\n",
       "              0.5640696788409739],\n",
       "             2008000641: [0.3709520995616913,\n",
       "              0.3729855407722077,\n",
       "              0.743937640333899],\n",
       "             2008000645: [0.05605008453130722,\n",
       "              0.1664049989304067,\n",
       "              0.22245508346171392],\n",
       "             2008000646: [0.1533709019422531,\n",
       "              0.12396949248273133,\n",
       "              0.2773403944249844],\n",
       "             2008000648: [0.061175838112831116,\n",
       "              0.17490794926101277,\n",
       "              0.2360837873738439],\n",
       "             2008000650: [0.05757622793316841,\n",
       "              0.0994260164851656,\n",
       "              0.157002244418334],\n",
       "             2008000655: [0.08220583945512772,\n",
       "              0.26818374047750076,\n",
       "              0.3503895799326285],\n",
       "             2009000082: [0.014092219062149525,\n",
       "              0.16485795874734024,\n",
       "              0.17895017780948977],\n",
       "             2009000085: [0.6577007174491882,\n",
       "              0.3976625784443193,\n",
       "              1.0553632958935075],\n",
       "             2009000088: [0.02879283018410206,\n",
       "              0.16851271986455835,\n",
       "              0.1973055500486604],\n",
       "             2009000091: [0.2505798637866974,\n",
       "              0.21690336741290936,\n",
       "              0.4674832311996068],\n",
       "             2008000672: [0.024652205407619476,\n",
       "              0.08871204904041108,\n",
       "              0.11336425444803055],\n",
       "             2008000674: [0.13052724301815033,\n",
       "              0.570623032031509,\n",
       "              0.7011502750496593],\n",
       "             2008000676: [0.5733082890510559,\n",
       "              0.510697940227377,\n",
       "              1.084006229278433],\n",
       "             2009000100: [0.25779709219932556,\n",
       "              0.2474575326104944,\n",
       "              0.50525462480982],\n",
       "             2008000678: [0.16113604605197906,\n",
       "              0.06954226432660167,\n",
       "              0.23067831037858072],\n",
       "             2009000103: [0.5142459273338318,\n",
       "              0.46693225476448913,\n",
       "              0.981178182098321],\n",
       "             2009000104: [0.5252000689506531,\n",
       "              0.16308680559280866,\n",
       "              0.6882868745434617],\n",
       "             2009000105: [0.36076533794403076,\n",
       "              0.5370016862209666,\n",
       "              0.8977670241649973],\n",
       "             2008000683: [0.2895089089870453,\n",
       "              0.35670492508897855,\n",
       "              0.6462138340760238],\n",
       "             2009000109: [0.5296851396560669,\n",
       "              0.4877703149669033,\n",
       "              1.0174554546229702],\n",
       "             2008000689: [0.819885790348053,\n",
       "              0.6686739038831139,\n",
       "              1.4885596942311667],\n",
       "             2008000694: [0.6927633285522461,\n",
       "              0.4687523750381445,\n",
       "              1.1615157035903905],\n",
       "             2009000119: [0.43507853150367737,\n",
       "              0.4248154812344183,\n",
       "              0.8598940127380956],\n",
       "             2009000120: [0.06823413074016571,\n",
       "              0.24285163135784454,\n",
       "              0.31108576209801025],\n",
       "             2008000696: [0.2132018804550171,\n",
       "              0.27294940345053387,\n",
       "              0.48615128390555096],\n",
       "             2009000122: [0.016360217705368996,\n",
       "              0.2868096850232624,\n",
       "              0.30316990272863137],\n",
       "             2008000703: [0.026543281972408295,\n",
       "              0.2643219734406408,\n",
       "              0.2908652554130491],\n",
       "             2008000704: [0.5074626207351685,\n",
       "              0.411954385967992,\n",
       "              0.9194170067031604],\n",
       "             2009000128: [0.7800717353820801,\n",
       "              0.3589393988361089,\n",
       "              1.139011134218189],\n",
       "             2009000130: [0.03432223200798035,\n",
       "              0.17917874365981593,\n",
       "              0.21350097566779627],\n",
       "             2009000131: [0.534399688243866,\n",
       "              0.36433904750969226,\n",
       "              0.8987387357535582],\n",
       "             2009000132: [1.0637428760528564,\n",
       "              0.1999562197778149,\n",
       "              1.2636990958306713],\n",
       "             2009000133: [0.07192974537611008,\n",
       "              0.30930002002123813,\n",
       "              0.3812297653973482],\n",
       "             2009000135: [0.036525752395391464,\n",
       "              0.10794244110439506,\n",
       "              0.1444681934997865],\n",
       "             2008000711: [0.10323730111122131,\n",
       "              0.28607322996082407,\n",
       "              0.3893105310720454],\n",
       "             2009000140: [1.1003386974334717,\n",
       "              0.5137626350772445,\n",
       "              1.614101332510716],\n",
       "             2009000141: [0.2593291997909546,\n",
       "              0.32682136557495955,\n",
       "              0.5861505653659141],\n",
       "             2008000716: [0.09943833202123642,\n",
       "              0.13404340854111021,\n",
       "              0.23348174056234663],\n",
       "             2008000719: [0.648280143737793,\n",
       "              0.47406328490478683,\n",
       "              1.1223434286425797],\n",
       "             2009000145: [0.588412344455719,\n",
       "              0.5589994881237725,\n",
       "              1.1474118325794915],\n",
       "             2008000721: [0.06810028105974197,\n",
       "              0.12188768106335535,\n",
       "              0.18998796212309732],\n",
       "             2008000723: [0.016736861318349838,\n",
       "              0.16122435736681473,\n",
       "              0.17796121868516457],\n",
       "             2008000724: [0.0007786484202370048,\n",
       "              0.038383480558224076,\n",
       "              0.03916212897846108],\n",
       "             2008000726: [0.04599134624004364,\n",
       "              0.2807525934925076,\n",
       "              0.3267439397325512],\n",
       "             2009000151: [0.926954448223114,\n",
       "              0.39125590651860137,\n",
       "              1.3182103547417154],\n",
       "             2009000150: [0.042328447103500366,\n",
       "              0.11624938713707422,\n",
       "              0.1585778342405746],\n",
       "             2008000729: [0.03817753866314888,\n",
       "              0.24058684803460856,\n",
       "              0.27876438669775744],\n",
       "             2008000732: [0.3534483015537262,\n",
       "              0.3756205517429922,\n",
       "              0.7290688532967184],\n",
       "             2008000733: [0.1457008719444275,\n",
       "              0.1972318835668234,\n",
       "              0.3429327555112509],\n",
       "             2009000159: [0.15525124967098236,\n",
       "              0.22065979715734438,\n",
       "              0.37591104682832677],\n",
       "             2009000160: [1.5364824533462524,\n",
       "              0.5636228198948957,\n",
       "              2.100105273241148],\n",
       "             2009000161: [0.011209165677428246,\n",
       "              0.11750580500287461,\n",
       "              0.12871497068030285],\n",
       "             2009000164: [1.2566403150558472,\n",
       "              0.7144582237714329,\n",
       "              1.9710985388272801],\n",
       "             2008000742: [0.19977551698684692,\n",
       "              0.3414677171287457,\n",
       "              0.5412432341155926],\n",
       "             2009000168: [0.20494060218334198,\n",
       "              0.11341225340595981,\n",
       "              0.3183528555893018],\n",
       "             2009000176: [0.6901445984840393,\n",
       "              0.6210482795343049,\n",
       "              1.3111928780183442],\n",
       "             2008000753: [0.15463057160377502,\n",
       "              0.22952760333460925,\n",
       "              0.3841581749383843],\n",
       "             2008000756: [0.09794759750366211,\n",
       "              0.2721009010532473,\n",
       "              0.3700484985569094],\n",
       "             2008000758: [0.21966658532619476,\n",
       "              0.3621480829752709,\n",
       "              0.5818146683014657],\n",
       "             2008000760: [0.8729180097579956,\n",
       "              0.607454801377288,\n",
       "              1.4803728111352836],\n",
       "             2008000761: [0.02804824709892273,\n",
       "              0.06607088404742559,\n",
       "              0.09411913114634832],\n",
       "             2008000764: [0.30212751030921936,\n",
       "              0.3595870837220831,\n",
       "              0.6617145940313025],\n",
       "             2009000188: [0.2761271893978119,\n",
       "              0.49617359317802456,\n",
       "              0.7723007825758365],\n",
       "             2009000195: [0.10962101817131042,\n",
       "              0.17754997389843033,\n",
       "              0.2871709920697407],\n",
       "             2009000197: [1.1290310621261597,\n",
       "              0.6616023906561783,\n",
       "              1.790633452782338],\n",
       "             2008000775: [0.25394296646118164,\n",
       "              0.4256533717718688,\n",
       "              0.6795963382330504],\n",
       "             2008000777: [0.006195694673806429,\n",
       "              0.10467278464000189,\n",
       "              0.11086847931380832],\n",
       "             2008000778: [0.3642037510871887,\n",
       "              0.24654445095641273,\n",
       "              0.6107482020436015],\n",
       "             2009000203: [0.040466006845235825,\n",
       "              0.09962996099729798,\n",
       "              0.1400959678425338],\n",
       "             2009000209: [0.06992531567811966,\n",
       "              0.2592444578719868,\n",
       "              0.32916977355010646],\n",
       "             2008000785: [1.1609582901000977,\n",
       "              0.5731309550681165,\n",
       "              1.7340892451682142],\n",
       "             2008000787: [0.45447781682014465,\n",
       "              0.33029823765405003,\n",
       "              0.7847760544741946],\n",
       "             2008000790: [1.6195588111877441,\n",
       "              0.7752926642327403,\n",
       "              2.3948514754204844],\n",
       "             2008000792: [0.3033553957939148,\n",
       "              0.3915715561203223,\n",
       "              0.6949269519142371],\n",
       "             2009000218: [0.4725766181945801,\n",
       "              0.4622260319845638,\n",
       "              0.9348026501791439],\n",
       "             2008000798: [1.961769938468933,\n",
       "              0.7574239537730509,\n",
       "              2.719193892241984],\n",
       "             2009000223: [1.58595609664917,\n",
       "              0.6107024592283671,\n",
       "              2.196658555877537],\n",
       "             2008000801: [0.6036200523376465,\n",
       "              0.5567751516665426,\n",
       "              1.160395204004189],\n",
       "             2009000227: [0.04382336884737015,\n",
       "              0.08228040640308369,\n",
       "              0.12610377525045385],\n",
       "             2009000229: [0.19063256680965424,\n",
       "              0.20592439047094646,\n",
       "              0.3965569572806007],\n",
       "             2010001797: [0.0029050528537482023,\n",
       "              0.029802156688529754,\n",
       "              0.03270720954227796],\n",
       "             2008000808: [0.33427202701568604,\n",
       "              0.2743750373870245,\n",
       "              0.6086470644027105],\n",
       "             2009000233: [0.03908723220229149,\n",
       "              0.07530010663598945,\n",
       "              0.11438733883828094],\n",
       "             2009000232: [0.43038585782051086,\n",
       "              0.3230411147856411,\n",
       "              0.753426972606152],\n",
       "             2009000237: [0.48956194519996643,\n",
       "              0.4943038436951812,\n",
       "              0.9838657888951476],\n",
       "             2008000814: [0.4117041528224945,\n",
       "              0.6534195778972528,\n",
       "              1.0651237307197472],\n",
       "             2008000815: [0.6312203407287598,\n",
       "              0.33355474682913633,\n",
       "              0.9647750875578961],\n",
       "             2009000239: [2.3145573139190674,\n",
       "              0.6302047997577264,\n",
       "              2.9447621136767936],\n",
       "             2009000248: [0.19519267976284027,\n",
       "              0.3252233792978986,\n",
       "              0.5204160590607388],\n",
       "             2008000824: [0.2932477593421936,\n",
       "              0.3377483952796872,\n",
       "              0.6309961546218807],\n",
       "             2009000250: [0.0644921362400055,\n",
       "              0.07611062696860901,\n",
       "              0.1406027632086145],\n",
       "             2009000251: [0.14855767786502838,\n",
       "              0.27875071828373116,\n",
       "              0.42730839614875954],\n",
       "             2008000829: [0.20077289640903473,\n",
       "              0.37754431218919254,\n",
       "              0.5783172085982273],\n",
       "             2009000253: [0.028671706095337868,\n",
       "              0.11504521100225197,\n",
       "              0.14371691709758982],\n",
       "             2008000832: [0.010850992053747177,\n",
       "              0.13818771727351595,\n",
       "              0.14903870932726312],\n",
       "             2008000833: [0.32367709279060364,\n",
       "              0.42039703571049797,\n",
       "              0.7440741285011017],\n",
       "             2008000841: [0.19411839544773102,\n",
       "              0.36761750688202977,\n",
       "              0.5617359023297608],\n",
       "             2008000842: [0.008852174505591393,\n",
       "              0.21604069669087045,\n",
       "              0.22489287119646184],\n",
       "             2008000844: [0.046785835176706314,\n",
       "              0.3371001670528567,\n",
       "              0.38388600222956304],\n",
       "             2009000268: [0.01868094690144062,\n",
       "              0.09462381093224236,\n",
       "              0.11330475783368298],\n",
       "             2008000847: [0.7521953582763672,\n",
       "              0.3772790499186288,\n",
       "              1.1294744081949961],\n",
       "             2008003975: [0.10542496293783188,\n",
       "              0.10618480323334119,\n",
       "              0.21160976617117305],\n",
       "             2008000851: [0.3921683430671692,\n",
       "              0.3114012968374302,\n",
       "              0.7035696399045994],\n",
       "             2009000277: [0.8706573843955994,\n",
       "              0.44392633210262733,\n",
       "              1.3145837164982268],\n",
       "             2008000854: [0.1161520779132843,\n",
       "              0.41163957704034837,\n",
       "              0.5277916549536327],\n",
       "             2009000280: [0.5474718809127808,\n",
       "              0.2613051574299351,\n",
       "              0.8087770383427159],\n",
       "             2009000281: [0.6780602335929871,\n",
       "              0.5249036076125814,\n",
       "              1.2029638412055683],\n",
       "             2008000860: [0.0016687167808413506, 0.0, 0.0016687167808413506],\n",
       "             2008000861: [1.3596625328063965,\n",
       "              0.6793112064126092,\n",
       "              2.0389737392190055],\n",
       "             2009000285: [0.25156840682029724,\n",
       "              0.26417146829530413,\n",
       "              0.5157398751156014],\n",
       "             2009000287: [0.03572872653603554,\n",
       "              0.05085400509373496,\n",
       "              0.0865827316297705],\n",
       "             2011001537: [0.047864459455013275,\n",
       "              0.271117910065858,\n",
       "              0.3189823695208713],\n",
       "             2009000290: [0.5903723835945129,\n",
       "              0.3265302542326847,\n",
       "              0.9169026378271976],\n",
       "             2008000867: [0.319068044424057,\n",
       "              0.31674428196102933,\n",
       "              0.6358123263850863],\n",
       "             2009003708: [0.002177298767492175,\n",
       "              0.08234782679250484,\n",
       "              0.08452512555999701],\n",
       "             2008000870: [0.8435390591621399,\n",
       "              0.5879119810189722,\n",
       "              1.4314510401811122],\n",
       "             2008000873: [0.9864612221717834,\n",
       "              0.5543755610573871,\n",
       "              1.5408367832291705],\n",
       "             2008000875: [0.11873243749141693,\n",
       "              0.32004950356475687,\n",
       "              0.4387819410561738],\n",
       "             2009000303: [0.04213948920369148,\n",
       "              0.10415078632128283,\n",
       "              0.1462902755249743],\n",
       "             2008000881: [0.3077286183834076,\n",
       "              0.49728709141059657,\n",
       "              0.8050157097940042],\n",
       "             2008000883: [0.8440549373626709,\n",
       "              0.039136983608132205,\n",
       "              0.8831919209708031],\n",
       "             2008000887: [0.011559255421161652,\n",
       "              0.22206825313810732,\n",
       "              0.23362750855926898],\n",
       "             2009000317: [0.30485716462135315,\n",
       "              0.3166740165859235,\n",
       "              0.6215311812072766],\n",
       "             2009000320: [0.4443315267562866,\n",
       "              0.5846086066736496,\n",
       "              1.0289401334299362],\n",
       "             2009000322: [0.0008528309990651906,\n",
       "              0.06817492257703192,\n",
       "              0.06902775357609711],\n",
       "             2008000899: [0.0899585634469986,\n",
       "              0.2260894919488904,\n",
       "              0.316048055395889],\n",
       "             2008000901: [0.00832674466073513,\n",
       "              0.1591954839359619,\n",
       "              0.16752222859669702],\n",
       "             2008000902: [0.5231418609619141,\n",
       "              0.27730966864345363,\n",
       "              0.8004515296053677],\n",
       "             2009000327: [0.16663332283496857,\n",
       "              0.3016001484820608,\n",
       "              0.46823347131702936],\n",
       "             2008000905: [0.024956723675131798,\n",
       "              0.1663221510480133,\n",
       "              0.1912788747231451],\n",
       "             2008000908: [0.08470447361469269,\n",
       "              0.2642459697316297,\n",
       "              0.3489504433463224],\n",
       "             2009003717: [0.07484015822410583,\n",
       "              0.1640653346683989,\n",
       "              0.23890549289250473],\n",
       "             2008000912: [0.07523886114358902,\n",
       "              0.22096546320645308,\n",
       "              0.29620432435004207],\n",
       "             2008000914: [0.05441096052527428,\n",
       "              0.2554443058958712,\n",
       "              0.30985526642114547],\n",
       "             2009000339: [0.05175888165831566,\n",
       "              0.12041299203413008,\n",
       "              0.17217187369244574],\n",
       "             2009000340: [0.18814650177955627,\n",
       "              0.4230022787867327,\n",
       "              0.611148780566289],\n",
       "             2009000341: [1.0176448822021484,\n",
       "              0.49712973872475275,\n",
       "              1.514774620926901],\n",
       "             2009000343: [0.5880259871482849,\n",
       "              0.5571067220423677,\n",
       "              1.1451327091906527],\n",
       "             2009000344: [0.18735913932323456,\n",
       "              0.11907394086494706,\n",
       "              0.3064330801881816],\n",
       "             2008000924: [0.18962717056274414,\n",
       "              0.42255165708599485,\n",
       "              0.612178827648739],\n",
       "             2009000350: [0.0002481565170455724,\n",
       "              0.048436249622900295,\n",
       "              0.04868440613994587],\n",
       "             2008000928: [0.5668401718139648,\n",
       "              0.33684319779499733,\n",
       "              0.9036833696089621],\n",
       "             2009002093: [0.40488019585609436,\n",
       "              0.2913152535260426,\n",
       "              0.696195449382137],\n",
       "             2008000934: [0.2722485065460205,\n",
       "              0.3138068781251049,\n",
       "              0.5860553846711254],\n",
       "             2008000941: [0.14795129001140594,\n",
       "              0.331249710776705,\n",
       "              0.47920100078811095],\n",
       "             2009000367: [0.6479730606079102,\n",
       "              0.4895030123461512,\n",
       "              1.1374760729540614],\n",
       "             2008000944: [0.8417762517929077,\n",
       "              0.6160558343256283,\n",
       "              1.457832086118536],\n",
       "             2009000375: [0.9853478670120239,\n",
       "              0.6057942415308328,\n",
       "              1.5911421085428568],\n",
       "             2008000953: [1.208646297454834,\n",
       "              0.6597193053975337,\n",
       "              1.8683656028523676],\n",
       "             2009000377: [0.4727606475353241,\n",
       "              0.160416814596001,\n",
       "              0.6331774621313251],\n",
       "             2009000379: [0.12806819379329681,\n",
       "              0.41272775824052527,\n",
       "              0.5407959520338221],\n",
       "             2008000959: [0.13645800948143005,\n",
       "              0.19075281870007138,\n",
       "              0.32721082818150143],\n",
       "             2009000385: [0.022189831361174583,\n",
       "              0.16077647689422916,\n",
       "              0.18296630825540375],\n",
       "             2009000390: [0.2804107069969177,\n",
       "              0.14714743126765617,\n",
       "              0.4275581382645739],\n",
       "             2009000393: [0.32362067699432373,\n",
       "              0.24642157249615887,\n",
       "              0.5700422494904827],\n",
       "             2008000970: [0.44921252131462097,\n",
       "              0.6202178287315124,\n",
       "              1.0694303500461335],\n",
       "             2008000973: [0.0006830552592873573,\n",
       "              0.17627161660975513,\n",
       "              0.17695467186904248],\n",
       "             2009000400: [1.9259556531906128,\n",
       "              0.7778708754323563,\n",
       "              2.7038265286229692],\n",
       "             2008000979: [0.5457937717437744,\n",
       "              0.44047555568574603,\n",
       "              0.9862693274295204],\n",
       "             2008000981: [1.10127854347229,\n",
       "              0.49289606973005434,\n",
       "              1.5941746132023444],\n",
       "             2009000405: [0.22936423122882843,\n",
       "              0.2147265319986369,\n",
       "              0.4440907632274653],\n",
       "             2009000408: [0.30046936869621277,\n",
       "              0.3864424799774662,\n",
       "              0.6869118486736789],\n",
       "             2008000985: [0.1427803486585617,\n",
       "              0.20677981275964247,\n",
       "              0.3495601614182042],\n",
       "             2009000409: [0.15871474146842957,\n",
       "              0.3814144339523448,\n",
       "              0.5401291754207744],\n",
       "             2008000987: [0.32423803210258484,\n",
       "              0.3771728314024529,\n",
       "              0.7014108635050378],\n",
       "             2009000416: [0.03162279352545738,\n",
       "              0.11156924253538116,\n",
       "              0.14319203606083852],\n",
       "             2009000420: [0.009600743651390076,\n",
       "              0.16886799438201935,\n",
       "              0.17846873803340943],\n",
       "             2008000999: [0.025223836302757263,\n",
       "              0.12568724927098338,\n",
       "              0.15091108557374064],\n",
       "             2009000438: [0.02734149806201458,\n",
       "              0.23602375118074348,\n",
       "              0.26336524924275806],\n",
       "             2009000439: [0.01807417720556259,\n",
       "              0.24228796406785388,\n",
       "              0.2603621412734165],\n",
       "             2008001018: [0.3799501657485962,\n",
       "              0.3246248086869855,\n",
       "              0.7045749744355817],\n",
       "             2008001020: [1.2723253965377808,\n",
       "              0.5759474156057082,\n",
       "              1.848272812143489],\n",
       "             2008001021: [0.28917890787124634,\n",
       "              0.4235399107716143,\n",
       "              0.7127188186428606],\n",
       "             2008001022: [0.335533082485199,\n",
       "              0.28929173922560114,\n",
       "              0.6248248217108001],\n",
       "             2008001023: [0.12820793688297272,\n",
       "              0.30595095407499356,\n",
       "              0.4341588909579663],\n",
       "             2009000445: [0.053329333662986755,\n",
       "              0.1338101862602891,\n",
       "              0.18713951992327585],\n",
       "             2009000449: [0.6899758577346802,\n",
       "              0.40602814744913635,\n",
       "              1.0960040051838165],\n",
       "             2008001026: [0.017327619716525078,\n",
       "              0.1368194567002151,\n",
       "              0.15414707641674016],\n",
       "             2009000452: [0.781548023223877,\n",
       "              0.2328185420354326,\n",
       "              1.0143665652593095],\n",
       "             2009000454: [0.0911727175116539,\n",
       "              0.16187088247139408,\n",
       "              0.253043599983048],\n",
       "             2008001031: [0.2407871037721634,\n",
       "              0.5568180865496326,\n",
       "              0.797605190321796],\n",
       "             2008001035: [0.30240392684936523,\n",
       "              0.44426371858424846,\n",
       "              0.7466676454336136],\n",
       "             2008001036: [0.09978381544351578,\n",
       "              0.18137177328964246,\n",
       "              0.28115558873315827],\n",
       "             2008001039: [0.1608559936285019,\n",
       "              0.2985335314213732,\n",
       "              0.45938952504987507],\n",
       "             2009000463: [0.7008728981018066,\n",
       "              0.423515941406184,\n",
       "              1.1243888395079906],\n",
       "             2009000464: [0.2686026990413666,\n",
       "              0.3318428471089384,\n",
       "              0.600445546150305],\n",
       "             2008001042: [0.2867875099182129,\n",
       "              0.40133804343689444,\n",
       "              0.6881255533551074],\n",
       "             2008001047: [0.652690052986145,\n",
       "              0.4593854517003843,\n",
       "              1.1120755046865294],\n",
       "             2008001048: [0.29038071632385254,\n",
       "              0.4375018639890977,\n",
       "              0.7278825803129503],\n",
       "             2009000471: [0.8726621270179749,\n",
       "              0.5821060227700782,\n",
       "              1.4547681497880531],\n",
       "             2009000474: [0.4364877939224243,\n",
       "              0.3621262806720723,\n",
       "              0.7986140745944966],\n",
       "             2008001052: [0.02324667200446129,\n",
       "              0.2926385914535439,\n",
       "              0.31588526345800516],\n",
       "             2009000476: [0.28674694895744324,\n",
       "              0.41417117641526924,\n",
       "              0.7009181253727125],\n",
       "             2008001054: [1.5544674396514893,\n",
       "              0.5702648332176075,\n",
       "              2.124732272869097],\n",
       "             2009000477: [0.4819799065589905,\n",
       "              0.2977567711719408,\n",
       "              0.7797366777309314],\n",
       "             2008001056: [1.5488308668136597,\n",
       "              0.6571599814025992,\n",
       "              2.205990848216259],\n",
       "             2008001057: [0.2599669396877289,\n",
       "              0.4171747372630098,\n",
       "              0.6771416769507387],\n",
       "             2009000486: [0.4298875033855438,\n",
       "              0.5330375017857342,\n",
       "              0.962925005171278],\n",
       "             2009000491: [0.4573150873184204,\n",
       "              0.25030810080517807,\n",
       "              0.7076231881235985],\n",
       "             2009000493: [0.14719220995903015,\n",
       "              0.07789334462046421,\n",
       "              0.22508555457949436],\n",
       "             2009000494: [0.019078021869063377,\n",
       "              0.14800959443438028,\n",
       "              0.16708761630344365],\n",
       "             2008001071: [0.2768464982509613,\n",
       "              0.3110378906122349,\n",
       "              0.5878843888631962],\n",
       "             2008001073: [0.35642433166503906,\n",
       "              0.39702148350894345,\n",
       "              0.7534458151739825],\n",
       "             2009000500: [0.010414737276732922,\n",
       "              0.14843122832664768,\n",
       "              0.1588459656033806],\n",
       "             ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:48:23.556669Z",
     "start_time": "2021-05-27T13:48:23.548848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/deeplab307/Documents/Anaconda/Shiang/CL/model/w_distillation/round1/15+1/losses.pickle'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"/\".join(get_checkpoint_path(method, 1, 50).split('/')[:-1]), 'losses.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:58:00.745982Z",
     "start_time": "2021-05-27T13:58:00.740224Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = [v[2] for v in losses.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T13:58:20.940278Z",
     "start_time": "2021-05-27T13:58:20.917729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2699,\n",
       " 4388,\n",
       " 440,\n",
       " 3593,\n",
       " 3070,\n",
       " 903,\n",
       " 2985,\n",
       " 5149,\n",
       " 3697,\n",
       " 4962,\n",
       " 759,\n",
       " 1984,\n",
       " 2918,\n",
       " 3670,\n",
       " 4276,\n",
       " 4480,\n",
       " 5117,\n",
       " 2049,\n",
       " 1065,\n",
       " 4818,\n",
       " 2760,\n",
       " 2179,\n",
       " 2923,\n",
       " 3111,\n",
       " 4958,\n",
       " 1162,\n",
       " 3723,\n",
       " 2110,\n",
       " 781,\n",
       " 4756,\n",
       " 3363,\n",
       " 4921,\n",
       " 59,\n",
       " 5138,\n",
       " 3475,\n",
       " 2054,\n",
       " 5161,\n",
       " 1885,\n",
       " 2769,\n",
       " 2846,\n",
       " 4486,\n",
       " 39,\n",
       " 876,\n",
       " 4141,\n",
       " 2416,\n",
       " 4917,\n",
       " 2292,\n",
       " 398,\n",
       " 2816,\n",
       " 3194,\n",
       " 155,\n",
       " 1345,\n",
       " 838,\n",
       " 683,\n",
       " 3148,\n",
       " 667,\n",
       " 2280,\n",
       " 2976,\n",
       " 3756,\n",
       " 4310,\n",
       " 1811,\n",
       " 3430,\n",
       " 3909,\n",
       " 2734,\n",
       " 758,\n",
       " 2880,\n",
       " 2595,\n",
       " 148,\n",
       " 1785,\n",
       " 180,\n",
       " 2184,\n",
       " 4717,\n",
       " 936,\n",
       " 64,\n",
       " 3674,\n",
       " 4573,\n",
       " 4126,\n",
       " 4429,\n",
       " 3494,\n",
       " 596,\n",
       " 3703,\n",
       " 213,\n",
       " 4433,\n",
       " 69,\n",
       " 1430,\n",
       " 4753,\n",
       " 2341,\n",
       " 719,\n",
       " 1719,\n",
       " 5013,\n",
       " 359,\n",
       " 4703,\n",
       " 3892,\n",
       " 292,\n",
       " 4372,\n",
       " 2551,\n",
       " 1545,\n",
       " 3093,\n",
       " 207,\n",
       " 2283,\n",
       " 4230,\n",
       " 1121,\n",
       " 4496,\n",
       " 5011,\n",
       " 1135,\n",
       " 1793,\n",
       " 2323,\n",
       " 1584,\n",
       " 3058,\n",
       " 1079,\n",
       " 4778,\n",
       " 2463,\n",
       " 776,\n",
       " 171,\n",
       " 3804,\n",
       " 1599,\n",
       " 2078,\n",
       " 1895,\n",
       " 3530,\n",
       " 4116,\n",
       " 1504,\n",
       " 3208,\n",
       " 4547,\n",
       " 2262,\n",
       " 4247,\n",
       " 1254,\n",
       " 3075,\n",
       " 364,\n",
       " 4596,\n",
       " 1978,\n",
       " 425,\n",
       " 1248,\n",
       " 4028,\n",
       " 2697,\n",
       " 4200,\n",
       " 920,\n",
       " 1169,\n",
       " 1140,\n",
       " 2797,\n",
       " 2555,\n",
       " 5135,\n",
       " 1838,\n",
       " 1890,\n",
       " 4945,\n",
       " 521,\n",
       " 3396,\n",
       " 3133,\n",
       " 1776,\n",
       " 2600,\n",
       " 1076,\n",
       " 4608,\n",
       " 569,\n",
       " 2255,\n",
       " 4948,\n",
       " 1161,\n",
       " 4275,\n",
       " 4724,\n",
       " 4344,\n",
       " 2708,\n",
       " 4516,\n",
       " 3481,\n",
       " 1975,\n",
       " 1638,\n",
       " 2652,\n",
       " 2259,\n",
       " 3444,\n",
       " 1953,\n",
       " 2181,\n",
       " 29,\n",
       " 3755,\n",
       " 3880,\n",
       " 738,\n",
       " 529,\n",
       " 3245,\n",
       " 2311,\n",
       " 3039,\n",
       " 5180,\n",
       " 3468,\n",
       " 2583,\n",
       " 1122,\n",
       " 5163,\n",
       " 1578,\n",
       " 4035,\n",
       " 1704,\n",
       " 910,\n",
       " 2839,\n",
       " 5147,\n",
       " 3628,\n",
       " 251,\n",
       " 4663,\n",
       " 3358,\n",
       " 2104,\n",
       " 3623,\n",
       " 4846,\n",
       " 2742,\n",
       " 2707,\n",
       " 2077,\n",
       " 406,\n",
       " 906,\n",
       " 3333,\n",
       " 76,\n",
       " 439,\n",
       " 2190,\n",
       " 2883,\n",
       " 3767,\n",
       " 3115,\n",
       " 2275,\n",
       " 1505,\n",
       " 4349,\n",
       " 4774,\n",
       " 2538,\n",
       " 321,\n",
       " 4484,\n",
       " 153,\n",
       " 1088,\n",
       " 4856,\n",
       " 4895,\n",
       " 2992,\n",
       " 701,\n",
       " 4105,\n",
       " 1563,\n",
       " 1214,\n",
       " 2235,\n",
       " 2729,\n",
       " 2136,\n",
       " 4737,\n",
       " 3152,\n",
       " 631,\n",
       " 3620,\n",
       " 1926,\n",
       " 4328,\n",
       " 4654,\n",
       " 22,\n",
       " 1014,\n",
       " 3376,\n",
       " 3451,\n",
       " 3219,\n",
       " 357,\n",
       " 3555,\n",
       " 1503,\n",
       " 1934,\n",
       " 4514,\n",
       " 3179,\n",
       " 2257,\n",
       " 2535,\n",
       " 856,\n",
       " 5154,\n",
       " 1354,\n",
       " 4214,\n",
       " 4366,\n",
       " 4873,\n",
       " 2540,\n",
       " 1641,\n",
       " 218,\n",
       " 3295,\n",
       " 1305,\n",
       " 4332,\n",
       " 4219,\n",
       " 3702,\n",
       " 2281,\n",
       " 460,\n",
       " 1742,\n",
       " 4207,\n",
       " 3466,\n",
       " 2678,\n",
       " 2058,\n",
       " 3917,\n",
       " 1181,\n",
       " 4411,\n",
       " 1145,\n",
       " 3322,\n",
       " 4215,\n",
       " 1768,\n",
       " 3126,\n",
       " 4336,\n",
       " 3927,\n",
       " 4748,\n",
       " 3757,\n",
       " 4507,\n",
       " 780,\n",
       " 3309,\n",
       " 4562,\n",
       " 2428,\n",
       " 3341,\n",
       " 3308,\n",
       " 2464,\n",
       " 4910,\n",
       " 1679,\n",
       " 3932,\n",
       " 5000,\n",
       " 2802,\n",
       " 115,\n",
       " 3157,\n",
       " 4919,\n",
       " 3656,\n",
       " 1907,\n",
       " 5038,\n",
       " 494,\n",
       " 3316,\n",
       " 2293,\n",
       " 3969,\n",
       " 1829,\n",
       " 4327,\n",
       " 1536,\n",
       " 2107,\n",
       " 2618,\n",
       " 3568,\n",
       " 1801,\n",
       " 2101,\n",
       " 647,\n",
       " 263,\n",
       " 3644,\n",
       " 4387,\n",
       " 1606,\n",
       " 281,\n",
       " 3422,\n",
       " 1710,\n",
       " 524,\n",
       " 2935,\n",
       " 4162,\n",
       " 4845,\n",
       " 2879,\n",
       " 3550,\n",
       " 3661,\n",
       " 4644,\n",
       " 5189,\n",
       " 5004,\n",
       " 372,\n",
       " 4891,\n",
       " 1570,\n",
       " 4500,\n",
       " 862,\n",
       " 2221,\n",
       " 3273,\n",
       " 2346,\n",
       " 2805,\n",
       " 2449,\n",
       " 3320,\n",
       " 258,\n",
       " 4591,\n",
       " 2098,\n",
       " 4447,\n",
       " 3303,\n",
       " 2271,\n",
       " 1398,\n",
       " 3836,\n",
       " 895,\n",
       " 806,\n",
       " 1678,\n",
       " 2599,\n",
       " 102,\n",
       " 579,\n",
       " 1541,\n",
       " 1423,\n",
       " 1820,\n",
       " 2557,\n",
       " 4325,\n",
       " 1667,\n",
       " 345,\n",
       " 878,\n",
       " 2165,\n",
       " 1357,\n",
       " 4323,\n",
       " 4982,\n",
       " 5179,\n",
       " 2427,\n",
       " 1597,\n",
       " 2156,\n",
       " 3216,\n",
       " 3888,\n",
       " 2153,\n",
       " 4087,\n",
       " 581,\n",
       " 2453,\n",
       " 1954,\n",
       " 2523,\n",
       " 2338,\n",
       " 699,\n",
       " 1666,\n",
       " 3879,\n",
       " 3018,\n",
       " 1050,\n",
       " 4132,\n",
       " 5024,\n",
       " 2197,\n",
       " 1867,\n",
       " 2776,\n",
       " 1858,\n",
       " 3549,\n",
       " 578,\n",
       " 273,\n",
       " 4622,\n",
       " 2354,\n",
       " 3147,\n",
       " 1915,\n",
       " 2251,\n",
       " 2244,\n",
       " 2122,\n",
       " 1963,\n",
       " 790,\n",
       " 4285,\n",
       " 3769,\n",
       " 2278,\n",
       " 4282,\n",
       " 3553,\n",
       " 2919,\n",
       " 4853,\n",
       " 2237,\n",
       " 4534,\n",
       " 3114,\n",
       " 4268,\n",
       " 4117,\n",
       " 4343,\n",
       " 3240,\n",
       " 4914,\n",
       " 4097,\n",
       " 4980,\n",
       " 1438,\n",
       " 3261,\n",
       " 874,\n",
       " 1792,\n",
       " 2548,\n",
       " 4662,\n",
       " 4631,\n",
       " 3235,\n",
       " 4414,\n",
       " 3895,\n",
       " 4348,\n",
       " 3339,\n",
       " 1557,\n",
       " 2954,\n",
       " 4394,\n",
       " 3828,\n",
       " 2192,\n",
       " 847,\n",
       " 2004,\n",
       " 1352,\n",
       " 4223,\n",
       " 2853,\n",
       " 2437,\n",
       " 1549,\n",
       " 334,\n",
       " 1400,\n",
       " 5022,\n",
       " 2533,\n",
       " 3605,\n",
       " 1054,\n",
       " 1645,\n",
       " 1834,\n",
       " 2660,\n",
       " 170,\n",
       " 433,\n",
       " 5036,\n",
       " 1000,\n",
       " 4668,\n",
       " 689,\n",
       " 122,\n",
       " 4442,\n",
       " 3186,\n",
       " 1183,\n",
       " 129,\n",
       " 1414,\n",
       " 1225,\n",
       " 4677,\n",
       " 3164,\n",
       " 770,\n",
       " 306,\n",
       " 4151,\n",
       " 2969,\n",
       " 1335,\n",
       " 363,\n",
       " 4783,\n",
       " 1119,\n",
       " 2475,\n",
       " 1446,\n",
       " 2527,\n",
       " 2712,\n",
       " 3285,\n",
       " 1716,\n",
       " 5097,\n",
       " 1795,\n",
       " 3709,\n",
       " 4420,\n",
       " 3896,\n",
       " 3732,\n",
       " 4319,\n",
       " 5049,\n",
       " 4153,\n",
       " 3956,\n",
       " 2972,\n",
       " 3427,\n",
       " 255,\n",
       " 622,\n",
       " 108,\n",
       " 3421,\n",
       " 2946,\n",
       " 673,\n",
       " 4469,\n",
       " 2693,\n",
       " 3368,\n",
       " 4554,\n",
       " 3288,\n",
       " 2814,\n",
       " 864,\n",
       " 3667,\n",
       " 2172,\n",
       " 2764,\n",
       " 4187,\n",
       " 3601,\n",
       " 886,\n",
       " 1218,\n",
       " 513,\n",
       " 4172,\n",
       " 4040,\n",
       " 1752,\n",
       " 11,\n",
       " 1733,\n",
       " 2161,\n",
       " 2481,\n",
       " 3041,\n",
       " 1342,\n",
       " 3271,\n",
       " 567,\n",
       " 2833,\n",
       " 195,\n",
       " 1273,\n",
       " 4882,\n",
       " 1628,\n",
       " 2626,\n",
       " 2748,\n",
       " 4563,\n",
       " 2263,\n",
       " 661,\n",
       " 2546,\n",
       " 4001,\n",
       " 2424,\n",
       " 961,\n",
       " 228,\n",
       " 1089,\n",
       " 889,\n",
       " 2570,\n",
       " 3053,\n",
       " 5133,\n",
       " 4685,\n",
       " 3520,\n",
       " 829,\n",
       " 3844,\n",
       " 219,\n",
       " 4502,\n",
       " 2242,\n",
       " 3594,\n",
       " 2719,\n",
       " 4673,\n",
       " 3121,\n",
       " 695,\n",
       " 4786,\n",
       " 3204,\n",
       " 123,\n",
       " 2314,\n",
       " 3213,\n",
       " 4857,\n",
       " 3814,\n",
       " 914,\n",
       " 1499,\n",
       " 4574,\n",
       " 1278,\n",
       " 3472,\n",
       " 45,\n",
       " 4769,\n",
       " 4242,\n",
       " 4552,\n",
       " 1670,\n",
       " 2913,\n",
       " 2325,\n",
       " 3014,\n",
       " 3629,\n",
       " 4955,\n",
       " 1461,\n",
       " 1409,\n",
       " 1943,\n",
       " 4879,\n",
       " 5059,\n",
       " 890,\n",
       " 178,\n",
       " 4770,\n",
       " 3287,\n",
       " 1827,\n",
       " 2874,\n",
       " 5033,\n",
       " 1242,\n",
       " 1297,\n",
       " 265,\n",
       " 3727,\n",
       " 1807,\n",
       " 203,\n",
       " 3912,\n",
       " 963,\n",
       " 1453,\n",
       " 261,\n",
       " 2134,\n",
       " 3972,\n",
       " 608,\n",
       " 2397,\n",
       " 3229,\n",
       " 4889,\n",
       " 4400,\n",
       " 2435,\n",
       " 322,\n",
       " 4773,\n",
       " 2907,\n",
       " 4613,\n",
       " 2547,\n",
       " 2512,\n",
       " 1069,\n",
       " 973,\n",
       " 4767,\n",
       " 4118,\n",
       " 3512,\n",
       " 221,\n",
       " 409,\n",
       " 2641,\n",
       " 4675,\n",
       " 3867,\n",
       " 4692,\n",
       " 3431,\n",
       " 3035,\n",
       " 5175,\n",
       " 3865,\n",
       " 1043,\n",
       " 487,\n",
       " 131,\n",
       " 4875,\n",
       " 3394,\n",
       " 2342,\n",
       " 2370,\n",
       " 3877,\n",
       " 2902,\n",
       " 1348,\n",
       " 800,\n",
       " 3386,\n",
       " 2264,\n",
       " 4765,\n",
       " 339,\n",
       " 2762,\n",
       " 4298,\n",
       " 3529,\n",
       " 841,\n",
       " 999,\n",
       " 1770,\n",
       " 3167,\n",
       " 593,\n",
       " 3122,\n",
       " 4155,\n",
       " 1863,\n",
       " 3653,\n",
       " 2092,\n",
       " 3929,\n",
       " 502,\n",
       " 70,\n",
       " 4851,\n",
       " 4900,\n",
       " 4195,\n",
       " 84,\n",
       " 4858,\n",
       " 3383,\n",
       " 3848,\n",
       " 4395,\n",
       " 1465,\n",
       " 534,\n",
       " 21,\n",
       " 2248,\n",
       " 5080,\n",
       " 1386,\n",
       " 589,\n",
       " 254,\n",
       " 2815,\n",
       " 1458,\n",
       " 1179,\n",
       " 1366,\n",
       " 548,\n",
       " 1029,\n",
       " 2329,\n",
       " 2063,\n",
       " 5095,\n",
       " 1349,\n",
       " 1250,\n",
       " 727,\n",
       " 3819,\n",
       " 2859,\n",
       " 1598,\n",
       " 3681,\n",
       " 4287,\n",
       " 4826,\n",
       " 4825,\n",
       " 3988,\n",
       " 5048,\n",
       " 3825,\n",
       " 5062,\n",
       " 4730,\n",
       " 4740,\n",
       " 3353,\n",
       " 3237,\n",
       " 3637,\n",
       " 1828,\n",
       " 2877,\n",
       " 4520,\n",
       " 3384,\n",
       " 718,\n",
       " 88,\n",
       " 3647,\n",
       " 2375,\n",
       " 1010,\n",
       " 2691,\n",
       " 4253,\n",
       " 996,\n",
       " 1328,\n",
       " 597,\n",
       " 922,\n",
       " 3022,\n",
       " 3403,\n",
       " 3138,\n",
       " 2180,\n",
       " 1566,\n",
       " 1647,\n",
       " 5075,\n",
       " 523,\n",
       " 3793,\n",
       " 20,\n",
       " 628,\n",
       " 2308,\n",
       " 2849,\n",
       " 4170,\n",
       " 1117,\n",
       " 4940,\n",
       " 3154,\n",
       " 3862,\n",
       " 401,\n",
       " 3425,\n",
       " 5125,\n",
       " 2162,\n",
       " 4578,\n",
       " 2024,\n",
       " 1073,\n",
       " 3519,\n",
       " 930,\n",
       " 341,\n",
       " 4341,\n",
       " 1375,\n",
       " 2639,\n",
       " 2653,\n",
       " 1059,\n",
       " 2288,\n",
       " 4745,\n",
       " 5186,\n",
       " 1090,\n",
       " 2133,\n",
       " 4495,\n",
       " 1938,\n",
       " 4555,\n",
       " 4286,\n",
       " 5136,\n",
       " 1778,\n",
       " 1239,\n",
       " 1100,\n",
       " 3714,\n",
       " 606,\n",
       " 1936,\n",
       " 500,\n",
       " 4714,\n",
       " 2973,\n",
       " 952,\n",
       " 18,\n",
       " 3583,\n",
       " 1243,\n",
       " 676,\n",
       " 3521,\n",
       " 837,\n",
       " 42,\n",
       " 4659,\n",
       " 3345,\n",
       " 2402,\n",
       " 962,\n",
       " 2597,\n",
       " 3042,\n",
       " 1703,\n",
       " 802,\n",
       " 3416,\n",
       " 4733,\n",
       " 2003,\n",
       " 3146,\n",
       " 4977,\n",
       " 5128,\n",
       " 5042,\n",
       " 2171,\n",
       " 2786,\n",
       " 2045,\n",
       " 1346,\n",
       " 4058,\n",
       " 767,\n",
       " 197,\n",
       " 1277,\n",
       " 614,\n",
       " 3639,\n",
       " 413,\n",
       " 2451,\n",
       " 94,\n",
       " 4680,\n",
       " 2379,\n",
       " 3785,\n",
       " 3056,\n",
       " 4475,\n",
       " 3252,\n",
       " 3935,\n",
       " 948,\n",
       " 2365,\n",
       " 1856,\n",
       " 4531,\n",
       " 1428,\n",
       " 3750,\n",
       " 4194,\n",
       " 705,\n",
       " 4356,\n",
       " 1336,\n",
       " 2383,\n",
       " 2688,\n",
       " 4909,\n",
       " 3552,\n",
       " 227,\n",
       " 1188,\n",
       " 741,\n",
       " 4805,\n",
       " 4483,\n",
       " 1475,\n",
       " 1367,\n",
       " 3184,\n",
       " 3292,\n",
       " 1983,\n",
       " 2598,\n",
       " 1300,\n",
       " 316,\n",
       " 3300,\n",
       " 4457,\n",
       " 971,\n",
       " 4549,\n",
       " 2069,\n",
       " 67,\n",
       " 3476,\n",
       " 3967,\n",
       " 4961,\n",
       " 1003,\n",
       " 328,\n",
       " 2558,\n",
       " 2550,\n",
       " 1364,\n",
       " 2508,\n",
       " 253,\n",
       " 5058,\n",
       " 4067,\n",
       " 3281,\n",
       " 2799,\n",
       " 836,\n",
       " 2977,\n",
       " 1515,\n",
       " 3823,\n",
       " 2025,\n",
       " 4589,\n",
       " 637,\n",
       " 110,\n",
       " 4404,\n",
       " 3832,\n",
       " 4294,\n",
       " 4056,\n",
       " 925,\n",
       " 4540,\n",
       " 4797,\n",
       " 1874,\n",
       " 2939,\n",
       " 2318,\n",
       " 4148,\n",
       " 2916,\n",
       " 570,\n",
       " 544,\n",
       " 3831,\n",
       " 233,\n",
       " 370,\n",
       " 2617,\n",
       " 247,\n",
       " 1036,\n",
       " 2850,\n",
       " 2093,\n",
       " 4519,\n",
       " 4772,\n",
       " 1569,\n",
       " 4357,\n",
       " 447,\n",
       " 4109,\n",
       " 4987,\n",
       " 15,\n",
       " 1869,\n",
       " 2425,\n",
       " 3652,\n",
       " 2855,\n",
       " 1805,\n",
       " 1527,\n",
       " 5028,\n",
       " 4392,\n",
       " 625,\n",
       " 274,\n",
       " 1150,\n",
       " 248,\n",
       " 420,\n",
       " 2851,\n",
       " 3314,\n",
       " 462,\n",
       " 644,\n",
       " 804,\n",
       " 495,\n",
       " 3771,\n",
       " 3128,\n",
       " 1649,\n",
       " 4052,\n",
       " 2213,\n",
       " 3776,\n",
       " 3095,\n",
       " 1240,\n",
       " 3443,\n",
       " 520,\n",
       " 5037,\n",
       " 2511,\n",
       " 2755,\n",
       " 4586,\n",
       " 2858,\n",
       " 5111,\n",
       " 5178,\n",
       " 4973,\n",
       " 412,\n",
       " 456,\n",
       " 5160,\n",
       " 4464,\n",
       " 3055,\n",
       " 4888,\n",
       " 3187,\n",
       " 1334,\n",
       " 3381,\n",
       " 3761,\n",
       " 3826,\n",
       " 121,\n",
       " 158,\n",
       " 2337,\n",
       " 428,\n",
       " 4601,\n",
       " 2927,\n",
       " 5002,\n",
       " 2771,\n",
       " 1077,\n",
       " 3563,\n",
       " 2827,\n",
       " 3609,\n",
       " 2460,\n",
       " 2689,\n",
       " 3749,\n",
       " 4281,\n",
       " 4511,\n",
       " 2888,\n",
       " 4594,\n",
       " 3051,\n",
       " 2368,\n",
       " 1417,\n",
       " 3682,\n",
       " 4779,\n",
       " 331,\n",
       " 4997,\n",
       " 1152,\n",
       " 3251,\n",
       " 4614,\n",
       " 2828,\n",
       " 555,\n",
       " 1787,\n",
       " 897,\n",
       " 4145,\n",
       " 4412,\n",
       " 3482,\n",
       " 2811,\n",
       " 4880,\n",
       " 2469,\n",
       " 3195,\n",
       " 5027,\n",
       " 4510,\n",
       " 3891,\n",
       " 703,\n",
       " 825,\n",
       " 1573,\n",
       " 4564,\n",
       " 223,\n",
       " 2268,\n",
       " 4582,\n",
       " 1319,\n",
       " 3943,\n",
       " 3663,\n",
       " 12,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:10:11.360151Z",
     "start_time": "2021-04-06T18:10:11.054507Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'w_distillation'\n",
    "\n",
    "for i in range(50,51,10):\n",
    "    print(np.mean(readCheckpoint(method, 1, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:09:05.086275Z",
     "start_time": "2021-04-06T18:09:04.282162Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'incremental'\n",
    "\n",
    "for i in range(10,31,10):\n",
    "    print(np.mean(readCheckpoint(method, 1, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T18:05:13.442587Z",
     "start_time": "2021-04-06T18:05:12.170534Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'incremental'\n",
    "\n",
    "for i in range(10,51,10):\n",
    "    print(np.mean(readCheckpoint(method, 0, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(readCheckpoint(method, 0, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:32:59.345393Z",
     "start_time": "2021-04-06T04:32:59.339106Z"
    }
   },
   "outputs": [],
   "source": [
    "parts = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    parts.append(retinanet.classificationModel.output.weight.data[i*9: i*9 + 9,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:33:01.611318Z",
     "start_time": "2021-04-06T04:33:01.601089Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "output = nn.Conv2d(256, 9 * 20, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:36:24.690145Z",
     "start_time": "2021-04-06T04:36:24.679573Z"
    }
   },
   "outputs": [],
   "source": [
    "output.weight.data[0:9,:,:,:] = parts[3]\n",
    "#vehicle(7)\n",
    "for i in range(0,7):\n",
    "    output.weight.data[9 + i*9:9 + i*9 + 9,:,:,:] = parts[0]\n",
    "#furniture(6)\n",
    "for i in range(0,6):\n",
    "    output.weight.data[72 + i*9:72 + i*9 + 9,:,:,:] = parts[1]\n",
    "#animals(6)\n",
    "for i in range(0,6):\n",
    "    output.weight.data[126 + i*9:126 + i*9 + 9,:,:,:] = parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:36:53.559211Z",
     "start_time": "2021-04-06T04:36:53.550539Z"
    }
   },
   "outputs": [],
   "source": [
    "(output.weight.data[72:81,:,:,:] == output.weight.data[81:90,:,:,:]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:31.511361Z",
     "start_time": "2021-03-16T17:29:31.372639Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "prev_model = copy.deepcopy(retinanet)\n",
    "retinanet.increase_class(1)\n",
    "\n",
    "retinanet.cuda()\n",
    "prev_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:38.239997Z",
     "start_time": "2021-03-16T17:29:38.234111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.ones(2,256,30,30).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:39.297163Z",
     "start_time": "2021-03-16T17:29:39.286858Z"
    }
   },
   "outputs": [],
   "source": [
    "prev_out = prev_model.classificationModel.output_act(prev_model.classificationModel.output(test))\n",
    "cur_out = retinanet.classificationModel.output_act(retinanet.classificationModel.output(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:52.746312Z",
     "start_time": "2021-03-16T17:29:52.734269Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_shape1(out, num_classes):\n",
    "    out1 = out.permute(0, 2, 3, 1)\n",
    "    batch_size, width, height, channels = out1.shape\n",
    "    out1 = out1.view(batch_size, width, height, 9, num_classes)\n",
    "    \n",
    "    return out1.contiguous().view(2, -1, num_classes)\n",
    "def change_shape2(out, num_classes):\n",
    "    out1 = out.permute(0, 2, 3, 1)\n",
    "    batch_size, width, height, channels = out1.shape\n",
    "    out1 = out1.view(batch_size, width, height,num_classes, 9)\n",
    "    \n",
    "    out1 = out1.permute(0, 1, 2, 4, 3)\n",
    "    \n",
    "    return out1.contiguous().view(2, -1, num_classes)\n",
    "prev_out_new1 = change_shape1(prev_out, 19)\n",
    "cur_out_new1 = change_shape1(cur_out, 20)\n",
    "\n",
    "# prev_out_new2 = change_shape2(prev_out, 19)\n",
    "# cur_out_new2 = change_shape2(cur_out, 20)\n",
    "# (prev_out == cur_out[:,:171,:,:]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:29:54.846712Z",
     "start_time": "2021-03-16T17:29:54.840638Z"
    }
   },
   "outputs": [],
   "source": [
    "(prev_out_new1[:,:,:] == cur_out_new[:,:,:,:,:19]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:55:49.181142Z",
     "start_time": "2021-03-16T16:55:49.174175Z"
    }
   },
   "outputs": [],
   "source": [
    "(cur_out_new1 == cur_out_new2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:49:43.834567Z",
     "start_time": "2021-03-16T16:49:43.826089Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(prev_out_new[:,:,:,:,:] == cur_out_new[:,:,:,:,:19]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:50:11.811545Z",
     "start_time": "2021-03-16T16:50:11.803428Z"
    }
   },
   "outputs": [],
   "source": [
    "(prev_out_new[:,:,:] == cur_out_new[:,:,:19]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:34:58.780124Z",
     "start_time": "2021-03-16T16:34:58.754078Z"
    }
   },
   "outputs": [],
   "source": [
    "(prev_out_new[:,:,:,:,:] == cur_out_new[:,:,:,:19,:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:30:55.282386Z",
     "start_time": "2021-03-16T16:30:55.272479Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_out_new[0,0,0,0,:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:59:35.575091Z",
     "start_time": "2021-03-16T03:59:35.566585Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(prev_out_new[0,0,0,0,:] == cur_out_new[0,0,0,0,:19]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:46:56.387557Z",
     "start_time": "2021-03-16T03:46:56.379688Z"
    }
   },
   "outputs": [],
   "source": [
    "(prev_out_new == cur_out_new[:,:,:,:171]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:42:07.269525Z",
     "start_time": "2021-03-16T03:42:07.108162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:42:12.735154Z",
     "start_time": "2021-03-16T03:42:12.588827Z"
    }
   },
   "outputs": [],
   "source": [
    "prev_out_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:39:44.668385Z",
     "start_time": "2021-03-16T03:39:44.522280Z"
    }
   },
   "outputs": [],
   "source": [
    " == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:38:50.483946Z",
     "start_time": "2021-03-16T03:38:50.477833Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_out_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:38:19.375882Z",
     "start_time": "2021-03-16T03:38:19.349020Z"
    }
   },
   "outputs": [],
   "source": [
    "(cur_out_new[:,:,:,:171] == prev_out_new).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T12:13:48.896405Z",
     "start_time": "2021-03-14T12:13:48.419875Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "root_path = '/home/deeplab307/Documents/Anaconda/Shiang/CL/'\n",
    "method = 'w_distillation'\n",
    "start_round = 2\n",
    "batch_size = 2\n",
    "\n",
    "def get_checkpoint_path(method, now_round, epoch):\n",
    "    global root_path\n",
    "\n",
    "    path = os.path.join(root_path, 'model', method, 'round{}'.format(now_round), 'voc_retinanet_{}_checkpoint.pt'.format(epoch))\n",
    "    return path\n",
    "\n",
    "def readCheckpoint(method, now_round, epoch):\n",
    "    prev_checkpoint = torch.load(get_checkpoint_path(method, now_round, epoch))\n",
    "    return prev_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T18:13:50.203803Z",
     "start_time": "2021-04-28T18:13:50.199736Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T18:14:28.660704Z",
     "start_time": "2021-04-28T18:14:28.654592Z"
    }
   },
   "outputs": [],
   "source": [
    "test = np.random.randn(5,2)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T18:14:42.174710Z",
     "start_time": "2021-04-28T18:14:42.167588Z"
    }
   },
   "outputs": [],
   "source": [
    "test[[1,2,3],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:shiang]",
   "language": "python",
   "name": "conda-env-shiang-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
